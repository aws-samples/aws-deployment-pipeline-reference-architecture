{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>A deployment pipeline is the key architectural construct for performing Continuous Integration, Delivery, and Deployment. Pipelines consist of a series of stages like source, build, test, or deploy. Stages consist of automated tasks in the software delivery lifecycle. There are different types of deployment pipelines for different use cases.</p> <p>The Deployment Pipeline Reference Architecture (DPRA) for AWS workloads describes the stages and actions for different types of pipelines that exist in modern systems. The DPRA also describes the practices teams employ to increase the velocity, stability, and security of software systems through the use of deployment pipelines. For a higher-level perspective, see Clare Liguori\u2019s article in the Amazon Builder\u2019s Library titled Automating safe, hands-off deployments.</p> <p>Customers and third-party vendors can use the DPRA to create implementations - reference or otherwise - using their own set of services and tools. We have included reference implementations that use AWS and third-party tools. When an AWS service/tool is available, we list it; when there are no AWS services/tools, we list third-party tools. There are many third-party tools that can run on AWS so the ones we chose should only be seen as examples for illustrative purposes. Choose the best tool that meets the needs of your organization.</p> <p>The DPRA covers the following deployment pipelines in detail:</p> Application <p>             Build, test, and deploy an application.         </p> Compute Image <p>             Build and publish machine or container images.         </p> Account Fleet Management <p>             Manage a fleet of AWS accounts through a pipeline.         </p> Dynamic Configuration <p>             Manage dynamic configuration for a workload.         </p>"},{"location":"#architecture","title":"Architecture","text":"<p>A typical solution uses multiple or all of the pipelines in combination as follows: </p>"},{"location":"#business-outcomes","title":"Business Outcomes","text":"<p>Modern deployment pipelines create the following business outcomes:</p> <ul> <li> <p>Automation - Everything necessary to build, test, deploy, and run an application should be defined as code - code for pipelines, accounts, networking, infrastructure, applications/services, configuration, data, security, compliance, governance, auditing, and documentation \u2013 any aspect inside and outside software systems.</p> </li> <li> <p>Consistency - The source code should only be built and packaged once. The packaged artifact should then be staged in a registry with appropriate metadata and ready for deployment to any environment. Build artifacts only once and then promote them through the pipeline. The output of the pipeline should be versioned and able to be traced back to the source it was built from and from the business requirements that defined it.</p> </li> <li> <p>Small Batches - The pipeline should be constructed in such a way as to encourage the delivery of software early and often. This is accomplished by removing toil from the software delivery process through automation and fast feedback. Likewise, the pipeline should discourage the use of long-lived branches and encourage trunk-based development. Developers should be able to merge their changes to the trunk and deploy through the pipeline daily.</p> </li> <li> <p>Orchestration - As part of a deployment pipeline, every merged code change has a fully-automated build, test, publish, deploy, and release process run across all environments. Each stage automatically transitions to the next stage of the pipeline upon success, or stops on failure. In some circumstances human approvals are necessary while organizations mature their automation practices. These approvals most often show up when automation is unable to assess the risk or specific context for approval. If used, human approvals should be used before production deployments only and should be reduced to a button-click interface that triggers an automated pipeline process to continue. A single pipeline should orchestrate the deployment to all environments rather than creating pipelines for each environment.</p> </li> <li> <p>Fast Feedback - Automatically notify engineers as soon as possible of build, test, quality, and security errors from deployment pipelines through the most effective means such as chat or email.</p> </li> <li> <p>Always Deployable - When an error occurs in the mainline of a deployment pipeline, the top priority is to fix the build and ensure deployment obtains and remains in a healthy state before introducing any further changes. The pipeline should be the authoritative source for deciding if and when changes are ready to be released into production.</p> </li> <li> <p>Measured - Provide real-time metrics for code quality, speed (deployment frequency and deployment lead time), security (security control automation %, mean time to resolve security errors), and reliability (change failures and time to restore service). View metrics through a real-time dashboard. When instrumentation is not yet possible, create a Likert-based questionnaire to determine these metrics across teams.</p> </li> </ul>"},{"location":"#definitions","title":"Definitions","text":""},{"location":"#component","title":"Component","text":"<p>A component is the code, configuration, and AWS Resources that together deliver against a requirement. A component is often the unit of technical ownership, and is decoupled from other components.</p> <p>(source: AWS Well-Architected Framework definitions)</p>"},{"location":"#workload","title":"Workload","text":"<p>A workload is a set of components that together deliver business value. A workload is usually the level of detail that business and technology leaders communicate about. Examples of workloads are marketing websites, e-commerce websites, the back-ends for a mobile app, analytic platforms, etc. Workloads vary in levels of architectural complexity, from static websites to architectures with multiple data stores and many components.</p> <p>(source: AWS Well-Architected Framework)</p>"},{"location":"#environment","title":"Environment","text":"<p>An environment is an isolated target for deploying and testing a workload and its dependencies. Environments can be created for validating changes, achieving data compliance, or for improving resiliency. Example environments include creating separate AWS accounts for each developer, creating separate AWS accounts for staging and production, and using multiple regions for production traffic. Best practice is for each environment to run in a separate AWS account.</p>"},{"location":"account-fleet-management-pipeline/","title":"Account Fleet Management Pipeline","text":"<p>Enterprises develop and deploy shared services that can be consumed by many teams in an organization. A shared services deployment pipeline may provision one or more services. For example, you may design a shared services deployment pipeline to launch an AWS Organization including provisioning multiple AWS accounts with guardrails. As part of this provisioning, certain AWS services and configuration may be enabled such as AWS Control Tower, AWS Organizations, AWS Service Catalog, AWS Security Hub, Amazon GuardDuty, Amazon Macie, and others. The automated provisioning of these services and configurations are run as part of these shared services deployment pipelines.</p>"},{"location":"account-fleet-management-pipeline/#source","title":"Source","text":"<ul> <li>Test source code</li> <li>Infrastructure as code</li> <li>Dependency libraries</li> <li>Configuration</li> </ul>"},{"location":"account-fleet-management-pipeline/#pre-commit-hooks","title":"Pre-Commit hooks","text":"<ul> <li>Secrets Detection - Identify secrets such as usernames, passwords, and access keys in code and other files before they are published to a repository by using pre-commit hooks. When discovering secrets, the code push should fail immediately.</li> <li>IDE Plugins - Warn developers in their IDE using plugins and extensions such as. Examples could include markdown linters, yaml/json validators, and flake8/PEP8 code quality analyzers.</li> </ul>"},{"location":"account-fleet-management-pipeline/#alpha","title":"Alpha","text":"<p>All actions run In this stage are also run on developers\u2019 local environments prior to code commit and peer review.</p> <ul> <li>Policy as Code - Run preventative automated checks are run to ensure that the code and environments conform to organization/team policy. For example, these checks might alert when code defines volumes or buckets as not encrypted, services not enabled, or endpoints not protected. When policies are violated, AWS recommends the build fails so that developers can fix the errors. (e.g., CloudFormation Guard, OPA, and IAM Access Analyzer)</li> <li>Secrets Detection &amp; Repo Cleansing - Identify secrets such as usernames, passwords, and access keys. When secrets are discovered, the build fails and all secrets in the source code repo history are purged. (e.g., Amazon CodeGuru Secrets Detection, git-secrets)</li> <li>IaC Unit Tests - Run automated test-driven infrastructure based on requirements (e.g., Cucumber/Gherkin, Rego)</li> </ul>"},{"location":"account-fleet-management-pipeline/#beta","title":"Beta","text":"<ul> <li>Provision Resources - Launch an environment in an AWS test account from IaC code templates.</li> <li>Integration Tests - Run tests against the provisioned resources to check for errors (e.g., AWS TaskCat)</li> </ul>"},{"location":"account-fleet-management-pipeline/#gamma","title":"Gamma","text":"<ul> <li>TODO</li> </ul>"},{"location":"account-fleet-management-pipeline/#prod","title":"Prod","text":"<ul> <li>Approval - Optional action as part of an automated workflow, obtain authorized human approval before completing deployment.</li> <li>Generate Service Catalog Product - Provide shared services through a service catalog so that teams can obtain the software through self-service means in a controlled manner. IAM policies ensure only the necessary access is applied to principals.</li> </ul>"},{"location":"application-pipeline/","title":"Architecture","text":"<p>The term \"Application\" is used synonymously with the term \"Component\" as defined by the Well-Architected Framework and other DPRA pipelines. Applications are the most common use case for a deployment pipeline. This pipeline type will take source code files, tests, static analysis, database deployment, configuration, and other code to perform build, test, deploy, and release processes. The pipeline launches an environment from the compute image artifacts generated in the compute image pipeline. Automated tests are run on the environment(s) as part of the deployment pipeline.</p> <p>This pipeline encourages trunk based development in which developers frequently avoid long-lived branches and regulary commit their changes to the trunk. Therefore this pipeline only executes for commits to the trunk. Every commit to the trunk has a change to go to production if all steps of the pipeline complete successfully.</p> <p></p> <p>The expected outcome of this pipeline is to be able to safely release software changes to customers within a couple hours. Deployment pipelines should publish the following metrics:</p> <ul> <li><code>Lead time</code> \u2013 the average amount of time it takes for a single commit to get all the way into production.</li> <li><code>Deploy frequency</code> \u2013 the number of production deployments within a given time period.</li> <li><code>Mean time between failure (MTBF)</code> \u2013 the average amount of time between the start of a successful pipeline and the start of a failed pipeline.</li> <li><code>Mean time to recover (MTTR)</code> \u2013 the average amount of time between the start of a failed pipeline and the start of the next successful pipeline.</li> </ul> <p>Each stage below will include a required and recommended actions. The actions will include guidance on what steps out to be perfomed in each action. References will be made to real-life examples of tools to help better define the actions involved in each stage. The use of these examples is not an endorsement of any specific tool.</p>"},{"location":"application-pipeline/#local-development","title":"Local Development","text":"<p>Developers need fast-feedback for potential issues with their code. Automation should run in their developer workspace to give them feedback before the deployment pipeline runs.</p> Pre-Commit Hooks <p>Pre-Commit hooks are scripts that are executed on the developer's workstation when they try to create a new commit. These hooks have an opportunity to inspect the state of the code before the commit occurs and abort the commit if tests fail. An example of pre-commit hooks are Git hooks.  Examples of tools to configure and store pre-commit hooks as code include but are not limited to husky and pre-commit.</p> IDE Plugins <p>Warn developers of potential issues with their source code in their IDE using plugins and extensions including but not limited to Visual Studio Code - Python Extension and IntelliJ IDEA - JavaScript linters.</p>"},{"location":"application-pipeline/#source","title":"Source","text":"<p>The source stage pulls in various types of code from a distributed version control system.</p> Application Source Code <p>Code that is compiled, transpiled or interpreted for the purpose of delivering business capabilities through applications and/or services.</p> Test Source Code <p>Code that verifies the expected functionality of the Application Source Code and the Infrastructure Source Code. This includes source code for unit, integration, end-to-end, capacity, chaos, and synthetic testing. All Test Source Code is required to be stored in the same repository as the app to allow tests to be created and updated on the same lifecycle as the Application Source Code.</p> Infrastructure Source Code <p>Code that defines the infrastructure necessary to run the Application Source Code. Examples of infrastructure source code include but are not limited to AWS Cloud Development Kit, AWS CloudFormation and HashiCorp Terraform. All Infrastructure Source Code is required to be stored in the same repository as the app to allow infrastructure to be created and updated on the same lifecycle as the Application Source Code.</p> Static Assets <p>Assets used by the Application Source Code such as html, css, and images.</p> Dependency Manifests <p>References to third-party code that is used by the Application Source Code. This could be libraries created by the same team, a separate team within the same organization, or from an external entity.</p> Static Configuration <p>Files (e.g. JSON, XML, YAML or HCL) used to configure the behavior of the Application Source Code. Any configuration that is environment specific should not be included in Application Source Code. Environment specific configuration should be defined in the Infrastructure Source Code and injected into the application at runtime through a mechanism such as environment variables.</p> Database Source Code <p>Code that defines the schema and reference data of the database used by the Application Source Code. Examples of database source code include but are not limited to Liquibase. If the Application Source Code uses a private database that no other application accesses, then the database source code is required to be stored in the same repository as the Application Source Code. This allows the Application Source Code and Database Source Code to be updated on the same lifecycle. However, if the database is shared by multiple applications then the Database Source Code should be maintained in a separate repository and managed by separate pipeline. It should be noted that this is undesireable as it introduces coupling between applications.</p> <p>All the above source code is versioned and securely accessed through role based access control with source code repositories including but not limited to AWS CodeCommit, GitHub, GitLab, and Bitbucket.</p>"},{"location":"application-pipeline/#build","title":"Build","text":"<p>All actions run in this stage are also run on developer's local environments prior to code commit and peer review. Actions in this stage should all run in less than 10 minutes so that developers can take action on fast feedback before moving on to their next task. If it\u2019s taking more time, consider decoupling the system to reduce dependencies, optimizing the process, using more efficient tooling, or moving some of the actions to latter stages. Each of the actions below are defined and run in code.</p> Build Code <p>Convert code into artifacts that can be promoted through environments. Most builds complete in seconds. Examples include but are not limited to Maven and tsc.</p> Unit Tests <p>Run the test code to verify that individual functions and methods of classes, components or modules of the Application Source Code are performing according to expectations. These tests are fast-running tests with zero dependencies on external systems returning results in seconds. Examples of unit testing frameworks include but are not limited to JUnit, Jest, and pytest. Test results should be published somewhere such as AWS CodeBuild Test Reports.</p> Code Quality <p>Run various automated static analysis tools that generate reports on code quality, coding standards, security, code coverage, and other aspects according to the team and/or organization\u2019s best practices. AWS recommends that teams fail the build when important practices are violated (e.g., a security violation is discovered in the code). These checks usually run in seconds. Examples of tools to measure code quality include but are not limited to Amazon CodeGuru, SonarQube, black, and ESLint.</p> <p></p> Secrets Detection <p>Identify secrets such as usernames, passwords, and access keys in code. When discovering secrets, the build should fail immediately. Examples of secret detection tools include but are not limited to GitGuardian and gitleaks.</p> <p></p> Static Application Security Testing (SAST) <p>Analyze code for application security violations such as XML External Entity Processing, SQL Injection, and Cross Site Scripting. Any findings that exceed the configured threshold will immediately fail the build and stop any forward progress in the pipeline. Examples of tools to perform static application security testing include but are not limited to Amazon CodeGuru, SonarQube, and Checkmarx.</p> <p></p> Package and Store Artifact(s) <p>While the Build Code action will package most of the relevant artifacts, there may be additional steps to automate for packaging the code artifacts. Artifacts should only be built and packaged once and then deployed to various environments to validate the artifact. Artifacts should never be rebuilt during subsequent deploy stages. Once packaged, automation is run in this action to store the artifacts in an artifact repository for future deployments. Examples of artifact repositories include but are not limited to AWS CodeArtifact, Amazon ECR, Nexus, and JFrog Artifactory.</p> <p>Packages should be signed with a digital-signature to allow deployment processes to confirm the code being deployed is from a trusted publisher and has not been altered. AWS Signer can be used to cryptographically sign code for AWS Lambda applications and AWS-supported IoT devices.</p> <p></p> Software Composition Analysis (SCA) <p>Run software composition analysis (SCA) tools to find vulnerabilities to package repositories related to open source use, licensing, and security vulnerabilities. SCA tools also launch workflows to fix these vulnerabilities. Any findings that exceed the configured threshold will immediately fail the build and stop any forward progress in the pipeline. These tools also require a software bill of materials (SBOM) exist. Example SCA tools include but are not limited to Dependabot, Snyk, and Blackduck.</p> <p></p> Software Bill of Materials (SBOM) <p>Generate a software bill of materials (SBOM) report detailing all the dependencies used. Examples of SBOM formats include SPDX and CycloneDX</p>"},{"location":"application-pipeline/#test-beta","title":"Test (Beta)","text":"<p>Testing is performed in a beta environment to validate that the latest code is functioning as expected. This validation is done by first deploying the code and then running integration and end-to-end tests against the deployment. Beta environments will have dependencies on the applications and services from other teams in their gamma environments. All actions performed in this stage should complete within 30 minutes to provide fast-feedback.</p> Launch Environment <p>Use a compute image from an image repository (e.g., AMI or a container repo) and launch an environment from the image using Infrastructure Source Code. The beta image is generally not accessible to public customers and is only used for internal software validation. The beta environment should be in a different AWS Account from the tools used to run the deployment pipeline.  Access to the beta environment should be handled via cross-account IAM roles rather than long lived credentials from IAM users. Example tools for defining infrastructure code include but are not limited to AWS Cloud Development Kit, AWS CloudFormation and HashiCorp Terraform.</p> <p></p> Database Deploy <p>Apply changes to the beta database using the Database Source Code. Changes should be made in a manner that ensures rollback safety. Best practice is to connect to the beta database through cross-account IAM roles and IAM database authentication for RDS rather than long lived database credentials. If database credentials must be used, then they should be loaded from a secret manager such as AWS Secrets Manager. Changes to the database should be incremental, only applying the changes since the prior deployment. Examples of tools that apply incremental database changes include but are not limited to Liquibase, VS Database Project, and Flyway.</p> <p>Test data management is beyond the scope of this reference architecuture but should be addressed during <code>Database Deploy</code> in preparation of subsequent testing activity.</p> <p></p> Deploy Software <p>Deploy software to the beta environment. Software is not deployed from source but rather the artifact that was packaged and stored in the Build Stage will be used for the deployment. Software to be deployed should include digital signatures to verify that the software came from a trusted source and that no changes were made to the software. Software deployments should be performed through Infrastructure Source Code. Access to the beta environment should be handled via cross-account IAM roles rather than long lived credentials from IAM users. Examples of tools to deploy software include but are not limited to AWS CodeDeploy, Octopus Deploy, and Spinnaker.</p> <p></p> Integration Tests <p>Run automated tests that verify if the application satisifes business requirements. These tests require the application to be running in the beta environment. Integration tests may come in the form of behavior-driven tests, automated acceptance tests, or automated tests linked to requirements and/or stories in a tracking system. Test results should be published somewhere such as AWS CodeBuild Test Reports. Examples of tools to define integration tests include but are not limited to Cucumber, vRest, and SoapUI.</p> <p></p> Acceptance Tests <p>Run automated testing from the users\u2019 perspective in the beta environment. These tests verify the user workflow, including when performed through a UI. These test are the slowest to run and hardest to maintain and therefore it is recommended to only have a few end-to-end tests that cover the most important application workflows. Test results should be published somewhere such as AWS CodeBuild Test Reports. Examples of tools to define end-to-end tests include but are not limited to Cypress, Selenium, and Telerik Test Studio.</p> <p></p>"},{"location":"application-pipeline/#test-gamma","title":"Test (Gamma)","text":"<p>Testing is performed in a gamma environment to validate that the latest code can be safely deployed to production. The environment is as production-like as possible including configuration, monitoring, and traffic. Additionally, the environment should match the same regions that the production environment uses. The gamma environment is used by other team's beta environments and therefore must maintain acceptable service levels to avoid impacting other team productivity. All actions performed in this stage should complete within 30 minutes to provide fast-feedback.</p> Launch Environment <p>Use the compute image from an image repository (e.g., AMI or a container repo) and launch an environment from the image using Infrastructure Source Code. The gamma environment should be in a different AWS Account from the tools used to run the deployment pipeline.  Access to the gamma environment should be handled via cross-account IAM roles rather than long lived credentials from IAM users. Example tools for defining infrastructure code include but are not limited to AWS Cloud Development Kit, AWS CloudFormation and HashiCorp Terraform.</p> <p></p> Database Deploy <p>Apply changes to the gamma database using the Database Source Code. Changes should be made in a manner that ensures rollback safety. Best practice is to connect to the gamma database through cross-account IAM roles and IAM database authentication for RDS rather than long lived database credentials. If database credentials must be used, then they should be loaded from a secret manager such as AWS Secrets Manager. Changes to the database should be incremental, only applying the changes since the prior deployment. Examples of tools that apply incremental database changes include but are not limited to Liquibase, VS Database Project, and Flyway.</p> <p></p> Deploy Software <p>Deploy software to the gamma environment. Software is not deployed from source but rather the artifact that was packaged and stored in the Build Stage will be used for the deployment. Software to be deployed should include digital signatures to verify that the software came from a trusted source and that no changes were made to the software. Software deployments should be performed through Infrastructure Source Code. Access to the gamma environment should be handled via cross-account IAM roles rather than long lived credentials from IAM users. Examples of tools to deploy software include but are not limited to AWS CodeDeploy, Octopus Deploy, and Spinnaker.</p> <p></p> Application Monitoring &amp; Logging <p>Monitor deployments across regions and fail when threshold breached. The thresholds for metric alarms should be defined in the Infrastructure Source Code and deployed along with the rest of the infrastructure in an environment. Ideally, deployments should be automatically failed and rolled back when error thresholds are breached. Examples of automated rollback include AWS CloudFormation monitor &amp; rollback, AWS CodeDeploy rollback and Flagger.</p> <p></p> Synthetic Tests <p>Tests that run continuously in the background in a given environment to generate traffic and verify the system is healthy. These tests serve two purposes: 1/ Ensure there is always adequate traffic in the environment to trigger alarms if a deployment is unhealthy 2/ Test specific workflows and assert that the system is functioning correctly. Examples of tools that can be used for synthetic tests include but are not limited to Amazon CloudWatch Synthetics,Dynatrace Synthetic Monitoring, and Datadog Synthetic Monitoring.</p> <p></p> Performance Tests <p>Run longer-running automated capacity tests against environments that simulate production capacity. Measure metrics such as the transaction success rates, response time and throughput. Determine if application meets performance requirements and compare metrics to past performance to look for performance degredation. Examples of tools that can be used for performance tests include but are not limited to JMeter, Locust, and Gatling.</p> <p></p> Resilience Tests <p>Inject failures into environments to identify areas of the application that are susceptible to failure. Tests are defined as code and applied to the environment while the system is under load. The success rate, response time and throughput are measured during the periods when the failures are injected and compared to periods without the failures. Any significant deviation should fail the pipeline. Examples of tools that can be used for chaos/resilience testing include but are not limited to AWS Fault Injection Simulator, Gremlin, and ChaosToolkit.</p> <p></p> Dynamic Application Security Testing (DAST) <p>Perform testing of web applications and APIs by running automated scans against it to identify vulnerabilities through techniques such as cross-site scripting (XSS) and SQL injection(SQLi).  Examples of tools that can be used for dynamic application security testing include but are not limited to OWASP ZAP, StackHawk, and AppScan. See AWS Guidance on Penetration Testing for info on penetration testing in an AWS environment.</p>"},{"location":"application-pipeline/#prod","title":"Prod","text":"Manual Approval <p>As part of an automated workflow, obtain authorized human approval before deploying to the production environment.</p> Database Deploy <p>Apply changes to the production database using the Database Source Code. Changes should be made in a manner that ensures rollback safety. Best practice is to connect to the production database through cross-account IAM roles and IAM database authentication for RDS rather than long lived database credentials. If database credentials must be used, then they should be loaded from a secret manager such as AWS Secrets Manager. Changes to the database should be incremental, only applying the changes since the prior deployment. Examples of tools that apply incremental database changes include but are not limited to Liquibase, VS Database Project, and Flyway.</p> <p></p> Progressive Deployment <p>Deployments should be made progressively in waves to limit the impact of failures. A common approach is to deploy changes to a subset of AWS regions and allow sufficient bake time to monitor performance and behavior before proceeding with additional waves of AWS regions.</p> <p>Software should be deployed using one of progressive deployment involving controlled rollout of a change through techniques such as canary deployments, feature flags, and traffic shifting. Software deployments should be performed through Infrastructure Source Code. Access to the production environment should be handled via cross-account IAM roles rather than long lived credentials from IAM users. Examples of tools to deploy software include but are not limited to AWS CodeDeploy. Ideally, deployments should be automatically failed and rolled back when error thresholds are breached. Examples of automated rollback include AWS CloudFormation monitor &amp; rollback, AWS CodeDeploy rollback and Flagger.</p> <p></p> Synthetic Tests <p>Tests that run continuously in the background in a given environment to generate traffic and verify the system is healthy. These tests serve two purposes: 1/ Ensure there is always adequate traffic in the environment to trigger alarms if a deployment is unhealthy 2/ Test specific workflows and assert that the system is functioning correctly. Examples of tools that can be used for synthetic tests include but are not limited to Amazon CloudWatch Synthetics,Dynatrace Synthetic Monitoring, and Datadog Synthetic Monitoring.</p> <p></p>"},{"location":"application-pipeline/additional-sources/","title":"Additional Sources","text":"<p>The following blog posts align to this reference architecture and will be used to build future reference implementations:</p> <ul> <li>Building end-to-end AWS DevSecOps CI/CD pipeline with open source SCA, SAST and DAST tools</li> <li>Building an end-to-end Kubernetes-based DevSecOps software factory on AWS</li> </ul>"},{"location":"application-pipeline/ri-cdk-pipeline/","title":"AWS CDK Pipeline","text":"<p>This presents a reference implementation of the Application Pipeline reference architecture. The pipeline is built with AWS CodePipeline and uses AWS CodeBuild for building the software and performing testing tasks. All the infrastructure for this reference implementation is defined with AWS Cloud Development Kit. The pipelines are defined using the CDK Pipelines L3 constructs. The source code for this reference implementation is available in GitHub for running in your own local account.</p> <p></p> Disclaimer <p>This reference implementation is intended to serve as an example of how to accomplish the guidance in the reference architecture using CDK Pipelines. The reference implementation has intentionally bypassed the following AWS Well-Architected best practices to make it accessible by a wider range of customers. Be sure to address these before using parts of this code for any workloads in your own environment:</p> <ul> <li> TLS on HTTP endpoint - the listener for the sample application uses HTTP instead of HTTPS to avoid having to create new ACM certificates and Route53 hosted zones. This should be replaced in your account with an <code>HTTPS</code> listener.</li> </ul> <p></p>"},{"location":"application-pipeline/ri-cdk-pipeline/#local-development","title":"Local Development","text":"<p>Developers need fast-feedback for potential issues with their code. Automation should run in their developer workspace to give them feedback before the deployment pipeline runs.</p> Pre-Commit Hooks <p>Pre-Commit hooks are scripts that are executed on the developer's workstation when they try to create a new commit. These hooks have an opportunity to inspect the state of the code before the commit occurs and abort the commit if tests fail. An example of pre-commit hooks are Git hooks.  Examples of tools to configure and store pre-commit hooks as code include but are not limited to husky and pre-commit.</p> <p>The following <code>.pre-commit-config.yaml</code> is added to the repository that will build the code with Maven, run unit tests with JUnit, check for code quality with Checkstyle, run static application security testing with PMD and check for secrets in the code with gitleaks.</p> <pre><code>repos:\n- repo: https://github.com/pre-commit/pre-commit-hooks\n  rev: v2.3.0\n  hooks:\n  -   id: check-yaml\n  -   id: check-json\n  -   id: trailing-whitespace\n- repo: https://github.com/pre-commit/mirrors-eslint\n  rev: v8.23.0\n  hooks:\n  -   id: eslint\n- repo: https://github.com/ejba/pre-commit-maven\n  rev: v0.3.3\n  hooks:\n  -   id: maven-test\n- repo: https://github.com/zricethezav/gitleaks\n  rev: v8.12.0\n  hooks:\n    - id: gitleaks\n</code></pre>"},{"location":"application-pipeline/ri-cdk-pipeline/#source","title":"Source","text":"Application Source Code <p>The application source code can be found in the src/main/java directory. It is intended to serve only as a reference and should be replaced by your own application source code.</p> <p>This reference implementation includes a Spring Boot application that exposes a REST API and uses a database for persistence. The API is implemented in <code>FruitController.java</code>:</p> <pre><code>public class FruitController {\n    /**\n     * JPA repository for fruits.\n     */\n    private final FruitRepository repository;\n\n    /**\n     * Logic to map between entities and DTOs\n     */\n    private final FruitMapper mapper;\n\n    FruitController(final FruitRepository r, final FruitMapper m) {\n        this.repository = r;\n        this.mapper = m;\n    }\n\n    @GetMapping(\"/api/fruits\")\n    List&lt;FruitDTO&gt; all() {\n        return repository.findAll()\n                .stream()\n                .map(mapper::toDto)\n                .collect(Collectors.toList());\n    }\n\n    @PostMapping(\"/api/fruits\")\n    FruitDTO newFruit(@RequestBody final FruitDTO fruit) {\n        return mapper.toDto(repository.save(mapper.toEntity(fruit)));\n    }\n\n    @GetMapping(\"/api/fruits/{id}\")\n    FruitDTO one(@PathVariable final Long id) {\n        return repository.findById(id)\n                .map(mapper::toDto)\n                .orElseThrow(() -&gt; new FruitNotFoundException(id));\n    }\n\n    @PutMapping(\"/api/fruits/{id}\")\n    FruitDTO replaceFruit(\n            @RequestBody final FruitDTO newFruit,\n            @PathVariable final Long id) {\n        newFruit.setId(id);\n        return mapper.toDto(repository.save(mapper.toEntity(newFruit)));\n    }\n\n    @DeleteMapping(\"/api/fruits/{id}\")\n    void deleteFruit(@PathVariable final Long id) {\n        repository.deleteById(id);\n    }\n}\n</code></pre> <p>The application source code is stored in AWS CodeCommit repository that is created and initialized from the CDK application in the <code>CodeCommitSource</code> construct:</p> <pre><code>import {prompts, prompt} from \"prompts\";\nimport {CodeConnectionsClient, CreateConnectionCommand, ProviderType, GetConnectionCommand} from \"@aws-sdk/client-codeconnections\";\nimport * as child from \"child_process\";\n\n\nasync function main() {\n    console.log(\"This script will help you create a connection to an external version control system\");\n    try {\n        const source = await promptSourceType();\n        console.log(source)\n        let cmd = 'npx cdk deploy --profile toolchain --all --require-approval never';\n        if (source != 'codecommit') {\n            const repoParameters = (await promptExternalSourceParamters(source));\n            const command = await setupCodeConnection(source, repoParameters);\n            console.log(\"The connection is created to connect the external version control system\");\n            await checkForCodeConnectionAvailable(repoParameters, 30, 10, command.ConnectionArn);\n            repoParameters[\"connectionArn\"]=command.ConnectionArn\n            repoParameters[\"providerType\"]=source\n            await updateCdkJson(repoParameters)\n            cmd=cmd.concat(` -c owner=${repoParameters.owner} -c repositoryName=${repoParameters.repositoryName} -c trunckBranchname=${repoParameters.trunkBranchName} -c connectionArn=${command.ConnectionArn} -c providerType=${source}`)\n        }\n        else {\n            console.log(\"No external paramters required, proceeding with codecommit\");\n        }\n        console.log(cmd)\n        const commandParts = cmd.split(/\\s+/);\n        const resp = child.spawnSync(commandParts[0], commandParts.slice(1),{ stdio: 'inherit' });\n        console.log(resp);\n        if (resp.status !== 0) {\n            throw new Error(`${resp.status} - Error`);\n        }\n    }catch (e) {\n        console.error(e);\n        process.exit(1);\n    }\n}\n\nasync function promptSourceType() {\n  const source = prompts.select({\n      type: 'select',\n      name: 'source',\n      message: 'Select pipeline source',\n      choices: [\n          { title: 'GitHub', value: ProviderType.GITHUB.toString() },\n          { title: 'BitBucket', value: ProviderType.BITBUCKET.toString() },\n          { title: 'Github Enterprise Server', value: ProviderType.GITHUB_ENTERPRISE_SERVER.toString() },\n          { title: 'CodeCommit', value: 'codecommit' },\n      ],\n  }) as unknown as string;\n  return source;\n}\n\nasync function promptExternalSourceParamters(source :string) {\n  const externalSourcePrompts= [\n    {\n        type: 'text',\n        name:'profile',\n        message: `Enter your AWS CLI profile name or ( Press ENTER to choose toolchain as default profile )`,\n        default: 'toolchain'\n    },\n    {\n        type: 'text',\n        name: 'owner',\n        message: `Enter ${source} owner`,\n    },\n    {\n        type: 'text',\n        name: 'repositoryName',\n        message: `Enter ${source} repository name`,\n    },\n    {\n        type: 'text',\n        name: 'trunkBranchName',\n        message: `Enter ${source} trunk branch name`,\n    }\n  ]\n\n  const response= prompt(externalSourcePrompts);\n  return response\n}\n\nasync function setupCodeConnection( source: String, repoParameters: any){\n    const client = new CodeConnectionsClient({ region: process.env.AWS_REGION ,profile:repoParameters.profile });\n    let input = {\n        ProviderType: source as ProviderType,\n        ConnectionName: `dpri-${source}-${repoParameters.owner}`,\n    };\n    const command = new CreateConnectionCommand(input);\n    const response = await client.send(command);\n    return response\n}\n\nasync function checkForCodeConnectionAvailable(\n    repoParameters: any,\n    maxRetries: number = 30,\n    delay: number = 10,\n    connectionArn:any\n\n): Promise&lt;boolean&gt; {\n    console.log(\"\\n Waiting for connection to become available ...\")\n    console.log (\"\\n Please complete the authorization in the console when prompted\")\n\n    const client = new CodeConnectionsClient({ region: process.env.AWS_REGION ,profile:repoParameters.profile });\n\n    for ( let attempt = 0; attempt &lt; maxRetries; attempt ++ ){\n        try{\n            const getConnectionCommand = new GetConnectionCommand({\n                ConnectionArn: connectionArn\n            });\n            const response = await client.send(getConnectionCommand);\n            const status = response.Connection?.ConnectionStatus;\n\n            if (response.Connection?.ConnectionStatus === \"AVAILABLE\") {\n                console.log(\"\\n Connection is available\");\n                return true;\n            }\n            else if (response.Connection?.ConnectionStatus == \"ERROR\"){\n                console.log(\"\\n Connection is not available yet. Retrying ...\")\n                await new Promise(resolve =&gt; setTimeout(resolve, delay * 1000));\n\n            }\n            console.log(`\\n\u23f3 Current status: ${status} (Attempt ${attempt + 1}/${maxRetries})`);\n            await new Promise(resolve =&gt; setTimeout(resolve, delay * 1000));\n\n        }catch(error){\n            console.error(`Error checking connection status: ${error}`);\n            return false;\n        }\n    } \n    throw new Error(`Connection not available after ${maxRetries} attempts`);\n}\n\n// create function to read cdk.json and add update context values as per prompt response\nfunction updateCdkJson(repoParameters: any) {\n    const fs = require('fs');\n    const cdkJson = JSON.parse(fs.readFileSync('cdk.json', 'utf8'));\n    cdkJson.context.owner = repoParameters.owner;\n    cdkJson.context.repositoryName = repoParameters.repositoryName;\n    cdkJson.context.trunkBranchName = repoParameters.trunkBranchName;\n    cdkJson.context.connectionArn = repoParameters.connectionArn;\n    cdkJson.context.providerType = repoParameters.providerType;\n    fs.writeFileSync('cdk.json', JSON.stringify(cdkJson, null, 2));\n\n}\n\nmain().catch(console.error);\n</code></pre> Test Source Code <p>The test source code can be found in the src/test/java directory. It is intended to serve only as a reference and should be replaced by your own test source code.</p> <p>The reference implementation includes source code for unit, integration and end-to-end testing. Unit and integration tests can be found in <code>src/test/java</code>. For example, <code>FruitControllerWithoutClassificationTest.java</code> performs unit tests of each API path with the JUnit testing library:</p> <pre><code>public void shouldReturnList() throws Exception {\n  when(repository.findAll()).thenReturn(Arrays.asList(new Fruit(\"Mango\", FruitClassification.pome), new Fruit(\"Dragonfruit\", FruitClassification.berry)));\n\n  this.mockMvc.perform(get(\"/api/fruits\")).andDo(print()).andExpect(status().isOk())\n      .andExpect(content().json(\"[{\\\"name\\\": \\\"Mango\\\"}, {\\\"name\\\": \\\"Dragonfruit\\\"}]\"));\n}\n</code></pre> <p>Acceptance tests are preformed with SoapUI and are defined in <code>fruit-api-soapui-project.xml</code>. They are executed by Maven using plugins in <code>pom.xml</code>.</p> Infrastructure Source Code <p>The infrastructure source code can be found in the infrastructure directory. It is intended to serve as a reference but much of the code can also be reused in your own CDK applications.</p> <p>Infrastructure source code defines both the deployment of the pipeline and the deployment of the application are stored in <code>infrastructure/</code> folder and uses AWS Cloud Development Kit.</p> <pre><code>super(scope, id, props);\n\nconst image = new AssetImage('.', { target: 'build' });\n\nconst appName = Stack.of(this)\n  .stackName.toLowerCase()\n  .replace(`-${Stack.of(this).region}-`, '-');\nconst vpc = new ec2.Vpc(this, 'Vpc', {\n  maxAzs: 3,\n  natGateways: props?.natGateways,\n});\nnew FlowLog(this, 'VpcFlowLog', {\n  resourceType: FlowLogResourceType.fromVpc(vpc),\n});\n\nconst dbName = 'fruits';\nconst dbSecret = new DatabaseSecret(this, 'AuroraSecret', {\n  username: 'fruitapi',\n  secretName: `${appName}-DB`,\n});\n\nconst db = new DatabaseCluster(this, 'Database', {\n  engine: DatabaseClusterEngine.auroraMysql({\n    version: AuroraMysqlEngineVersion.VER_3_07_1,\n  }),\n  credentials: Credentials.fromSecret(dbSecret),\n  writer: ClusterInstance.serverlessV2('writer'),\n  defaultDatabaseName: dbName,\n  serverlessV2MaxCapacity: 2,\n  serverlessV2MinCapacity: 0.5,\n  vpc,\n  clusterIdentifier: appName,\n  storageEncrypted: true,\n});\n\nconst cluster = new ecs.Cluster(this, 'Cluster', {\n  vpc,\n  containerInsights: true,\n  clusterName: appName,\n});\nconst appLogGroup = new LogGroup(this, 'AppLogGroup', {\n  retention: RetentionDays.ONE_WEEK,\n  logGroupName: `/aws/ecs/service/${appName}`,\n  removalPolicy: RemovalPolicy.DESTROY,\n});\nlet deploymentConfig: IEcsDeploymentConfig | undefined = undefined;\nif (props?.deploymentConfigName) {\n  deploymentConfig = EcsDeploymentConfig.fromEcsDeploymentConfigName(\n    this,\n    'DeploymentConfig',\n    props.deploymentConfigName,\n  );\n}\nconst appConfigEnabled =\n  props?.appConfigRoleArn !== undefined &amp;&amp;\n  props.appConfigRoleArn.length &gt; 0;\nconst service = new ApplicationLoadBalancedCodeDeployedFargateService(\n  this,\n  'Api',\n  {\n    cluster,\n    capacityProviderStrategies: [\n      {\n        capacityProvider: 'FARGATE_SPOT',\n        weight: 1,\n      },\n    ],\n    minHealthyPercent: 50,\n    maxHealthyPercent: 200,\n    desiredCount: 3,\n    cpu: 512,\n    memoryLimitMiB: 1024,\n    taskImageOptions: {\n      image,\n      containerName: 'api',\n      containerPort: 8080,\n      family: appName,\n      logDriver: AwsLogDriver.awsLogs({\n        logGroup: appLogGroup,\n        streamPrefix: 'service',\n      }),\n      secrets: {\n        SPRING_DATASOURCE_USERNAME: Secret.fromSecretsManager(\n          dbSecret,\n          'username',\n        ),\n        SPRING_DATASOURCE_PASSWORD: Secret.fromSecretsManager(\n          dbSecret,\n          'password',\n        ),\n      },\n      environment: {\n        SPRING_DATASOURCE_URL: `jdbc:mysql://${db.clusterEndpoint.hostname}:${db.clusterEndpoint.port}/${dbName}`,\n        APPCONFIG_AGENT_APPLICATION:\n          this.node.tryGetContext('workloadName'),\n        APPCONFIG_AGENT_ENVIRONMENT:\n          this.node.tryGetContext('environmentName'),\n        APPCONFIG_AGENT_ENABLED: appConfigEnabled.toString(),\n      },\n    },\n    deregistrationDelay: Duration.seconds(5),\n    responseTimeAlarmThreshold: Duration.seconds(3),\n    targetHealthCheck: {\n      healthyThresholdCount: 2,\n      unhealthyThresholdCount: 2,\n      interval: Duration.seconds(60),\n      path: '/actuator/health',\n    },\n    deploymentConfig,\n    terminationWaitTime: Duration.minutes(5),\n    apiCanaryTimeout: Duration.seconds(5),\n    apiTestSteps: [\n      {\n        name: 'getAll',\n        path: '/api/fruits',\n        jmesPath: 'length(@)',\n        expectedValue: 5,\n      },\n    ],\n  },\n);\n\nif (appConfigEnabled) {\n  service.taskDefinition.addContainer('appconfig-agent', {\n    image: ecs.ContainerImage.fromRegistry(\n      'public.ecr.aws/aws-appconfig/aws-appconfig-agent:2.x',\n    ),\n    essential: false,\n    logging: AwsLogDriver.awsLogs({\n      logGroup: appLogGroup,\n      streamPrefix: 'service',\n    }),\n    environment: {\n      SERVICE_REGION: this.region,\n      ROLE_ARN: props!.appConfigRoleArn!,\n      ROLE_SESSION_NAME: appName,\n      LOG_LEVEL: 'info',\n    },\n    portMappings: [{ containerPort: 2772 }],\n  });\n\n  service.taskDefinition.addToTaskRolePolicy(\n    new PolicyStatement({\n      actions: ['sts:AssumeRole'],\n      resources: [props!.appConfigRoleArn!],\n    }),\n  );\n}\n\nservice.service.connections.allowTo(\n  db,\n  ec2.Port.tcp(db.clusterEndpoint.port),\n);\n\nthis.apiUrl = new CfnOutput(this, 'endpointUrl', {\n  value: `http://${service.listener.loadBalancer.loadBalancerDnsName}`,\n});\n</code></pre> <p>Notice that the infrastructure code is written in Typescript which is different from the Application Source Code (Java). This was done intentionally to demonstrate that CDK allows defining infrastructure code in whatever language is most appropriate for the team that owns the use of CDK in the organization.</p> Static Assets <p>There are no static assets used by the sample application.</p> Dependency Manifests <p>All third-party dependencies used by the sample application are define in the <code>pom.xml</code>:</p> <pre><code>&lt;dependencies&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n        &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;\n    &lt;/dependency&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n        &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt;\n    &lt;/dependency&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n        &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;\n    &lt;/dependency&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;org.liquibase&lt;/groupId&gt;\n        &lt;artifactId&gt;liquibase-core&lt;/artifactId&gt;\n    &lt;/dependency&gt;\n&lt;/dependencies&gt;\n</code></pre> Static Configuration <p>Static configuration for the application is defined in <code>src/main/resources/application.yml</code>:</p> <pre><code>spring:\n  application:\n    name: fruit-api\n  main:\n    banner-mode: \"off\"\n  jackson:\n    default-property-inclusion: non_null\n\n\nspringdoc:\n  swagger-ui:\n    path: /swagger-ui\n\nappconfig-agent:\n  environment: alpha\n  log-level-from:\n    configuration: operations\n</code></pre> Database Source Code <p>The database source code can be found in the src/main/resources/db directory. It is intended to serve only as a reference and should be replaced by your own database source code.</p> <p>The code that manages the schema and initial data for the application is defined using Liquibase in <code>src/main/resources/db/changelog/db.changelog-master.yml</code>:</p> <pre><code>databaseChangeLog:\n   - changeSet:\n       id: \"1\"\n       author: AWS\n       changes:\n       - createTable:\n           tableName: fruit\n           columns:\n           - column:\n               name: id\n               type: bigint\n               autoIncrement: true\n               constraints:\n                   primaryKey:  true\n                   nullable:  false\n           - column:\n               name: name\n               type: varchar(250)\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Apple\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Orange\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Banana\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Cherry\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Grape\n\n   - changeSet:\n       id: \"2\"\n       author: AWS\n       changes:\n       - addColumn:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               type: varchar(250)\n               constraints:\n                 nullable: true\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: pome\n           where: name='Apple'\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: berry\n           where: name='Orange'\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: berry\n           where: name='Banana'\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: drupe\n           where: name='Cherry'\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: berry\n           where: name='Grape'\n</code></pre>"},{"location":"application-pipeline/ri-cdk-pipeline/#build","title":"Build","text":"<p>Actions in this stage all run in less than 10 minutes so that developers can take action on fast feedback before moving on to their next task. Each of the actions below are defined as code with AWS Cloud Development Kit.</p> Build Code <p>The Java source code is compiled, unit tested and packaged by Maven. A step is added to the pipeline through a CDK construct called <code>MavenBuild</code>:</p> <pre><code>const stepProps = {\n  input: props.source,\n  commands: [],\n  buildEnvironment: {\n    buildImage: LinuxBuildImage.STANDARD_6_0,\n  },\n  partialBuildSpec: BuildSpec.fromObject({\n    env: {\n      variables: {\n        MAVEN_OPTS: props.mavenOpts || '-XX:+TieredCompilation -XX:TieredStopAtLevel=1',\n        MAVEN_ARGS: props.mavenArgs || '--batch-mode --no-transfer-progress',\n      },\n    },\n    phases: {\n      install: {\n        'runtime-versions': {\n          java: (props.javaRuntime || 'corretto17'),\n        },\n      },\n      build: {\n        commands: [`mvn \\${MAVEN_ARGS} clean ${props.mavenGoal || 'verify'}`],\n      },\n    },\n    cache: props.cacheBucket ? {\n      paths: ['/root/.m2/**/*'],\n    } : undefined,\n    reports: {\n      unit: {\n        'files': ['target/surefire-reports/*.xml'],\n        'file-format': 'JUNITXML',\n      },\n      integration: {\n        'files': ['target/soapui-reports/*.xml'],\n        'file-format': 'JUNITXML',\n      },\n    },\n    version: '0.2',\n  }),\n  cache: props.cacheBucket ? Cache.bucket(props.cacheBucket) : undefined,\n  primaryOutputDirectory: '.',\n};\nsuper(id, stepProps);\n</code></pre> Unit Tests <p>The unit tests are run by Maven at the same time the <code>Build Code</code> action occurs. The results of the unit tests are uploaded to AWS Code Build Test Reports to track over time.</p> <p></p> Code Quality <p>A CDK construct was created to require that Amazon CodeGuru performed a review on the most recent changes and that the recommendations don't exceed the severity thresholds. If no review was found or if the severity thresholds were exceeded, the pipeline fails. The construct is added to the pipeline with:</p> <pre><code>import { CodeGuruReviewCheck, CodeGuruReviewFilter } from './codeguru-review-check';\n\n\u22ef\n\n    const codeGuruSecurity = new CodeGuruReviewCheck('CodeGuruSecurity', {\n      source: pipelineSource,\n      reviewRequired: false,\n      filter: CodeGuruReviewFilter.defaultCodeSecurityFilter(),\n    });\n    const codeGuruQuality = new CodeGuruReviewCheck('CodeGuruQuality', {\n      source: pipelineSource,\n      reviewRequired: false,\n      filter: CodeGuruReviewFilter.defaultCodeQualityFilter(),\n    });\n</code></pre> <p>The <code>Filter</code> attribute can be customized to control what categories of recommendations are considered and what the thresholds are:</p> <pre><code>export enum CodeGuruReviewRecommendationCategory {\n    AWS_BEST_PRACTICES = 'AWSBestPractices',\n    AWS_CLOUDFORMATION_ISSUES = 'AWSCloudFormationIssues',\n    CODE_INCONSISTENCIES = 'CodeInconsistencies',\n    CODE_MAINTENANCE_ISSUES = 'CodeMaintenanceIssues',\n    CONCURRENCY_ISSUES = 'ConcurrencyIssues',\n    DUPLICATE_CODE = 'DuplicateCode',\n    INPUT_VALIDATIONS = 'InputValidations',\n    JAVA_BEST_PRACTICES = 'JavaBestPractices',\n    PYTHON_BEST_PRACTICES = 'PythonBestPractices',\n    RESOURCE_LEAKS = 'ResourceLeaks',\n    SECURITY_ISSUES = 'SecurityIssues',\n}\nexport class CodeGuruReviewFilter {\n    // Limit which recommendation categories to include\n    recommendationCategories!: CodeGuruReviewRecommendationCategory[];\n\n    // Fail if more that this # of lines of code were suppressed aws-codeguru-reviewer.yml\n    maxSuppressedLinesOfCodeCount?: number;\n\n    // Fail if more than this # of CRITICAL recommendations were found\n    maxCriticalRecommendations?: number;\n\n    // Fail if more than this # of HIGH recommendations were found\n    maxHighRecommendations?: number;\n\n    // Fail if more than this # of MEDIUM recommendations were found\n    maxMediumRecommendations?: number;\n\n    // Fail if more than this # of INFO recommendations were found\n    maxInfoRecommendations?: number;\n\n    // Fail if more than this # of LOW recommendations were found\n    maxLowRecommendations?: number;\n}\n</code></pre> <p></p> <p>Additionally, cdk-nag is run against both the pipeline stack and the deployment stack to identify any security issues with the resources being created. The pipeline will fail if any are detected. The following code demonstrates how cdk-nag is called as a part of the build stage. The code also demonstrates how to suppress findings.</p> <pre><code>import { App, Aspects } from 'aws-cdk-lib';\nimport { Annotations, Match, Template } from 'aws-cdk-lib/assertions';\nimport { SynthesisMessage } from 'aws-cdk-lib/cx-api';\nimport { AwsSolutionsChecks, NagSuppressions } from 'cdk-nag';\nimport { DeploymentStack } from '../src/deployment';\n\n\nfunction synthesisMessageToString(sm: SynthesisMessage): string {\n  return `${sm.entry.data} [${sm.id}]`;\n}\nexpect.addSnapshotSerializer({\n  test: (val) =&gt; typeof val === 'string' &amp;&amp; val.match(/^dummy.dkr.ecr.us-east.1/) !== null,\n  serialize: () =&gt; '\"dummy-ecr-image\"',\n});\nexpect.addSnapshotSerializer({\n  test: (val) =&gt; typeof val === 'string' &amp;&amp; val.match(/^[a-f0-9]+\\.zip$/) !== null,\n  serialize: () =&gt; '\"code.zip\"',\n});\n\ndescribe('cdk-nag', () =&gt; {\n  let stack: DeploymentStack;\n  let app: App;\n\n  beforeAll(() =&gt; {\n    const appName = 'fruit-api';\n    const workloadName = 'food';\n    const environmentName = 'unit-test';\n    app = new App({ context: { appName, environmentName, workloadName } });\n    stack = new DeploymentStack(app, 'TestStack', {\n      env: {\n        account: 'dummy',\n        region: 'us-east-1',\n      },\n    });\n    Aspects.of(stack).add(new AwsSolutionsChecks());\n\n    // Suppress CDK-NAG for TaskDefinition role and ecr:GetAuthorizationToken permission\n    NagSuppressions.addResourceSuppressionsByPath(\n      stack,\n      `/${stack.stackName}/Api/TaskDef/ExecutionRole/DefaultPolicy/Resource`,\n      [{ id: 'AwsSolutions-IAM5', reason: 'Allow ecr:GetAuthorizationToken', appliesTo: ['Resource::*'] }],\n    );\n\n    // Suppress CDK-NAG for secret rotation\n    NagSuppressions.addResourceSuppressionsByPath(\n      stack,\n      `/${stack.stackName}/AuroraSecret/Resource`,\n      [{ id: 'AwsSolutions-SMG4', reason: 'Dont require secret rotation' }],\n    );\n\n    // Suppress CDK-NAG for RDS Serverless\n    NagSuppressions.addResourceSuppressionsByPath(\n      stack,\n      `/${stack.stackName}/Database/Resource`,\n      [\n        { id: 'AwsSolutions-RDS6', reason: 'IAM authentication not supported on Serverless v1' },\n        { id: 'AwsSolutions-RDS10', reason: 'Disable delete protection to simplify cleanup of Reference Implementation' },\n        { id: 'AwsSolutions-RDS11', reason: 'Custom port not supported on Serverless v1' },\n        { id: 'AwsSolutions-RDS14', reason: 'Backtrack not supported on Serverless v1' },\n        { id: 'AwsSolutions-RDS16', reason: 'CloudWatch Log Export not supported on Serverless v1' },\n      ],\n    );\n\n    NagSuppressions.addResourceSuppressionsByPath(stack, [\n      `/${stack.stackName}/Api/DeploymentGroup/Deployment/DeploymentProvider/framework-onEvent`,\n      `/${stack.stackName}/Api/DeploymentGroup/Deployment/DeploymentProvider/framework-isComplete`,\n      `/${stack.stackName}/Api/DeploymentGroup/Deployment/DeploymentProvider/framework-onTimeout`,\n      `/${stack.stackName}/Api/DeploymentGroup/Deployment/DeploymentProvider/waiter-state-machine`,\n    ], [\n      { id: 'AwsSolutions-IAM5', reason: 'Unrelated to construct under test' },\n      { id: 'AwsSolutions-L1', reason: 'Unrelated to construct under test' },\n      { id: 'AwsSolutions-SF1', reason: 'Unrelated to construct under test' },\n      { id: 'AwsSolutions-SF2', reason: 'Unrelated to construct under test' },\n    ], true);\n\n    // Ignore findings from access log bucket\n    NagSuppressions.addResourceSuppressionsByPath(stack, [\n      `/${stack.stackName}/Api/AccessLogBucket`,\n    ], [\n      { id: 'AwsSolutions-S1', reason: 'Dont need access logs for access log bucket' },\n      { id: 'AwsSolutions-IAM5', reason: 'Allow resource:*', appliesTo: ['Resource::*'] },\n    ]);\n\n    NagSuppressions.addResourceSuppressionsByPath(stack, [\n      `/${stack.stackName}/Api/Canary/ServiceRole`,\n    ], [{ id: 'AwsSolutions-IAM5', reason: 'Allow resource:*' }]);\n\n    NagSuppressions.addResourceSuppressionsByPath(stack, [\n      `/${stack.stackName}/Api/CanaryArtifactsBucket`,\n    ], [{ id: 'AwsSolutions-S1', reason: 'Dont need access logs for canary bucket' }]);\n\n    NagSuppressions.addResourceSuppressionsByPath(stack, [\n      `/${stack.stackName}/Api/DeploymentGroup/ServiceRole`,\n    ], [\n      { id: 'AwsSolutions-IAM4', reason: 'Allow AWSCodeDeployRoleForECS policy', appliesTo: ['Policy::arn:&lt;AWS::Partition&gt;:iam::aws:policy/AWSCodeDeployRoleForECS'] },\n    ]);\n\n    NagSuppressions.addResourceSuppressionsByPath(stack, [\n      `/${stack.stackName}/Api/DeploymentGroup/Deployment`,\n    ], [\n      {\n        id: 'AwsSolutions-IAM4',\n        reason: 'Allow AWSLambdaBasicExecutionRole policy',\n        appliesTo: ['Policy::arn:&lt;AWS::Partition&gt;:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole'],\n      },\n    ], true);\n\n    NagSuppressions.addResourceSuppressionsByPath(stack, [\n      `/${stack.stackName}/Api/TaskDef`,\n    ], [\n      {\n        id: 'AwsSolutions-ECS2',\n        reason: 'Allow environment variables for configuration of values that are not confidential',\n      },\n    ]);\n\n    NagSuppressions.addResourceSuppressionsByPath(stack, [\n      `/${stack.stackName}/Api/LB/SecurityGroup`,\n    ], [\n      {\n        id: 'AwsSolutions-EC23',\n        reason: 'Allow public inbound access on ELB',\n      },\n    ]);\n  });\n\n  test('Snapshot', () =&gt; {\n    const template = Template.fromStack(stack);\n    expect(template.toJSON()).toMatchSnapshot();\n  });\n\n  test('cdk-nag AwsSolutions Pack errors', () =&gt; {\n    const errors = Annotations.fromStack(stack).findError(\n      '*',\n      Match.stringLikeRegexp('AwsSolutions-.*'),\n    ).map(synthesisMessageToString);\n    expect(errors).toHaveLength(2);\n  });\n\n  test('cdk-nag AwsSolutions Pack warnings', () =&gt; {\n    const warnings = Annotations.fromStack(stack).findWarning(\n      '*',\n      Match.stringLikeRegexp('AwsSolutions-.*'),\n    ).map(synthesisMessageToString);\n    expect(warnings).toHaveLength(0);\n  });\n});\n\ndescribe('Deployment without AppConfig', () =&gt; {\n  let stack: DeploymentStack;\n  let app: App;\n\n  beforeAll(() =&gt; {\n    const appName = 'fruit-api';\n    const environmentName = 'unit-test';\n    app = new App({ context: { appName, environmentName } });\n    stack = new DeploymentStack(app, 'TestStack', {\n      env: {\n        account: 'dummy',\n        region: 'us-east-1',\n      },\n    });\n  });\n\n  test('Snapshot', () =&gt; {\n    const template = Template.fromStack(stack);\n    expect(template.toJSON()).toMatchSnapshot();\n  });\n  test('taskdef', () =&gt; {\n    const template = Template.fromStack(stack);\n    template.hasResourceProperties('AWS::ECS::TaskDefinition', {\n      ContainerDefinitions: [\n        {\n          Environment: [{\n            Name: 'SPRING_DATASOURCE_URL',\n          }, {\n            Name: 'APPCONFIG_AGENT_APPLICATION',\n          }, {\n            Name: 'APPCONFIG_AGENT_ENVIRONMENT',\n            Value: 'unit-test',\n          }, {\n            Name: 'APPCONFIG_AGENT_ENABLED',\n            Value: 'false',\n          }],\n        },\n      ],\n    });\n  });\n});\n\ndescribe('Deployment with AppConfig', () =&gt; {\n  let stack: DeploymentStack;\n  let app: App;\n\n  beforeAll(() =&gt; {\n    const appName = 'fruit-api';\n    const workloadName = 'food';\n    const environmentName = 'unit-test';\n    app = new App({ context: { appName, environmentName, workloadName } });\n    stack = new DeploymentStack(app, 'TestStack', {\n      appConfigRoleArn: 'dummy-role-arn',\n      env: {\n        account: 'dummy',\n        region: 'us-east-1',\n      },\n    });\n  });\n\n  test('Snapshot', () =&gt; {\n    const template = Template.fromStack(stack);\n    expect(template.toJSON()).toMatchSnapshot();\n  });\n  test('taskdef', () =&gt; {\n    const template = Template.fromStack(stack);\n    template.hasResourceProperties('AWS::ECS::TaskDefinition', {\n      ContainerDefinitions: [\n        {\n          Environment: [{\n            Name: 'SPRING_DATASOURCE_URL',\n          }, {\n            Name: 'APPCONFIG_AGENT_APPLICATION',\n            Value: 'food',\n          }, {\n            Name: 'APPCONFIG_AGENT_ENVIRONMENT',\n            Value: 'unit-test',\n          }, {\n            Name: 'APPCONFIG_AGENT_ENABLED',\n            Value: 'true',\n          }],\n        },\n        {\n          Environment: [{\n            Name: 'SERVICE_REGION',\n            Value: 'us-east-1',\n          }, {\n            Name: 'ROLE_ARN',\n            Value: 'dummy-role-arn',\n          }, {\n            Name: 'ROLE_SESSION_NAME',\n          }, {\n            Name: 'LOG_LEVEL',\n            Value: 'info',\n          }],\n        },\n      ],\n    });\n  });\n});\n</code></pre> Secrets Detection <p>The same CDK construct that was created for Code Quality above is also used for secrets detection with Amazon CodeGuru.</p> Static Application Security Testing (SAST) <p>The same CDK construct that was created for Code Quality above is also used for SAST with Amazon CodeGuru.</p> Package and Store Artifact(s) <p>AWS Cloud Development Kit handles the packaging and storing of assets during the <code>Synth</code> action and <code>Assets</code> stage. The <code>Synth</code> action generates the CloudFormation templates to be deployed into the subsequent environments along with staging up the files necessary to create a docker image. The <code>Assets</code> stage then performs the docker build step to create a new image and push the image to Amazon ECR repositories in each environment account.</p> <p></p> Software Composition Analysis (SCA) <p>Trivy is used to scan the source for vulnerabilities in its dependencies. The <code>pom.xml</code> and <code>Dockerfile</code> files are scanned for configuration issues or vulnerabilities in any dependencies. The scanning is accomplished by a CDK construct that creates a CodeBuild job to run <code>trivy</code>:</p> <pre><code>import { TrivyScan } from './trivy-scan';\n\n\u22ef\n\n    const trivyScan = new TrivyScan('TrivyScan', {\n      source: pipelineSource,\n      severity: ['CRITICAL', 'HIGH'],\n      checks: ['misconfig'],\n    });\n</code></pre> <p>Trivy is also used within the <code>Dockerfile</code> to scan the image after it is built. The <code>docker build</code> will fail if Trivy finds any vulnerabilities in the final image:</p> <pre><code>FROM public.ecr.aws/amazoncorretto/amazoncorretto:17-al2022-jdk as build\nUSER nobody\nWORKDIR /app\nCOPY target/fruit-api.jar /app\nHEALTHCHECK --interval=30s --timeout=5s --start-period=30s --retries=3 CMD /bin/curl --fail --silent localhost:8080/actuator/health | grep UP || exit 1\nENTRYPOINT [\"java\",\"-jar\",\"/app/fruit-api.jar\"]\n\n# Use multi-stage builds to scan newly created image with Trivy. This second stage 'vulnscan'\n# isn't published to Amazon ECR and is never run. It is only used to run the Trivy scan\n# against the newly created image in the 'build' stage.\n#\n# This stage must run as root so Trivy can scan all files in the image, not just\n# those accessible by the nobody user. The user is switched back to 'nobody' at\n# the end to ensure that even if this image is used for something it is done\n# without the 'root' user.\n\nFROM build AS vulnscan\nUSER root\nCOPY --from=aquasec/trivy:latest /usr/local/bin/trivy /usr/local/bin/trivy\nRUN trivy filesystem --exit-code 1 --no-progress --ignore-unfixed -s CRITICAL /\nUSER nobody\n</code></pre> Software Bill of Materials (SBOM) <p>Trivy generates an SBOM in the form of a CycloneDX JSON report. The SBOM is saved as a CodePipeline asset.  Trivy supports additional SBOM formats such as SPDX, and SARIF.</p>"},{"location":"application-pipeline/ri-cdk-pipeline/#test-beta","title":"Test (Beta)","text":"Launch Environment <p>The infrastructure for each environment is defined in AWS Cloud Development Kit:</p> <pre><code>super(scope, id, props);\n\nconst image = new AssetImage('.', { target: 'build' });\n\nconst appName = Stack.of(this)\n  .stackName.toLowerCase()\n  .replace(`-${Stack.of(this).region}-`, '-');\nconst vpc = new ec2.Vpc(this, 'Vpc', {\n  maxAzs: 3,\n  natGateways: props?.natGateways,\n});\nnew FlowLog(this, 'VpcFlowLog', {\n  resourceType: FlowLogResourceType.fromVpc(vpc),\n});\n\nconst dbName = 'fruits';\nconst dbSecret = new DatabaseSecret(this, 'AuroraSecret', {\n  username: 'fruitapi',\n  secretName: `${appName}-DB`,\n});\n\nconst db = new DatabaseCluster(this, 'Database', {\n  engine: DatabaseClusterEngine.auroraMysql({\n    version: AuroraMysqlEngineVersion.VER_3_07_1,\n  }),\n  credentials: Credentials.fromSecret(dbSecret),\n  writer: ClusterInstance.serverlessV2('writer'),\n  defaultDatabaseName: dbName,\n  serverlessV2MaxCapacity: 2,\n  serverlessV2MinCapacity: 0.5,\n  vpc,\n  clusterIdentifier: appName,\n  storageEncrypted: true,\n});\n\nconst cluster = new ecs.Cluster(this, 'Cluster', {\n  vpc,\n  containerInsights: true,\n  clusterName: appName,\n});\nconst appLogGroup = new LogGroup(this, 'AppLogGroup', {\n  retention: RetentionDays.ONE_WEEK,\n  logGroupName: `/aws/ecs/service/${appName}`,\n  removalPolicy: RemovalPolicy.DESTROY,\n});\nlet deploymentConfig: IEcsDeploymentConfig | undefined = undefined;\nif (props?.deploymentConfigName) {\n  deploymentConfig = EcsDeploymentConfig.fromEcsDeploymentConfigName(\n    this,\n    'DeploymentConfig',\n    props.deploymentConfigName,\n  );\n}\nconst appConfigEnabled =\n  props?.appConfigRoleArn !== undefined &amp;&amp;\n  props.appConfigRoleArn.length &gt; 0;\nconst service = new ApplicationLoadBalancedCodeDeployedFargateService(\n  this,\n  'Api',\n  {\n    cluster,\n    capacityProviderStrategies: [\n      {\n        capacityProvider: 'FARGATE_SPOT',\n        weight: 1,\n      },\n    ],\n    minHealthyPercent: 50,\n    maxHealthyPercent: 200,\n    desiredCount: 3,\n    cpu: 512,\n    memoryLimitMiB: 1024,\n    taskImageOptions: {\n      image,\n      containerName: 'api',\n      containerPort: 8080,\n      family: appName,\n      logDriver: AwsLogDriver.awsLogs({\n        logGroup: appLogGroup,\n        streamPrefix: 'service',\n      }),\n      secrets: {\n        SPRING_DATASOURCE_USERNAME: Secret.fromSecretsManager(\n          dbSecret,\n          'username',\n        ),\n        SPRING_DATASOURCE_PASSWORD: Secret.fromSecretsManager(\n          dbSecret,\n          'password',\n        ),\n      },\n      environment: {\n        SPRING_DATASOURCE_URL: `jdbc:mysql://${db.clusterEndpoint.hostname}:${db.clusterEndpoint.port}/${dbName}`,\n        APPCONFIG_AGENT_APPLICATION:\n          this.node.tryGetContext('workloadName'),\n        APPCONFIG_AGENT_ENVIRONMENT:\n          this.node.tryGetContext('environmentName'),\n        APPCONFIG_AGENT_ENABLED: appConfigEnabled.toString(),\n      },\n    },\n    deregistrationDelay: Duration.seconds(5),\n    responseTimeAlarmThreshold: Duration.seconds(3),\n    targetHealthCheck: {\n      healthyThresholdCount: 2,\n      unhealthyThresholdCount: 2,\n      interval: Duration.seconds(60),\n      path: '/actuator/health',\n    },\n    deploymentConfig,\n    terminationWaitTime: Duration.minutes(5),\n    apiCanaryTimeout: Duration.seconds(5),\n    apiTestSteps: [\n      {\n        name: 'getAll',\n        path: '/api/fruits',\n        jmesPath: 'length(@)',\n        expectedValue: 5,\n      },\n    ],\n  },\n);\n\nif (appConfigEnabled) {\n  service.taskDefinition.addContainer('appconfig-agent', {\n    image: ecs.ContainerImage.fromRegistry(\n      'public.ecr.aws/aws-appconfig/aws-appconfig-agent:2.x',\n    ),\n    essential: false,\n    logging: AwsLogDriver.awsLogs({\n      logGroup: appLogGroup,\n      streamPrefix: 'service',\n    }),\n    environment: {\n      SERVICE_REGION: this.region,\n      ROLE_ARN: props!.appConfigRoleArn!,\n      ROLE_SESSION_NAME: appName,\n      LOG_LEVEL: 'info',\n    },\n    portMappings: [{ containerPort: 2772 }],\n  });\n\n  service.taskDefinition.addToTaskRolePolicy(\n    new PolicyStatement({\n      actions: ['sts:AssumeRole'],\n      resources: [props!.appConfigRoleArn!],\n    }),\n  );\n}\n\nservice.service.connections.allowTo(\n  db,\n  ec2.Port.tcp(db.clusterEndpoint.port),\n);\n\nthis.apiUrl = new CfnOutput(this, 'endpointUrl', {\n  value: `http://${service.listener.loadBalancer.loadBalancerDnsName}`,\n});\n</code></pre> <p>The <code>DeploymentStack</code> construct is then instantiated for each environment:</p> <pre><code>export const Beta: EnvironmentConfig = {\n  name: 'Beta',\n  account: accounts.beta,\n  waves: [\n    ['us-west-2'],\n  ],\n};\n\n\u22ef\n\n    new PipelineEnvironment(pipeline, Beta, (deployment, stage) =&gt; {\n      stage.addPost(\n        new SoapUITest('E2E Test', {\n          source: pipelineSource,\n          endpoint: deployment.apiUrl,\n          cacheBucket,\n        }),\n      );\n    });\n</code></pre> Database Deploy <p>Spring Boot is configured to run Liquibase on startup. This reads the configuration in <code>src/main/resources/db/changelog/db.changelog-master.yml</code> to define the tables and initial data for the database:</p> <pre><code>databaseChangeLog:\n   - changeSet:\n       id: \"1\"\n       author: AWS\n       changes:\n       - createTable:\n           tableName: fruit\n           columns:\n           - column:\n               name: id\n               type: bigint\n               autoIncrement: true\n               constraints:\n                   primaryKey:  true\n                   nullable:  false\n           - column:\n               name: name\n               type: varchar(250)\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Apple\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Orange\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Banana\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Cherry\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Grape\n\n   - changeSet:\n       id: \"2\"\n       author: AWS\n       changes:\n       - addColumn:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               type: varchar(250)\n               constraints:\n                 nullable: true\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: pome\n           where: name='Apple'\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: berry\n           where: name='Orange'\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: berry\n           where: name='Banana'\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: drupe\n           where: name='Cherry'\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: berry\n           where: name='Grape'\n</code></pre> Deploy Software <p>The Launch Environment action above creates a new Amazon ECS Task Definition for the new docker image and then updates the Amazon ECS Service to use the new Task Definition.</p> Integration Tests <p>Integration tests are preformed during the Build Source action. They are defined with SoapUI in <code>fruit-api-soapui-project.xml</code>. They are executed by Maven in the <code>integration-test</code> phase using plugins in <code>pom.xml</code>.  Spring Boot is configure to start a local instance of the application with an H2 database during the <code>pre-integration-test</code> phase and then to terminate on the <code>post-integration-test</code> phase.  The results of the unit tests are uploaded to AWS Code Build Test Reports to track over time.</p> <pre><code>&lt;plugins&gt;\n    &lt;plugin&gt;\n        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n        &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;\n        &lt;executions&gt;\n            &lt;execution&gt;\n                &lt;id&gt;pre-integration-test&lt;/id&gt;\n                &lt;goals&gt;\n                    &lt;goal&gt;start&lt;/goal&gt;\n                &lt;/goals&gt;\n            &lt;/execution&gt;\n            &lt;execution&gt;\n                &lt;id&gt;post-integration-test&lt;/id&gt;\n                &lt;goals&gt;\n                    &lt;goal&gt;stop&lt;/goal&gt;\n                &lt;/goals&gt;\n            &lt;/execution&gt;\n        &lt;/executions&gt;\n    &lt;/plugin&gt;\n    &lt;plugin&gt;\n        &lt;groupId&gt;com.smartbear.soapui&lt;/groupId&gt;\n        &lt;artifactId&gt;soapui-maven-plugin&lt;/artifactId&gt;\n        &lt;version&gt;5.7.0&lt;/version&gt;\n        &lt;configuration&gt;\n            &lt;junitReport&gt;true&lt;/junitReport&gt;\n            &lt;outputFolder&gt;target/soapui-reports&lt;/outputFolder&gt;\n            &lt;endpoint&gt;${soapui.endpoint}&lt;/endpoint&gt;\n        &lt;/configuration&gt;\n        &lt;executions&gt;\n            &lt;execution&gt;\n                &lt;phase&gt;integration-test&lt;/phase&gt;\n                &lt;goals&gt;\n                    &lt;goal&gt;test&lt;/goal&gt;\n                &lt;/goals&gt;\n            &lt;/execution&gt;\n        &lt;/executions&gt;\n    &lt;/plugin&gt;\n&lt;/plugins&gt;\n</code></pre> Acceptance Tests <p>Acceptance tests are preformed after the Launch Environment and Deploy Software actions:</p> <p></p> <p>The tests are defined with SoapUI in <code>fruit-api-soapui-project.xml</code>. They are executed by Maven with the endpoint overridden to the URL from the CloudFormation output. A CDK construct called <code>SoapUITest</code> was created to create the CodeBuild Project to run SoapUI.</p> <pre><code>const stepProps = {\n  envFromCfnOutputs: {\n    ENDPOINT: props.endpoint,\n  },\n  input: props.source,\n  commands: [],\n  buildEnvironment: {\n    buildImage: LinuxBuildImage.STANDARD_6_0,\n  },\n  partialBuildSpec: BuildSpec.fromObject({\n    env: {\n      variables: {\n        MAVEN_OPTS: props.mavenOpts || '-XX:+TieredCompilation -XX:TieredStopAtLevel=1',\n        MAVEN_ARGS: props.mavenArgs || '--batch-mode --no-transfer-progress',\n      },\n    },\n    phases: {\n      install: {\n        'runtime-versions': {\n          java: (props.javaRuntime || 'corretto17'),\n        },\n      },\n      build: {\n        commands: ['mvn ${MAVEN_ARGS} soapui:test -Dsoapui.endpoint=${ENDPOINT}'],\n      },\n    },\n    cache: props.cacheBucket ? {\n      paths: ['/root/.m2/**/*'],\n    } : undefined,\n    reports: {\n      e2e: {\n        'files': ['target/soapui-reports/*.xml'],\n        'file-format': 'JUNITXML',\n      },\n    },\n    version: '0.2',\n  }),\n  cache: props.cacheBucket ? Cache.bucket(props.cacheBucket) : undefined,\n};\nsuper(id, stepProps);\n</code></pre> <p>The results of the unit tests are uploaded to AWS Code Build Test Reports to track over time.</p>"},{"location":"application-pipeline/ri-cdk-pipeline/#test-gamma","title":"Test (Gamma)","text":"Launch Environment <p>The infrastructure for each environment is defined in AWS Cloud Development Kit:</p> <pre><code>super(scope, id, props);\n\nconst image = new AssetImage('.', { target: 'build' });\n\nconst appName = Stack.of(this)\n  .stackName.toLowerCase()\n  .replace(`-${Stack.of(this).region}-`, '-');\nconst vpc = new ec2.Vpc(this, 'Vpc', {\n  maxAzs: 3,\n  natGateways: props?.natGateways,\n});\nnew FlowLog(this, 'VpcFlowLog', {\n  resourceType: FlowLogResourceType.fromVpc(vpc),\n});\n\nconst dbName = 'fruits';\nconst dbSecret = new DatabaseSecret(this, 'AuroraSecret', {\n  username: 'fruitapi',\n  secretName: `${appName}-DB`,\n});\n\nconst db = new DatabaseCluster(this, 'Database', {\n  engine: DatabaseClusterEngine.auroraMysql({\n    version: AuroraMysqlEngineVersion.VER_3_07_1,\n  }),\n  credentials: Credentials.fromSecret(dbSecret),\n  writer: ClusterInstance.serverlessV2('writer'),\n  defaultDatabaseName: dbName,\n  serverlessV2MaxCapacity: 2,\n  serverlessV2MinCapacity: 0.5,\n  vpc,\n  clusterIdentifier: appName,\n  storageEncrypted: true,\n});\n\nconst cluster = new ecs.Cluster(this, 'Cluster', {\n  vpc,\n  containerInsights: true,\n  clusterName: appName,\n});\nconst appLogGroup = new LogGroup(this, 'AppLogGroup', {\n  retention: RetentionDays.ONE_WEEK,\n  logGroupName: `/aws/ecs/service/${appName}`,\n  removalPolicy: RemovalPolicy.DESTROY,\n});\nlet deploymentConfig: IEcsDeploymentConfig | undefined = undefined;\nif (props?.deploymentConfigName) {\n  deploymentConfig = EcsDeploymentConfig.fromEcsDeploymentConfigName(\n    this,\n    'DeploymentConfig',\n    props.deploymentConfigName,\n  );\n}\nconst appConfigEnabled =\n  props?.appConfigRoleArn !== undefined &amp;&amp;\n  props.appConfigRoleArn.length &gt; 0;\nconst service = new ApplicationLoadBalancedCodeDeployedFargateService(\n  this,\n  'Api',\n  {\n    cluster,\n    capacityProviderStrategies: [\n      {\n        capacityProvider: 'FARGATE_SPOT',\n        weight: 1,\n      },\n    ],\n    minHealthyPercent: 50,\n    maxHealthyPercent: 200,\n    desiredCount: 3,\n    cpu: 512,\n    memoryLimitMiB: 1024,\n    taskImageOptions: {\n      image,\n      containerName: 'api',\n      containerPort: 8080,\n      family: appName,\n      logDriver: AwsLogDriver.awsLogs({\n        logGroup: appLogGroup,\n        streamPrefix: 'service',\n      }),\n      secrets: {\n        SPRING_DATASOURCE_USERNAME: Secret.fromSecretsManager(\n          dbSecret,\n          'username',\n        ),\n        SPRING_DATASOURCE_PASSWORD: Secret.fromSecretsManager(\n          dbSecret,\n          'password',\n        ),\n      },\n      environment: {\n        SPRING_DATASOURCE_URL: `jdbc:mysql://${db.clusterEndpoint.hostname}:${db.clusterEndpoint.port}/${dbName}`,\n        APPCONFIG_AGENT_APPLICATION:\n          this.node.tryGetContext('workloadName'),\n        APPCONFIG_AGENT_ENVIRONMENT:\n          this.node.tryGetContext('environmentName'),\n        APPCONFIG_AGENT_ENABLED: appConfigEnabled.toString(),\n      },\n    },\n    deregistrationDelay: Duration.seconds(5),\n    responseTimeAlarmThreshold: Duration.seconds(3),\n    targetHealthCheck: {\n      healthyThresholdCount: 2,\n      unhealthyThresholdCount: 2,\n      interval: Duration.seconds(60),\n      path: '/actuator/health',\n    },\n    deploymentConfig,\n    terminationWaitTime: Duration.minutes(5),\n    apiCanaryTimeout: Duration.seconds(5),\n    apiTestSteps: [\n      {\n        name: 'getAll',\n        path: '/api/fruits',\n        jmesPath: 'length(@)',\n        expectedValue: 5,\n      },\n    ],\n  },\n);\n\nif (appConfigEnabled) {\n  service.taskDefinition.addContainer('appconfig-agent', {\n    image: ecs.ContainerImage.fromRegistry(\n      'public.ecr.aws/aws-appconfig/aws-appconfig-agent:2.x',\n    ),\n    essential: false,\n    logging: AwsLogDriver.awsLogs({\n      logGroup: appLogGroup,\n      streamPrefix: 'service',\n    }),\n    environment: {\n      SERVICE_REGION: this.region,\n      ROLE_ARN: props!.appConfigRoleArn!,\n      ROLE_SESSION_NAME: appName,\n      LOG_LEVEL: 'info',\n    },\n    portMappings: [{ containerPort: 2772 }],\n  });\n\n  service.taskDefinition.addToTaskRolePolicy(\n    new PolicyStatement({\n      actions: ['sts:AssumeRole'],\n      resources: [props!.appConfigRoleArn!],\n    }),\n  );\n}\n\nservice.service.connections.allowTo(\n  db,\n  ec2.Port.tcp(db.clusterEndpoint.port),\n);\n\nthis.apiUrl = new CfnOutput(this, 'endpointUrl', {\n  value: `http://${service.listener.loadBalancer.loadBalancerDnsName}`,\n});\n</code></pre> <p>The <code>DeploymentStack</code> construct is then instantiated for each environment:</p> <pre><code>export const Gamma: EnvironmentConfig = {\n  name: 'Gamma',\n  account: accounts.gamma,\n  waves: [\n    ['us-west-2', 'us-east-1'],\n  ],\n};\n\n\u22ef\n\n    new PipelineEnvironment(pipeline, Gamma, (deployment, stage) =&gt; {\n      stage.addPost(\n        new JMeterTest('Performance Test', {\n          source: pipelineSource,\n          endpoint: deployment.apiUrl,\n          threads: 300,\n          duration: 300,\n          throughput: 6000,\n          cacheBucket,\n        }),\n      );\n    }, wave =&gt; {\n      wave.addPost(\n        new ManualApprovalStep('PromoteToProd'),\n      );\n    });\n</code></pre> Database Deploy <p>Spring Boot is configured to run Liquibase on startup. This reads the configuration in <code>src/main/resources/db/changelog/db.changelog-master.yml</code> to define the tables and initial data for the database:</p> <pre><code>databaseChangeLog:\n   - changeSet:\n       id: \"1\"\n       author: AWS\n       changes:\n       - createTable:\n           tableName: fruit\n           columns:\n           - column:\n               name: id\n               type: bigint\n               autoIncrement: true\n               constraints:\n                   primaryKey:  true\n                   nullable:  false\n           - column:\n               name: name\n               type: varchar(250)\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Apple\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Orange\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Banana\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Cherry\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Grape\n\n   - changeSet:\n       id: \"2\"\n       author: AWS\n       changes:\n       - addColumn:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               type: varchar(250)\n               constraints:\n                 nullable: true\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: pome\n           where: name='Apple'\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: berry\n           where: name='Orange'\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: berry\n           where: name='Banana'\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: drupe\n           where: name='Cherry'\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: berry\n           where: name='Grape'\n</code></pre> Deploy Software <p>The Launch Environment action above creates a new Amazon ECS Task Definition for the new docker image and then updates the Amazon ECS Service to use the new Task Definition.</p> Application Monitoring &amp; Logging <p>Amazon ECS uses Amazon CloudWatch Metrics and Amazon CloudWatch Logs for observability by default.</p> Synthetic Tests <p>Amazon CloudWatch Synthetics is used to continuously deliver traffic to the application and assert that requests are successful and responses are received within a given threshold. The canary is defined via CDK using the @cdklabs/cdk-ecs-codedeploy construct:</p> <pre><code>const service = new ApplicationLoadBalancedCodeDeployedFargateService(this, 'Api', {\n  ...\n\n  apiCanaryTimeout: Duration.seconds(5),\n  apiTestSteps: [{\n    name: 'getAll',\n    path: '/api/fruits',\n    jmesPath: 'length(@)',\n    expectedValue: 5,\n  }],\n</code></pre> Performance Tests <p>Apache JMeter is used to run performance tests against the deployed application. The tests are stored in <code>src/test/jmeter</code> and added to the pipeline via CDK:</p> <pre><code>import { JMeterTest } from './jmeter-test';\n\n\u22ef\n\n        new JMeterTest('Performance Test', {\n          source: pipelineSource,\n          endpoint: deployment.apiUrl,\n          threads: 300,\n          duration: 300,\n          throughput: 6000,\n          cacheBucket,\n        }),\n</code></pre> Resilience Tests <p><code>Not Implemented</code></p> Dynamic Application Security Testing (DAST) <p><code>Not Implemented</code></p>"},{"location":"application-pipeline/ri-cdk-pipeline/#prod","title":"Prod","text":"Manual Approval <p>A manual approval step is added to the end of the <code>Gamma</code> stage. The step is added at the end to keep the environment in a stable state while manual testing is performed. Once the step is approved, the pipeline continues execution to the next stage.</p> <pre><code>    new PipelineEnvironment(pipeline, Gamma, (deployment, stage) =&gt; {\n        stage.addPost(\n            new JMeterTest('Performance Test', {\n            source: source.codePipelineSource,\n            endpoint: deployment.apiUrl,\n            threads: 300,\n            duration: 300,\n            throughput: 6000,\n            cacheBucket,\n            }),\n            new ManualApprovalStep('PromoteFromGamma'),\n        );\n    });\n</code></pre> <p>When a manual approval step is used, IAM permissions should be used to restrict which principals can approve actions and stages to enforce least privilege.</p> <pre><code>    {\n        \"Effect\": \"Allow\",\n        \"Action\": [\n            \"codepipeline:PutApprovalResult\"\n        ],\n        \"Resource\": \"arn:aws:codepipeline:us-east-2:80398EXAMPLE:MyFirstPipeline/MyApprovalStage/MyApprovalAction\"\n    }\n</code></pre> Database Deploy <p>Spring Boot is configured to run Liquibase on startup. This reads the configuration in <code>src/main/resources/db/changelog/db.changelog-master.yml</code> to define the tables and initial data for the database:</p> <pre><code>databaseChangeLog:\n   - changeSet:\n       id: \"1\"\n       author: AWS\n       changes:\n       - createTable:\n           tableName: fruit\n           columns:\n           - column:\n               name: id\n               type: bigint\n               autoIncrement: true\n               constraints:\n                   primaryKey:  true\n                   nullable:  false\n           - column:\n               name: name\n               type: varchar(250)\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Apple\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Orange\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Banana\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Cherry\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Grape\n\n   - changeSet:\n       id: \"2\"\n       author: AWS\n       changes:\n       - addColumn:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               type: varchar(250)\n               constraints:\n                 nullable: true\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: pome\n           where: name='Apple'\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: berry\n           where: name='Orange'\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: berry\n           where: name='Banana'\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: drupe\n           where: name='Cherry'\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: berry\n           where: name='Grape'\n</code></pre> Progressive Deployment <p>Progressive deployment is implemented with AWS CodeDeploy for ECS. CodeDeploy performs a linear blue/green by deploying the new task definition as a new task with a separate target group and then shifting 10% of traffic every minute until all traffic is shifted. A CloudWatch alarm is monitored by CodeDeploy and an automatic rollback is triggered if the alarm exceeds the threshold.</p> <p></p> <p>Implementation of this type deployment presents challenges due to the following limitations:</p> <ul> <li>aws/aws-cdk #19163 - CDK Pipelines aren't intended to be used with CodeDeploy actions.</li> <li>AWS CloudFormation User Guide - The use of <code>AWS::CodeDeploy::BlueGreen</code> hooks and <code>AWS::CodeDeployBlueGreen</code> restricts the types of changes that can be made. Additionally, you can't use auto-rollback capabilities of CodeDeploy.</li> <li>aws/aws-cdk #5170 - CDK doesn't support defining CloudFormation rollback triggers. This rules out CloudFormation based blue/green deployments.</li> </ul> <p>The solution was to use the @cdklabs/cdk-ecs-codedeploy construct from the Construct Hub which addresses aws/aws-cdk #1559 - Lack of support for Blue/Green ECS Deployment in CDK. </p> <pre><code>const service = new ApplicationLoadBalancedCodeDeployedFargateService(this, 'Api', {\n  cluster,\n  capacityProviderStrategies: [\n    {\n      capacityProvider: 'FARGATE_SPOT',\n      weight: 1,\n    },\n  ],\n  minHealthyPercent: 50,\n  maxHealthyPercent: 200,\n  desiredCount: 3,\n  cpu: 512,\n  memoryLimitMiB: 1024,\n  taskImageOptions: {\n    image,\n    containerName: 'api',\n    containerPort: 8080,\n    family: appName,\n    logDriver: AwsLogDriver.awsLogs({\n      logGroup: appLogGroup,\n      streamPrefix: 'service',\n    }),\n    secrets: {\n      SPRING_DATASOURCE_USERNAME: Secret.fromSecretsManager( dbSecret, 'username' ),\n      SPRING_DATASOURCE_PASSWORD: Secret.fromSecretsManager( dbSecret, 'password' ),\n    },\n    environment: {\n      SPRING_DATASOURCE_URL: `jdbc:mysql://${db.clusterEndpoint.hostname}:${db.clusterEndpoint.port}/${dbName}`,\n    },\n  },\n  deregistrationDelay: Duration.seconds(5),\n  responseTimeAlarmThreshold: Duration.seconds(3),\n  healthCheck: {\n    healthyThresholdCount: 2,\n    unhealthyThresholdCount: 2,\n    interval: Duration.seconds(60),\n    path: '/actuator/health',\n  },\n  deploymentConfig,\n  terminationWaitTime: Duration.minutes(5),\n  apiCanaryTimeout: Duration.seconds(5),\n  apiTestSteps: [{\n    name: 'getAll',\n    path: '/api/fruits',\n    jmesPath: 'length(@)',\n    expectedValue: 5,\n  }],\n});\n\nthis.apiUrl = new CfnOutput(this, 'endpointUrl', {\n  value: `http://${service.listener.loadBalancer.loadBalancerDnsName}`,\n});\n</code></pre> <p>Deployments are made incrementally across regions using the CDK Pipeline - Wave construct. Each wave contains a list of regions to deploy to in parallel. One wave must fully complete before the next wave starts. The diagram below shows how each wave deploys to 2 regions at a time.</p> <p></p> <p>Environments are configured in CDK with the list of waves:</p> <pre><code>// BETA environment is 1 wave with 1 region\nexport const Beta: EnvironmentConfig = {\n    name: 'Beta',\n    account: accounts.beta,\n    waves: [\n        ['us-west-2'],\n    ],\n};\n\n// GAMMA environment is 1 wave with 2 regions\nexport const Gamma: EnvironmentConfig = {\n    name: 'Gamma',\n    account: accounts.gamma,\n    waves: [\n        ['us-west-2', 'us-east-1'],\n    ],\n};\n\n// PROD environment is 3 wave with 2 regions each wave\nexport const Prod: EnvironmentConfig = {\n    name: 'Prod',\n    account: accounts.production,\n    waves: [\n        ['us-west-2', 'us-east-1'],\n        ['eu-central-1', 'eu-west-1'],\n        ['ap-south-1', 'ap-southeast-2'],\n    ],\n};\n</code></pre> <p>A <code>PipelineEnvironment</code> class is responsible for loading the <code>EnvironmentConfig</code> into CodePipeline stages:</p> <pre><code>    new PipelineEnvironment(pipeline, Beta, (deployment, stage) =&gt; {\n      stage.addPost(\n        new SoapUITest('E2E Test', {\n          source: pipelineSource,\n          endpoint: deployment.apiUrl,\n          cacheBucket,\n        }),\n      );\n    });\n\n    \u22ef\n\n    new PipelineEnvironment(pipeline, Gamma, (deployment, stage) =&gt; {\n      stage.addPost(\n        new JMeterTest('Performance Test', {\n          source: pipelineSource,\n          endpoint: deployment.apiUrl,\n          threads: 300,\n          duration: 300,\n          throughput: 6000,\n          cacheBucket,\n        }),\n      );\n    }, wave =&gt; {\n      wave.addPost(\n        new ManualApprovalStep('PromoteToProd'),\n      );\n    });\n\n    \u22ef\n\nclass PipelineEnvironment {\n  constructor(\n    pipeline: CodePipeline,\n    environment: EnvironmentConfig,\n    stagePostProcessor?: PipelineEnvironmentStageProcessor,\n    wavePostProcessor?: PipelineEnvironmentWaveProcessor) {\n    if (!environment.account?.accountId) {\n      throw new Error(`Missing accountId for environment '${environment.name}'. Do you need to update '.accounts.env'?`);\n    }\n    for (const [i, regions] of environment.waves.entries()) {\n      const wave = pipeline.addWave(`${environment.name}-${i}`);\n      for (const region of regions) {\n        const deployment = new Deployment(pipeline, environment.name, {\n          account: environment.account!.accountId!,\n          region,\n        });\n        const stage = wave.addStage(deployment);\n        if (stagePostProcessor) {\n          stagePostProcessor(deployment, stage);\n        }\n      }\n      if (wavePostProcessor) {\n        wavePostProcessor(wave);\n      }\n    }\n  }\n}\n</code></pre> Synthetic Tests <p>Amazon CloudWatch Synthetics is used to continuously deliver traffic to the application and assert that requests are successful and responses are received within a given threshold. The canary is defined via CDK using the @cdklabs/cdk-ecs-codedeploy construct:</p> <pre><code>const service = new ApplicationLoadBalancedCodeDeployedFargateService(this, 'Api', {\n  ...\n\n  apiCanaryTimeout: Duration.seconds(5),\n  apiTestSteps: [{\n    name: 'getAll',\n    path: '/api/fruits',\n    jmesPath: 'length(@)',\n    expectedValue: 5,\n  }],\n</code></pre>"},{"location":"application-pipeline/ri-cdk-pipeline/#frequently-asked-questions","title":"Frequently Asked Questions","text":"What operating models does this reference implementation support? <p>This reference implementation can accomodate any operation model with minor updates:</p> <ul> <li>Fully Separated - Restrict the role that CDK uses for CloudFormation execution to only create resources from approved product portfolios in AWS Service Catalog. Ownership of creating the products in Service Catalog is owned by the Platform Engineering team and operational support of Service Catalog is owned by the Platform Operations team. The Platform Engineering team should publish CDK constructs internally that provision AWS resources through Service Catalog. Update the CDK app in the <code>infrastructure/</code> directory to use CDK constructs provided by the <code>Platform Engineering</code> team. Use a CODEOWNERS file to require all changes to the <code>infrastructure/</code> directory be approved by the Application Operations team. Additionally, restrict permissions to the Manual Approval action to only allow members of the Application Operations to approve.</li> <li>Separated AEO and IEO with Centralized Governance - Restrict the role that CDK uses for CloudFormation execution to only create resources from approved product portfolios in AWS Service Catalog. Ownership of creating the products in Service Catalog is owned by the Platform Engineering team and operational support of Service Catalog is owned by the Platform Engineering team. The Platform Engineering team should publish CDK constructs internally that provision AWS resources through Service Catalog. Update the CDK app in the <code>infrastructure/</code> directory to use CDK constructs provided by the <code>Platform Engineering</code> team.</li> <li>Separated AEO and IEO with Decentralized Governance - The Platform Engineering team should publish CDK constructs internally that provision AWS resources in manner that achieve organizational compliance. Update the CDK app in the <code>infrastructure/</code> directory to use CDK constructs provided by the <code>Platform Engineering</code> team.</li> </ul> Where is manual testing performed in this pipeline? <p>Ideally, all testing is accomplished through automation in the Integration Tests and Acceptance Tests actions. If an organization relies on people manually executing tests then these tests would be performed in the Gamma Stage. The Manual Approval action would be required and the approval would be granted by a Quality Assurance team member once the manual testing completes successfully.</p>"},{"location":"application-pipeline/ri-circleci-pipeline/","title":"CircleCI Pipeline","text":"<p>This presents a reference implementation of the Application Pipeline reference architecture. The pipeline is created with CircleCI for building the software and performing testing tasks. All the infrastructure for this reference implementation is defined with AWS Cloud Development Kit. The source code for this reference implementation is available in GitHub for review.</p> <p>This reference implementation can be utilized by forking this repository and onboarding the project to CircleCI. Once you've forked the repo, you can use the provided CircleCI configuration file to set up your CI/CD pipeline and start automating your builds, tests, and deployments on CircleCI.</p> <p>Additionally, you will need to create CircleCI contexts to securely store credentials and environment variables. These contexts will allow your pipeline to interact with CircleCI and any third-party services required by the project.</p> <p>Lastly, establish an OIDC connection between CircleCI and AWS to enable secure, identity-based access. Once the necessary IAM role has been created, update the CircleCI configuration parameters with your AWS account ID and the role\u2019s ARN to complete the setup.</p> <p></p> Disclaimer <p>This reference implementation is intended to serve as an example of how to accomplish the guidance in the reference architecture using CircleCI. The reference implementation has intentionally not followed the following AWS Well-Architected best practices to make it accessible by a wider range of customers. Be sure to address these before using parts of this code for any workloads in your own environment:</p> <ul> <li> cdk bootstrap with AdministratorAccess - the default policy used for <code>cdk bootstrap</code> is <code>AdministratorAccess</code> but should be replaced with a more appropriate policy with least privilege in your account.</li> <li> TLS on HTTP endpoint - the listener for the sample application uses HTTP instead of HTTPS to avoid having to create new ACM certificates and Route53 hosted zones. This should be replaced in your account with an <code>HTTPS</code> listener.</li> </ul> <p></p>"},{"location":"application-pipeline/ri-circleci-pipeline/#local-development","title":"Local Development","text":"<p>Developers need fast-feedback for potential issues with their code. Automation should run in their developer workspace to give them feedback before the deployment pipeline runs.</p> Pre-Commit Hooks <p>Pre-Commit hooks are scripts that are executed on the developer's workstation when they try to create a new commit. These hooks have an opportunity to inspect the state of the code before the commit occurs and abort the commit if tests fail. An example of pre-commit hooks are Git hooks.  Examples of tools to configure and store pre-commit hooks as code include but are not limited to husky and pre-commit.</p> <p>The following <code>.pre-commit-config.yaml</code> is added to the repository that will build the code with Maven, run unit tests with JUnit, check for code quality with Checkstyle, run static application security testing with PMD and check for secrets in the code with gitleaks.</p> <pre><code>repos:\n- repo: https://github.com/pre-commit/pre-commit-hooks\n  rev: v2.3.0\n  hooks:\n  -   id: check-yaml\n  -   id: check-json\n  -   id: trailing-whitespace\n- repo: https://github.com/pre-commit/mirrors-eslint\n  rev: v8.23.0\n  hooks:\n  -   id: eslint\n- repo: https://github.com/ejba/pre-commit-maven\n  rev: v0.3.3\n  hooks:\n  -   id: maven-test\n- repo: https://github.com/zricethezav/gitleaks\n  rev: v8.12.0\n  hooks:\n    - id: gitleaks\n</code></pre>"},{"location":"application-pipeline/ri-circleci-pipeline/#source","title":"Source","text":"Application Source Code <p>The application source code can be found in the src/main/java directory. It is intended to serve only as a reference and should be replaced by your own application source code.</p> <p>This reference implementation includes a Spring Boot application that exposes a REST API and uses a database for persistence. The API is implemented in <code>FruitController.java</code>:</p> <pre><code>public class FruitController {\n    /**\n     * JPA repository for fruits.\n     */\n    private final FruitRepository repository;\n\n    /**\n     * Logic to map between entities and DTOs\n     */\n    private final FruitMapper mapper;\n\n    FruitController(final FruitRepository r, final FruitMapper m) {\n        this.repository = r;\n        this.mapper = m;\n    }\n\n    @GetMapping(\"/api/fruits\")\n    List&lt;FruitDTO&gt; all() {\n        return repository.findAll()\n                .stream()\n                .map(mapper::toDto)\n                .collect(Collectors.toList());\n    }\n\n    @PostMapping(\"/api/fruits\")\n    FruitDTO newFruit(@RequestBody final FruitDTO fruit) {\n        return mapper.toDto(repository.save(mapper.toEntity(fruit)));\n    }\n\n    @GetMapping(\"/api/fruits/{id}\")\n    FruitDTO one(@PathVariable final Long id) {\n        return repository.findById(id)\n                .map(mapper::toDto)\n                .orElseThrow(() -&gt; new FruitNotFoundException(id));\n    }\n\n    @PutMapping(\"/api/fruits/{id}\")\n    FruitDTO replaceFruit(\n            @RequestBody final FruitDTO newFruit,\n            @PathVariable final Long id) {\n        newFruit.setId(id);\n        return mapper.toDto(repository.save(mapper.toEntity(newFruit)));\n    }\n\n    @DeleteMapping(\"/api/fruits/{id}\")\n    void deleteFruit(@PathVariable final Long id) {\n        repository.deleteById(id);\n    }\n}\n</code></pre> <p>The application source code is stored in the version control system (VCS) you link to CircleCI. To get started quickly, simply fork this repository and connect it to CircleCI to begin setting up your CI/CD pipeline.</p> Test Source Code <p>The test source code can be found in the src/test/java directory. It is intended to serve only as a reference and should be replaced by your own test source code.</p> <p>The reference implementation includes source code for unit, integration and end-to-end testing. Unit and integration tests can be found in <code>src/test/java</code>. For example, <code>FruitControllerWithoutClassificationTest.java</code> performs unit tests of each API path with the JUnit testing library:</p> <pre><code>public void shouldReturnList() throws Exception {\n  when(repository.findAll()).thenReturn(Arrays.asList(new Fruit(\"Mango\", FruitClassification.pome), new Fruit(\"Dragonfruit\", FruitClassification.berry)));\n\n  this.mockMvc.perform(get(\"/api/fruits\")).andDo(print()).andExpect(status().isOk())\n      .andExpect(content().json(\"[{\\\"name\\\": \\\"Mango\\\"}, {\\\"name\\\": \\\"Dragonfruit\\\"}]\"));\n}\n</code></pre> <p>Acceptance tests are preformed with SoapUI and are defined in <code>fruit-api-soapui-project.xml</code>. They are executed by Maven using plugins in <code>pom.xml</code>.</p> Infrastructure Source Code <p>The infrastructure source code can be found in the infrastructure directory. It is intended to serve as a reference but much of the code can also be reused in your own CDK applications.</p> <p>Infrastructure source code defines the deployment of the application are stored in <code>infrastructure/</code> folder and uses AWS Cloud Development Kit.</p> <pre><code>super(scope, id, props);\n\nconst image = new AssetImage('.', { target: 'build' });\n\nconst appName = Stack.of(this)\n  .stackName.toLowerCase()\n  .replace(`-${Stack.of(this).region}-`, '-');\nconst vpc = new ec2.Vpc(this, 'Vpc', {\n  maxAzs: 3,\n  natGateways: props?.natGateways,\n});\nnew FlowLog(this, 'VpcFlowLog', {\n  resourceType: FlowLogResourceType.fromVpc(vpc),\n});\n\nconst dbName = 'fruits';\nconst dbSecret = new DatabaseSecret(this, 'AuroraSecret', {\n  username: 'fruitapi',\n  secretName: `${appName}-DB`,\n});\n\nconst db = new DatabaseCluster(this, 'Database', {\n  engine: DatabaseClusterEngine.auroraMysql({\n    version: AuroraMysqlEngineVersion.VER_3_07_1,\n  }),\n  credentials: Credentials.fromSecret(dbSecret),\n  writer: ClusterInstance.serverlessV2('writer'),\n  defaultDatabaseName: dbName,\n  serverlessV2MaxCapacity: 2,\n  serverlessV2MinCapacity: 0.5,\n  vpc,\n  clusterIdentifier: appName,\n  storageEncrypted: true,\n});\n\nconst cluster = new ecs.Cluster(this, 'Cluster', {\n  vpc,\n  containerInsights: true,\n  clusterName: appName,\n});\nconst appLogGroup = new LogGroup(this, 'AppLogGroup', {\n  retention: RetentionDays.ONE_WEEK,\n  logGroupName: `/aws/ecs/service/${appName}`,\n  removalPolicy: RemovalPolicy.DESTROY,\n});\nlet deploymentConfig: IEcsDeploymentConfig | undefined = undefined;\nif (props?.deploymentConfigName) {\n  deploymentConfig = EcsDeploymentConfig.fromEcsDeploymentConfigName(\n    this,\n    'DeploymentConfig',\n    props.deploymentConfigName,\n  );\n}\nconst appConfigEnabled =\n  props?.appConfigRoleArn !== undefined &amp;&amp;\n  props.appConfigRoleArn.length &gt; 0;\nconst service = new ApplicationLoadBalancedCodeDeployedFargateService(\n  this,\n  'Api',\n  {\n    cluster,\n    capacityProviderStrategies: [\n      {\n        capacityProvider: 'FARGATE_SPOT',\n        weight: 1,\n      },\n    ],\n    minHealthyPercent: 50,\n    maxHealthyPercent: 200,\n    desiredCount: 3,\n    cpu: 512,\n    memoryLimitMiB: 1024,\n    taskImageOptions: {\n      image,\n      containerName: 'api',\n      containerPort: 8080,\n      family: appName,\n      logDriver: AwsLogDriver.awsLogs({\n        logGroup: appLogGroup,\n        streamPrefix: 'service',\n      }),\n      secrets: {\n        SPRING_DATASOURCE_USERNAME: Secret.fromSecretsManager(\n          dbSecret,\n          'username',\n        ),\n        SPRING_DATASOURCE_PASSWORD: Secret.fromSecretsManager(\n          dbSecret,\n          'password',\n        ),\n      },\n      environment: {\n        SPRING_DATASOURCE_URL: `jdbc:mysql://${db.clusterEndpoint.hostname}:${db.clusterEndpoint.port}/${dbName}`,\n        APPCONFIG_AGENT_APPLICATION:\n          this.node.tryGetContext('workloadName'),\n        APPCONFIG_AGENT_ENVIRONMENT:\n          this.node.tryGetContext('environmentName'),\n        APPCONFIG_AGENT_ENABLED: appConfigEnabled.toString(),\n      },\n    },\n    deregistrationDelay: Duration.seconds(5),\n    responseTimeAlarmThreshold: Duration.seconds(3),\n    targetHealthCheck: {\n      healthyThresholdCount: 2,\n      unhealthyThresholdCount: 2,\n      interval: Duration.seconds(60),\n      path: '/actuator/health',\n    },\n    deploymentConfig,\n    terminationWaitTime: Duration.minutes(5),\n    apiCanaryTimeout: Duration.seconds(5),\n    apiTestSteps: [\n      {\n        name: 'getAll',\n        path: '/api/fruits',\n        jmesPath: 'length(@)',\n        expectedValue: 5,\n      },\n    ],\n  },\n);\n\nif (appConfigEnabled) {\n  service.taskDefinition.addContainer('appconfig-agent', {\n    image: ecs.ContainerImage.fromRegistry(\n      'public.ecr.aws/aws-appconfig/aws-appconfig-agent:2.x',\n    ),\n    essential: false,\n    logging: AwsLogDriver.awsLogs({\n      logGroup: appLogGroup,\n      streamPrefix: 'service',\n    }),\n    environment: {\n      SERVICE_REGION: this.region,\n      ROLE_ARN: props!.appConfigRoleArn!,\n      ROLE_SESSION_NAME: appName,\n      LOG_LEVEL: 'info',\n    },\n    portMappings: [{ containerPort: 2772 }],\n  });\n\n  service.taskDefinition.addToTaskRolePolicy(\n    new PolicyStatement({\n      actions: ['sts:AssumeRole'],\n      resources: [props!.appConfigRoleArn!],\n    }),\n  );\n}\n\nservice.service.connections.allowTo(\n  db,\n  ec2.Port.tcp(db.clusterEndpoint.port),\n);\n\nthis.apiUrl = new CfnOutput(this, 'endpointUrl', {\n  value: `http://${service.listener.loadBalancer.loadBalancerDnsName}`,\n});\n</code></pre> <p>Notice that the infrastructure code is written in Typescript which is different from the Application Source Code (Java). This was done intentionally to demonstrate that CDK allows defining infrastructure code in whatever language is most appropriate for the team that owns the use of CDK in the organization.</p> Static Assets <p>There are no static assets used by the sample application.</p> Dependency Manifests <p>All third-party dependencies used by the sample application are define in the <code>pom.xml</code>:</p> <pre><code>&lt;dependencies&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n        &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;\n    &lt;/dependency&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n        &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt;\n    &lt;/dependency&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n        &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;\n    &lt;/dependency&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;org.liquibase&lt;/groupId&gt;\n        &lt;artifactId&gt;liquibase-core&lt;/artifactId&gt;\n    &lt;/dependency&gt;\n&lt;/dependencies&gt;\n</code></pre> Static Configuration <p>Static configuration for the application is defined in <code>src/main/resources/application.yml</code>:</p> <pre><code>spring:\n  application:\n    name: fruit-api\n  main:\n    banner-mode: \"off\"\n  jackson:\n    default-property-inclusion: non_null\n\n\nspringdoc:\n  swagger-ui:\n    path: /swagger-ui\n\nappconfig-agent:\n  environment: alpha\n  log-level-from:\n    configuration: operations\n</code></pre> Database Source Code <p>The database source code can be found in the src/main/resources/db directory. It is intended to serve only as a reference and should be replaced by your own database source code.</p> <p>The code that manages the schema and initial data for the application is defined using Liquibase in <code>src/main/resources/db/changelog/db.changelog-master.yml</code>:</p> <pre><code>databaseChangeLog:\n   - changeSet:\n       id: \"1\"\n       author: AWS\n       changes:\n       - createTable:\n           tableName: fruit\n           columns:\n           - column:\n               name: id\n               type: bigint\n               autoIncrement: true\n               constraints:\n                   primaryKey:  true\n                   nullable:  false\n           - column:\n               name: name\n               type: varchar(250)\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Apple\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Orange\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Banana\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Cherry\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Grape\n\n   - changeSet:\n       id: \"2\"\n       author: AWS\n       changes:\n       - addColumn:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               type: varchar(250)\n               constraints:\n                 nullable: true\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: pome\n           where: name='Apple'\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: berry\n           where: name='Orange'\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: berry\n           where: name='Banana'\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: drupe\n           where: name='Cherry'\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: berry\n           where: name='Grape'\n</code></pre>"},{"location":"application-pipeline/ri-circleci-pipeline/#build","title":"Build","text":"<p>Actions in this stage are designed to complete in under 3 minutes, allowing developers to quickly receive feedback and address any issues before moving on to their next task. Each of the actions below is defined as code using the CircleCI configuration file to ensure consistency and repeatability.</p> Build Code <p>The Java source code is compiled, unit tested and packaged by Maven. An action is added to the workflow to build and package the source code:</p> <pre><code>  package:\n    docker:\n      - image: cimg/openjdk:17.0\n    working_directory: &lt;&lt; pipeline.parameters.working-dir &gt;&gt;\n    environment:\n      TRIVY_DB_REPOSITORY: public.ecr.aws/aquasecurity/trivy-db\n    steps:\n      - checkout:\n          path: ~/aws-deployment-pipeline-reference-architecture\n      - restore_cache:\n          keys:\n            - maven-repo-v1-{{ checksum \"pom.xml\" }}\n      - run:\n          name: Run Maven Verify\n          command: mvn verify --batch-mode --no-transfer-progress\n      - save_cache:\n          paths:\n            - ~/.m2/repository\n          key: maven-repo-v1-{{ checksum \"pom.xml\" }}\n      - trivy/scan:\n          scan-type: fs\n          ignore-unfixed: true\n          format: spdx-json\n          output: &lt;&lt; pipeline.parameters.working-dir &gt;&gt;/dependency-results.sbom.json\n      - store_test_results:\n          path: &lt;&lt; pipeline.parameters.working-dir &gt;&gt;/target/surefire-reports\n      - store_test_results:\n          path: &lt;&lt; pipeline.parameters.working-dir &gt;&gt;/target/soapui-reports\n      - store_artifacts:\n          path: &lt;&lt; pipeline.parameters.working-dir &gt;&gt;/dependency-results.sbom.json\n      - store_artifacts:\n          path: &lt;&lt; pipeline.parameters.working-dir &gt;&gt;/target/spotbugsXml.xml    \n      - store_artifacts:\n          path: &lt;&lt; pipeline.parameters.working-dir &gt;&gt;/target/jmeter/results\n      - store_artifacts:\n          path: &lt;&lt; pipeline.parameters.working-dir &gt;&gt;/target/fruit-api.jar\n      - persist_to_workspace:\n          root: &lt;&lt; pipeline.parameters.working-dir &gt;&gt;\n          paths:\n            - target/fruit-api.jar\n</code></pre> Unit Tests <p>The unit tests are run by Maven simultaneously with the <code>Build Code</code> step. The results of the unit tests are then uploaded to CircleCI Test Insights to track test performance and trends over time. CircleCI Test Insights provides visibility into test results, helping you monitor for any regressions and improve the efficiency of your testing process.</p> <p></p> Code Quality <p>Code quality is enforced through the PMD and Checkstyle Maven plugins:</p> <pre><code>    &lt;plugin&gt;\n        &lt;artifactId&gt;maven-pmd-plugin&lt;/artifactId&gt;\n        &lt;configuration&gt;\n            &lt;printFailingErrors&gt;&lt;/printFailingErrors&gt;\n        &lt;/configuration&gt;\n        &lt;executions&gt;\n            &lt;execution&gt;\n                &lt;phase&gt;test&lt;/phase&gt;\n                &lt;goals&gt;\n                    &lt;goal&gt;check&lt;/goal&gt;\n                &lt;/goals&gt;\n            &lt;/execution&gt;\n        &lt;/executions&gt;\n    &lt;/plugin&gt;\n    &lt;plugin&gt;\n        &lt;artifactId&gt;maven-checkstyle-plugin&lt;/artifactId&gt;\n        &lt;configuration&gt;\n            &lt;printFailingErrors&gt;&lt;/printFailingErrors&gt;\n        &lt;/configuration&gt;\n        &lt;executions&gt;\n            &lt;execution&gt;\n                &lt;phase&gt;test&lt;/phase&gt;\n                &lt;goals&gt;\n                    &lt;goal&gt;check&lt;/goal&gt;\n                &lt;/goals&gt;\n            &lt;/execution&gt;\n        &lt;/executions&gt;\n    &lt;/plugin&gt;\n</code></pre> <p>Additionally, cdk-nag is run against the deployment stack to identify any security issues with the resources being created. The pipeline will fail if any are detected. The following code demonstrates how cdk-nag is called as a part of the build stage. The code also demonstrates how to suppress findings.</p> <pre><code>import { App, Aspects } from 'aws-cdk-lib';\nimport { Annotations, Match, Template } from 'aws-cdk-lib/assertions';\nimport { SynthesisMessage } from 'aws-cdk-lib/cx-api';\nimport { AwsSolutionsChecks, NagSuppressions } from 'cdk-nag';\nimport { DeploymentStack } from '../src/deployment';\n\n\nfunction synthesisMessageToString(sm: SynthesisMessage): string {\n  return `${sm.entry.data} [${sm.id}]`;\n}\nexpect.addSnapshotSerializer({\n  test: (val) =&gt; typeof val === 'string' &amp;&amp; val.match(/^dummy.dkr.ecr.us-east.1/) !== null,\n  serialize: () =&gt; '\"dummy-ecr-image\"',\n});\nexpect.addSnapshotSerializer({\n  test: (val) =&gt; typeof val === 'string' &amp;&amp; val.match(/^[a-f0-9]+\\.zip$/) !== null,\n  serialize: () =&gt; '\"code.zip\"',\n});\n\ndescribe('cdk-nag', () =&gt; {\n  let stack: DeploymentStack;\n  let app: App;\n\n  beforeAll(() =&gt; {\n    const appName = 'fruit-api';\n    const workloadName = 'food';\n    const environmentName = 'unit-test';\n    app = new App({ context: { appName, environmentName, workloadName } });\n    stack = new DeploymentStack(app, 'TestStack', {\n      env: {\n        account: 'dummy',\n        region: 'us-east-1',\n      },\n    });\n    Aspects.of(stack).add(new AwsSolutionsChecks());\n\n    // Suppress CDK-NAG for TaskDefinition role and ecr:GetAuthorizationToken permission\n    NagSuppressions.addResourceSuppressionsByPath(\n      stack,\n      `/${stack.stackName}/Api/TaskDef/ExecutionRole/DefaultPolicy/Resource`,\n      [{ id: 'AwsSolutions-IAM5', reason: 'Allow ecr:GetAuthorizationToken', appliesTo: ['Resource::*'] }],\n    );\n\n    // Suppress CDK-NAG for secret rotation\n    NagSuppressions.addResourceSuppressionsByPath(\n      stack,\n      `/${stack.stackName}/AuroraSecret/Resource`,\n      [{ id: 'AwsSolutions-SMG4', reason: 'Dont require secret rotation' }],\n    );\n\n    // Suppress CDK-NAG for RDS Serverless\n    NagSuppressions.addResourceSuppressionsByPath(\n      stack,\n      `/${stack.stackName}/Database/Resource`,\n      [\n        { id: 'AwsSolutions-RDS6', reason: 'IAM authentication not supported on Serverless v1' },\n        { id: 'AwsSolutions-RDS10', reason: 'Disable delete protection to simplify cleanup of Reference Implementation' },\n        { id: 'AwsSolutions-RDS11', reason: 'Custom port not supported on Serverless v1' },\n        { id: 'AwsSolutions-RDS14', reason: 'Backtrack not supported on Serverless v1' },\n        { id: 'AwsSolutions-RDS16', reason: 'CloudWatch Log Export not supported on Serverless v1' },\n      ],\n    );\n\n    NagSuppressions.addResourceSuppressionsByPath(stack, [\n      `/${stack.stackName}/Api/DeploymentGroup/Deployment/DeploymentProvider/framework-onEvent`,\n      `/${stack.stackName}/Api/DeploymentGroup/Deployment/DeploymentProvider/framework-isComplete`,\n      `/${stack.stackName}/Api/DeploymentGroup/Deployment/DeploymentProvider/framework-onTimeout`,\n      `/${stack.stackName}/Api/DeploymentGroup/Deployment/DeploymentProvider/waiter-state-machine`,\n    ], [\n      { id: 'AwsSolutions-IAM5', reason: 'Unrelated to construct under test' },\n      { id: 'AwsSolutions-L1', reason: 'Unrelated to construct under test' },\n      { id: 'AwsSolutions-SF1', reason: 'Unrelated to construct under test' },\n      { id: 'AwsSolutions-SF2', reason: 'Unrelated to construct under test' },\n    ], true);\n\n    // Ignore findings from access log bucket\n    NagSuppressions.addResourceSuppressionsByPath(stack, [\n      `/${stack.stackName}/Api/AccessLogBucket`,\n    ], [\n      { id: 'AwsSolutions-S1', reason: 'Dont need access logs for access log bucket' },\n      { id: 'AwsSolutions-IAM5', reason: 'Allow resource:*', appliesTo: ['Resource::*'] },\n    ]);\n\n    NagSuppressions.addResourceSuppressionsByPath(stack, [\n      `/${stack.stackName}/Api/Canary/ServiceRole`,\n    ], [{ id: 'AwsSolutions-IAM5', reason: 'Allow resource:*' }]);\n\n    NagSuppressions.addResourceSuppressionsByPath(stack, [\n      `/${stack.stackName}/Api/CanaryArtifactsBucket`,\n    ], [{ id: 'AwsSolutions-S1', reason: 'Dont need access logs for canary bucket' }]);\n\n    NagSuppressions.addResourceSuppressionsByPath(stack, [\n      `/${stack.stackName}/Api/DeploymentGroup/ServiceRole`,\n    ], [\n      { id: 'AwsSolutions-IAM4', reason: 'Allow AWSCodeDeployRoleForECS policy', appliesTo: ['Policy::arn:&lt;AWS::Partition&gt;:iam::aws:policy/AWSCodeDeployRoleForECS'] },\n    ]);\n\n    NagSuppressions.addResourceSuppressionsByPath(stack, [\n      `/${stack.stackName}/Api/DeploymentGroup/Deployment`,\n    ], [\n      {\n        id: 'AwsSolutions-IAM4',\n        reason: 'Allow AWSLambdaBasicExecutionRole policy',\n        appliesTo: ['Policy::arn:&lt;AWS::Partition&gt;:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole'],\n      },\n    ], true);\n\n    NagSuppressions.addResourceSuppressionsByPath(stack, [\n      `/${stack.stackName}/Api/TaskDef`,\n    ], [\n      {\n        id: 'AwsSolutions-ECS2',\n        reason: 'Allow environment variables for configuration of values that are not confidential',\n      },\n    ]);\n\n    NagSuppressions.addResourceSuppressionsByPath(stack, [\n      `/${stack.stackName}/Api/LB/SecurityGroup`,\n    ], [\n      {\n        id: 'AwsSolutions-EC23',\n        reason: 'Allow public inbound access on ELB',\n      },\n    ]);\n  });\n\n  test('Snapshot', () =&gt; {\n    const template = Template.fromStack(stack);\n    expect(template.toJSON()).toMatchSnapshot();\n  });\n\n  test('cdk-nag AwsSolutions Pack errors', () =&gt; {\n    const errors = Annotations.fromStack(stack).findError(\n      '*',\n      Match.stringLikeRegexp('AwsSolutions-.*'),\n    ).map(synthesisMessageToString);\n    expect(errors).toHaveLength(0);\n  });\n\n  test('cdk-nag AwsSolutions Pack warnings', () =&gt; {\n    const warnings = Annotations.fromStack(stack).findWarning(\n      '*',\n      Match.stringLikeRegexp('AwsSolutions-.*'),\n    ).map(synthesisMessageToString);\n    expect(warnings).toHaveLength(0);\n  });\n});\n\ndescribe('Deployment without AppConfig', () =&gt; {\n  let stack: DeploymentStack;\n  let app: App;\n\n  beforeAll(() =&gt; {\n    const appName = 'fruit-api';\n    const environmentName = 'unit-test';\n    app = new App({ context: { appName, environmentName } });\n    stack = new DeploymentStack(app, 'TestStack', {\n      env: {\n        account: 'dummy',\n        region: 'us-east-1',\n      },\n    });\n  });\n\n  test('Snapshot', () =&gt; {\n    const template = Template.fromStack(stack);\n    expect(template.toJSON()).toMatchSnapshot();\n  });\n  test('taskdef', () =&gt; {\n    const template = Template.fromStack(stack);\n    template.hasResourceProperties('AWS::ECS::TaskDefinition', {\n      ContainerDefinitions: [\n        {\n          Environment: [{\n            Name: 'SPRING_DATASOURCE_URL',\n          }, {\n            Name: 'APPCONFIG_AGENT_APPLICATION',\n          }, {\n            Name: 'APPCONFIG_AGENT_ENVIRONMENT',\n            Value: 'unit-test',\n          }, {\n            Name: 'APPCONFIG_AGENT_ENABLED',\n            Value: 'false',\n          }],\n        },\n      ],\n    });\n  });\n});\n\ndescribe('Deployment with AppConfig', () =&gt; {\n  let stack: DeploymentStack;\n  let app: App;\n\n  beforeAll(() =&gt; {\n    const appName = 'fruit-api';\n    const workloadName = 'food';\n    const environmentName = 'unit-test';\n    app = new App({ context: { appName, environmentName, workloadName } });\n    stack = new DeploymentStack(app, 'TestStack', {\n      appConfigRoleArn: 'dummy-role-arn',\n      env: {\n        account: 'dummy',\n        region: 'us-east-1',\n      },\n    });\n  });\n\n  test('Snapshot', () =&gt; {\n    const template = Template.fromStack(stack);\n    expect(template.toJSON()).toMatchSnapshot();\n  });\n  test('taskdef', () =&gt; {\n    const template = Template.fromStack(stack);\n    template.hasResourceProperties('AWS::ECS::TaskDefinition', {\n      ContainerDefinitions: [\n        {\n          Environment: [{\n            Name: 'SPRING_DATASOURCE_URL',\n          }, {\n            Name: 'APPCONFIG_AGENT_APPLICATION',\n            Value: 'food',\n          }, {\n            Name: 'APPCONFIG_AGENT_ENVIRONMENT',\n            Value: 'unit-test',\n          }, {\n            Name: 'APPCONFIG_AGENT_ENABLED',\n            Value: 'true',\n          }],\n        },\n        {\n          Environment: [{\n            Name: 'SERVICE_REGION',\n            Value: 'us-east-1',\n          }, {\n            Name: 'ROLE_ARN',\n            Value: 'dummy-role-arn',\n          }, {\n            Name: 'ROLE_SESSION_NAME',\n          }, {\n            Name: 'LOG_LEVEL',\n            Value: 'info',\n          }],\n        },\n      ],\n    });\n  });\n});\n</code></pre> Secrets Detection <p>Trivy is used to scan the source for secrets. The Trivy CircleCI Orb is used in the CircleCI workflow to perform the scan:</p> <pre><code>  trivy:\n    docker:\n      - image: cimg/base:2024.11\n    working_directory: &lt;&lt; pipeline.parameters.working-dir &gt;&gt;\n    steps:\n      - checkout:\n          path: ~/aws-deployment-pipeline-reference-architecture\n      - trivy/scan:\n          scan-type: fs\n          ignore-unfixed: true\n          format: sarif\n          output: &lt;&lt; pipeline.parameters.working-dir &gt;&gt;/report.sarif\n          scanners: vuln,secret,misconfig,license\n      - store_artifacts:\n          path: &lt;&lt; pipeline.parameters.working-dir &gt;&gt;/report.sarif\n    environment:\n      TRIVY_DB_REPOSITORY: public.ecr.aws/aquasecurity/trivy-db\n</code></pre> Static Application Security Testing (SAST) <p>The SpotBugs Maven plugin is used along with the Find Security Bugs plugin to identify OWASP Top 10 and CWE vulnerabilities in the application source code.</p> Package and Store Artifact(s) <p>CircleCI utilizes workspaces to transfer artifacts between jobs in the pipeline, allowing seamless handoff from one job to the next. For logs, scan reports, and other assets users want to keep for reference, CircleCI\u2019s artifact storage can be used to retain these files.</p> <p>The AWS Cloud Development Kit (CDK) is then leveraged to use the artifacts generated earlier in the CircleCI pipeline in the deployment jobs. This process includes building Docker images and publishing them to Amazon ECR repositories, ensuring all assets are readily available for deployment across environments.</p> Software Composition Analysis (SCA) <p>Trivy is used to scan the source for vulnerabilities in its dependencies. The <code>pom.xml</code> and <code>Dockerfile</code> files are scanned for configuration issues or vulnerabilities in any dependencies. The scanning is accomplished by a CircleCI Orb that runs Trivy CLI:</p> <pre><code>  trivy:\n    docker:\n      - image: cimg/base:2024.11\n    working_directory: &lt;&lt; pipeline.parameters.working-dir &gt;&gt;\n    steps:\n      - checkout:\n          path: ~/aws-deployment-pipeline-reference-architecture\n      - trivy/scan:\n          scan-type: fs\n          ignore-unfixed: true\n          format: sarif\n          output: &lt;&lt; pipeline.parameters.working-dir &gt;&gt;/report.sarif\n          scanners: vuln,secret,misconfig,license\n      - store_artifacts:\n          path: &lt;&lt; pipeline.parameters.working-dir &gt;&gt;/report.sarif\n    environment:\n      TRIVY_DB_REPOSITORY: public.ecr.aws/aquasecurity/trivy-db\n</code></pre> <p>Trivy is also used within the <code>Dockerfile</code> to scan the image after it is built. The <code>docker build</code> will fail if Trivy finds any vulnerabilities in the final image:</p> <pre><code>FROM public.ecr.aws/amazoncorretto/amazoncorretto:17-al2022-jdk as build\nUSER nobody\nWORKDIR /app\nCOPY target/fruit-api.jar /app\nHEALTHCHECK --interval=30s --timeout=5s --start-period=30s --retries=3 CMD /bin/curl --fail --silent localhost:8080/actuator/health | grep UP || exit 1\nENTRYPOINT [\"java\",\"-jar\",\"/app/fruit-api.jar\"]\n\n# Use multi-stage builds to scan newly created image with Trivy. This second stage 'vulnscan'\n# isn't published to Amazon ECR and is never run. It is only used to run the Trivy scan\n# against the newly created image in the 'build' stage.\n#\n# This stage must run as root so Trivy can scan all files in the image, not just\n# those accessible by the nobody user. The user is switched back to 'nobody' at\n# the end to ensure that even if this image is used for something it is done\n# without the 'root' user.\n\nFROM build AS vulnscan\nUSER root\nCOPY --from=aquasec/trivy:latest /usr/local/bin/trivy /usr/local/bin/trivy\nRUN trivy filesystem --exit-code 1 --no-progress --ignore-unfixed -s CRITICAL /\nUSER nobody\n</code></pre> Software Bill of Materials (SBOM) <p>Trivy generates an SBOM in the form of a SPDX JSON report. The SBOM is saved as a CircleCI artifact.  Trivy supports additional SBOM formats such as CycloneDX, and SARIF.</p> <p>Immortally, the Trivy scan to generate the SBOM happens in the same job as the building of the application. This ensures that the software and dependencies used to build the artifact are tracked.</p> <pre><code>  package:\n    docker:\n      - image: cimg/openjdk:17.0\n    working_directory: &lt;&lt; pipeline.parameters.working-dir &gt;&gt;\n    environment:\n      TRIVY_DB_REPOSITORY: public.ecr.aws/aquasecurity/trivy-db\n    steps:\n      - checkout:\n          path: ~/aws-deployment-pipeline-reference-architecture\n      - restore_cache:\n          keys:\n            - maven-repo-v1-{{ checksum \"pom.xml\" }}\n      - run:\n          name: Run Maven Verify\n          command: mvn verify --batch-mode --no-transfer-progress\n      - save_cache:\n          paths:\n            - ~/.m2/repository\n          key: maven-repo-v1-{{ checksum \"pom.xml\" }}\n      - trivy/scan:\n          scan-type: fs\n          ignore-unfixed: true\n          format: spdx-json\n          output: &lt;&lt; pipeline.parameters.working-dir &gt;&gt;/dependency-results.sbom.json\n      - store_test_results:\n          path: &lt;&lt; pipeline.parameters.working-dir &gt;&gt;/target/surefire-reports\n      - store_test_results:\n          path: &lt;&lt; pipeline.parameters.working-dir &gt;&gt;/target/soapui-reports\n      - store_artifacts:\n          path: &lt;&lt; pipeline.parameters.working-dir &gt;&gt;/dependency-results.sbom.json\n      - store_artifacts:\n          path: &lt;&lt; pipeline.parameters.working-dir &gt;&gt;/target/spotbugsXml.xml    \n      - store_artifacts:\n          path: &lt;&lt; pipeline.parameters.working-dir &gt;&gt;/target/jmeter/results\n      - store_artifacts:\n          path: &lt;&lt; pipeline.parameters.working-dir &gt;&gt;/target/fruit-api.jar\n      - persist_to_workspace:\n          root: &lt;&lt; pipeline.parameters.working-dir &gt;&gt;\n          paths:\n            - target/fruit-api.jar\n</code></pre>"},{"location":"application-pipeline/ri-circleci-pipeline/#test-beta","title":"Test (Beta)","text":"Launch Environment <p>The infrastructure for each environment is defined in AWS Cloud Development Kit:</p> <pre><code>super(scope, id, props);\n\nconst image = new AssetImage('.', { target: 'build' });\n\nconst appName = Stack.of(this)\n  .stackName.toLowerCase()\n  .replace(`-${Stack.of(this).region}-`, '-');\nconst vpc = new ec2.Vpc(this, 'Vpc', {\n  maxAzs: 3,\n  natGateways: props?.natGateways,\n});\nnew FlowLog(this, 'VpcFlowLog', {\n  resourceType: FlowLogResourceType.fromVpc(vpc),\n});\n\nconst dbName = 'fruits';\nconst dbSecret = new DatabaseSecret(this, 'AuroraSecret', {\n  username: 'fruitapi',\n  secretName: `${appName}-DB`,\n});\n\nconst db = new DatabaseCluster(this, 'Database', {\n  engine: DatabaseClusterEngine.auroraMysql({\n    version: AuroraMysqlEngineVersion.VER_3_07_1,\n  }),\n  credentials: Credentials.fromSecret(dbSecret),\n  writer: ClusterInstance.serverlessV2('writer'),\n  defaultDatabaseName: dbName,\n  serverlessV2MaxCapacity: 2,\n  serverlessV2MinCapacity: 0.5,\n  vpc,\n  clusterIdentifier: appName,\n  storageEncrypted: true,\n});\n\nconst cluster = new ecs.Cluster(this, 'Cluster', {\n  vpc,\n  containerInsights: true,\n  clusterName: appName,\n});\nconst appLogGroup = new LogGroup(this, 'AppLogGroup', {\n  retention: RetentionDays.ONE_WEEK,\n  logGroupName: `/aws/ecs/service/${appName}`,\n  removalPolicy: RemovalPolicy.DESTROY,\n});\nlet deploymentConfig: IEcsDeploymentConfig | undefined = undefined;\nif (props?.deploymentConfigName) {\n  deploymentConfig = EcsDeploymentConfig.fromEcsDeploymentConfigName(\n    this,\n    'DeploymentConfig',\n    props.deploymentConfigName,\n  );\n}\nconst appConfigEnabled =\n  props?.appConfigRoleArn !== undefined &amp;&amp;\n  props.appConfigRoleArn.length &gt; 0;\nconst service = new ApplicationLoadBalancedCodeDeployedFargateService(\n  this,\n  'Api',\n  {\n    cluster,\n    capacityProviderStrategies: [\n      {\n        capacityProvider: 'FARGATE_SPOT',\n        weight: 1,\n      },\n    ],\n    minHealthyPercent: 50,\n    maxHealthyPercent: 200,\n    desiredCount: 3,\n    cpu: 512,\n    memoryLimitMiB: 1024,\n    taskImageOptions: {\n      image,\n      containerName: 'api',\n      containerPort: 8080,\n      family: appName,\n      logDriver: AwsLogDriver.awsLogs({\n        logGroup: appLogGroup,\n        streamPrefix: 'service',\n      }),\n      secrets: {\n        SPRING_DATASOURCE_USERNAME: Secret.fromSecretsManager(\n          dbSecret,\n          'username',\n        ),\n        SPRING_DATASOURCE_PASSWORD: Secret.fromSecretsManager(\n          dbSecret,\n          'password',\n        ),\n      },\n      environment: {\n        SPRING_DATASOURCE_URL: `jdbc:mysql://${db.clusterEndpoint.hostname}:${db.clusterEndpoint.port}/${dbName}`,\n        APPCONFIG_AGENT_APPLICATION:\n          this.node.tryGetContext('workloadName'),\n        APPCONFIG_AGENT_ENVIRONMENT:\n          this.node.tryGetContext('environmentName'),\n        APPCONFIG_AGENT_ENABLED: appConfigEnabled.toString(),\n      },\n    },\n    deregistrationDelay: Duration.seconds(5),\n    responseTimeAlarmThreshold: Duration.seconds(3),\n    targetHealthCheck: {\n      healthyThresholdCount: 2,\n      unhealthyThresholdCount: 2,\n      interval: Duration.seconds(60),\n      path: '/actuator/health',\n    },\n    deploymentConfig,\n    terminationWaitTime: Duration.minutes(5),\n    apiCanaryTimeout: Duration.seconds(5),\n    apiTestSteps: [\n      {\n        name: 'getAll',\n        path: '/api/fruits',\n        jmesPath: 'length(@)',\n        expectedValue: 5,\n      },\n    ],\n  },\n);\n\nif (appConfigEnabled) {\n  service.taskDefinition.addContainer('appconfig-agent', {\n    image: ecs.ContainerImage.fromRegistry(\n      'public.ecr.aws/aws-appconfig/aws-appconfig-agent:2.x',\n    ),\n    essential: false,\n    logging: AwsLogDriver.awsLogs({\n      logGroup: appLogGroup,\n      streamPrefix: 'service',\n    }),\n    environment: {\n      SERVICE_REGION: this.region,\n      ROLE_ARN: props!.appConfigRoleArn!,\n      ROLE_SESSION_NAME: appName,\n      LOG_LEVEL: 'info',\n    },\n    portMappings: [{ containerPort: 2772 }],\n  });\n\n  service.taskDefinition.addToTaskRolePolicy(\n    new PolicyStatement({\n      actions: ['sts:AssumeRole'],\n      resources: [props!.appConfigRoleArn!],\n    }),\n  );\n}\n\nservice.service.connections.allowTo(\n  db,\n  ec2.Port.tcp(db.clusterEndpoint.port),\n);\n\nthis.apiUrl = new CfnOutput(this, 'endpointUrl', {\n  value: `http://${service.listener.loadBalancer.loadBalancerDnsName}`,\n});\n</code></pre> <p>The CDK deployment is then performed for each environment. The deployment steps are the same for each environment with only the defined parameters changing like <code>environment</code> and <code>region</code>. Using CircleCI's reusable config, you can defined the deployment job once and use that job for other deployments. </p> <p>Here is the generic deployment job:</p> <pre><code>  deploy:\n      docker:\n        - image: cimg/aws:2024.03\n      parameters:\n        environment:\n          type: string\n          default: Beta\n        stack-name:\n          type: string\n          default: fruit-api\n        region:\n          type: string\n          default: us-east-1\n        cdk-context:\n          type: string\n          default: deploymentConfigurationName=CodeDeployDefault.ECSCanary10Percent5Minutes\n      working_directory: &lt;&lt; pipeline.parameters.working-dir &gt;&gt;\n      steps:\n        - checkout:\n            path: ~/aws-deployment-pipeline-reference-architecture\n        - attach_workspace:\n            at: &lt;&lt; pipeline.parameters.working-dir &gt;&gt;\n        - setup_remote_docker:\n            docker_layer_caching: true\n        - aws-cli/setup:\n            profile_name: default\n            role_arn: &lt;&lt; pipeline.parameters.aws-role-arn &gt;&gt;\n            role_session_name: default\n        - install-cdk\n        - run:\n            name: Set CDK Env Vars\n            command: |\n              echo \"export CDK_DEFAULT_ACCOUNT=&lt;&lt; pipeline.parameters.aws-account-id &gt;&gt;\" &gt;&gt; $BASH_ENV\n              echo \"export CDK_DEFAULT_REGION=&lt;&lt; parameters.region &gt;&gt;\" &gt;&gt; $BASH_ENV\n        - run:\n            name: Bootstrap CDK Environment\n            command: cdk bootstrap aws://$CDK_DEFAULT_ACCOUNT/$CDK_DEFAULT_REGION\n        - run:\n            name: Deploy CDK Stack &lt;&lt; parameters.stack-name &gt;&gt;\n            command: |\n              cdk deploy &lt;&lt; parameters.stack-name &gt;&gt; \\\n                --region &lt;&lt; parameters.region &gt;&gt; \\\n                --outputs-file cdk-output.json \\\n                --context &lt;&lt; parameters.cdk-context &gt;&gt; \\\n                --require-approval never\n            no_output_timeout: 20m\n        - run:\n            command: cat cdk-output.json\n        - store_artifacts:\n            path: &lt;&lt; pipeline.parameters.working-dir &gt;&gt;/cdk-output.json\n        - persist_to_workspace:\n            root: &lt;&lt; pipeline.parameters.working-dir &gt;&gt;\n            paths:\n              - cdk-output.json\n</code></pre> <p>For deploying to the Beta environment the follow job is used:</p> <pre><code>    - deploy:\n        name: Beta Deploy\n        requires:\n          - Verify CDK Synth\n          - Run Trivy Scan\n          - Build Application\n          - Scanning for Secrets with GitGuardian\n</code></pre> Database Deploy <p>Spring Boot is configured to run Liquibase on startup. This reads the configuration in <code>src/main/resources/db/changelog/db.changelog-master.yml</code> to define the tables and initial data for the database:</p> <pre><code>databaseChangeLog:\n   - changeSet:\n       id: \"1\"\n       author: AWS\n       changes:\n       - createTable:\n           tableName: fruit\n           columns:\n           - column:\n               name: id\n               type: bigint\n               autoIncrement: true\n               constraints:\n                   primaryKey:  true\n                   nullable:  false\n           - column:\n               name: name\n               type: varchar(250)\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Apple\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Orange\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Banana\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Cherry\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Grape\n\n   - changeSet:\n       id: \"2\"\n       author: AWS\n       changes:\n       - addColumn:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               type: varchar(250)\n               constraints:\n                 nullable: true\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: pome\n           where: name='Apple'\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: berry\n           where: name='Orange'\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: berry\n           where: name='Banana'\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: drupe\n           where: name='Cherry'\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: berry\n           where: name='Grape'\n</code></pre> Deploy Software <p>The Launch Environment action above creates a new Amazon ECS Task Definition for the new docker image and then updates the Amazon ECS Service to use the new Task Definition.</p> Integration Tests <p>Integration tests are preformed during the Build Source action. They are defined with SoapUI in <code>fruit-api-soapui-project.xml</code>. They are executed by Maven in the <code>integration-test</code> phase using plugins in <code>pom.xml</code>.  Spring Boot is configure to start a local instance of the application with an H2 database during the <code>pre-integration-test</code> phase and then to terminate on the <code>post-integration-test</code> phase.  The results of the unit tests are then uploaded to CircleCI Test Insights to track test performance and trends over time.</p> <pre><code>&lt;plugins&gt;\n    &lt;plugin&gt;\n        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n        &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;\n        &lt;executions&gt;\n            &lt;execution&gt;\n                &lt;id&gt;pre-integration-test&lt;/id&gt;\n                &lt;goals&gt;\n                    &lt;goal&gt;start&lt;/goal&gt;\n                &lt;/goals&gt;\n            &lt;/execution&gt;\n            &lt;execution&gt;\n                &lt;id&gt;post-integration-test&lt;/id&gt;\n                &lt;goals&gt;\n                    &lt;goal&gt;stop&lt;/goal&gt;\n                &lt;/goals&gt;\n            &lt;/execution&gt;\n        &lt;/executions&gt;\n    &lt;/plugin&gt;\n    &lt;plugin&gt;\n        &lt;groupId&gt;com.smartbear.soapui&lt;/groupId&gt;\n        &lt;artifactId&gt;soapui-maven-plugin&lt;/artifactId&gt;\n        &lt;version&gt;5.7.0&lt;/version&gt;\n        &lt;configuration&gt;\n            &lt;junitReport&gt;true&lt;/junitReport&gt;\n            &lt;outputFolder&gt;target/soapui-reports&lt;/outputFolder&gt;\n            &lt;endpoint&gt;${soapui.endpoint}&lt;/endpoint&gt;\n        &lt;/configuration&gt;\n        &lt;executions&gt;\n            &lt;execution&gt;\n                &lt;phase&gt;integration-test&lt;/phase&gt;\n                &lt;goals&gt;\n                    &lt;goal&gt;test&lt;/goal&gt;\n                &lt;/goals&gt;\n            &lt;/execution&gt;\n        &lt;/executions&gt;\n    &lt;/plugin&gt;\n&lt;/plugins&gt;\n</code></pre> Acceptance Tests <p>Acceptance tests are preformed after the Launch Environment and Deploy Software actions:</p> <p>The tests are defined with SoapUI in <code>fruit-api-soapui-project.xml</code>. They are executed by Maven with the endpoint overridden to the URL from the CloudFormation output. A test job is added after each deployment in the CircleCI workflow to run SoapUI.</p> <p>Just like the deployment, you can use CircleCI's reusable config, have a generic test job. The test steps are the same for each environment with only the defined parameters changing like <code>environment</code> and <code>region</code>.</p> <p>Here is the generic test job:</p> <pre><code>  test:\n      docker:\n        - image: cimg/openjdk:17.0\n      parameters:\n        environment:\n          type: string\n          default: Beta\n        region:\n          type: string\n          default: us-east-1\n        threads:\n          type: integer\n          default: 300\n        duration:\n          type: integer\n          default: 300\n        throughput:\n          type: integer\n          default: 6000\n      working_directory: &lt;&lt; pipeline.parameters.working-dir &gt;&gt;\n      steps:\n        - checkout:\n            path: ~/aws-deployment-pipeline-reference-architecture\n        - attach_workspace:\n            at: &lt;&lt; pipeline.parameters.working-dir &gt;&gt;\n        - run:\n            name: Extract endpointUrl from CDK Output\n            command: |\n              # Extract the endpointUrl from the CDK output JSON\n              if [ -f cdk-output.json ]; then\n                endpoint_url=$(jq -r '.[\"fruit-api\"].endpointUrl' cdk-output.json)\n                echo \"export ENDPOINT_URL=$endpoint_url\" &gt;&gt; $BASH_ENV\n                echo \"Endpoint URL extracted: $endpoint_url\"\n              else\n                echo \"CDK output file not found.\"\n                exit 1\n              fi\n        - restore_cache:\n            keys:\n              - maven-repo-v1-{{ checksum \"pom.xml\" }}\n        - run:\n            name: Run SoapUI Tests against &lt;&lt; parameters.environment &gt;&gt;\n            command: mvn --batch-mode --no-transfer-progress soapui:test -Dsoapui.endpoint=${ENDPOINT_URL}\n        - run:\n            name: Run JMeter Tests against &lt;&lt; parameters.environment &gt;&gt;\n            command: mvn --batch-mode --no-transfer-progress compile jmeter:jmeter jmeter:results -Djmeter.endpoint=${ENDPOINT_URL} -Djmeter.threads=&lt;&lt; parameters.threads &gt;&gt; -Djmeter.duration=&lt;&lt; parameters.duration &gt;&gt; -Djmeter.throughput=&lt;&lt; parameters.throughput &gt;&gt;\n        - save_cache:\n            paths:\n              - ~/.m2/repository\n            key: maven-repo-v1-{{ checksum \"pom.xml\" }}      \n        - store_test_results:\n            path: &lt;&lt; pipeline.parameters.working-dir &gt;&gt;/target/soapui-reports\n        - store_artifacts:\n            path: &lt;&lt; pipeline.parameters.working-dir &gt;&gt;/target/jmeter/results\n</code></pre> <p>For testing to the Beta environment the follow job is used:</p> <pre><code>    - test:\n        name: Beta Test\n        environment: Beta\n        requires:\n          - \"Beta Deploy\"\n</code></pre> <p>The results of the unit tests are uploaded to CircleCI Test Insights to be tracked over time.</p>"},{"location":"application-pipeline/ri-circleci-pipeline/#test-gamma","title":"Test (Gamma)","text":"Launch Environment <p>The infrastructure for each environment is defined in AWS Cloud Development Kit:</p> <pre><code>super(scope, id, props);\n\nconst image = new AssetImage('.', { target: 'build' });\n\nconst appName = Stack.of(this)\n  .stackName.toLowerCase()\n  .replace(`-${Stack.of(this).region}-`, '-');\nconst vpc = new ec2.Vpc(this, 'Vpc', {\n  maxAzs: 3,\n  natGateways: props?.natGateways,\n});\nnew FlowLog(this, 'VpcFlowLog', {\n  resourceType: FlowLogResourceType.fromVpc(vpc),\n});\n\nconst dbName = 'fruits';\nconst dbSecret = new DatabaseSecret(this, 'AuroraSecret', {\n  username: 'fruitapi',\n  secretName: `${appName}-DB`,\n});\n\nconst db = new DatabaseCluster(this, 'Database', {\n  engine: DatabaseClusterEngine.auroraMysql({\n    version: AuroraMysqlEngineVersion.VER_3_07_1,\n  }),\n  credentials: Credentials.fromSecret(dbSecret),\n  writer: ClusterInstance.serverlessV2('writer'),\n  defaultDatabaseName: dbName,\n  serverlessV2MaxCapacity: 2,\n  serverlessV2MinCapacity: 0.5,\n  vpc,\n  clusterIdentifier: appName,\n  storageEncrypted: true,\n});\n\nconst cluster = new ecs.Cluster(this, 'Cluster', {\n  vpc,\n  containerInsights: true,\n  clusterName: appName,\n});\nconst appLogGroup = new LogGroup(this, 'AppLogGroup', {\n  retention: RetentionDays.ONE_WEEK,\n  logGroupName: `/aws/ecs/service/${appName}`,\n  removalPolicy: RemovalPolicy.DESTROY,\n});\nlet deploymentConfig: IEcsDeploymentConfig | undefined = undefined;\nif (props?.deploymentConfigName) {\n  deploymentConfig = EcsDeploymentConfig.fromEcsDeploymentConfigName(\n    this,\n    'DeploymentConfig',\n    props.deploymentConfigName,\n  );\n}\nconst appConfigEnabled =\n  props?.appConfigRoleArn !== undefined &amp;&amp;\n  props.appConfigRoleArn.length &gt; 0;\nconst service = new ApplicationLoadBalancedCodeDeployedFargateService(\n  this,\n  'Api',\n  {\n    cluster,\n    capacityProviderStrategies: [\n      {\n        capacityProvider: 'FARGATE_SPOT',\n        weight: 1,\n      },\n    ],\n    minHealthyPercent: 50,\n    maxHealthyPercent: 200,\n    desiredCount: 3,\n    cpu: 512,\n    memoryLimitMiB: 1024,\n    taskImageOptions: {\n      image,\n      containerName: 'api',\n      containerPort: 8080,\n      family: appName,\n      logDriver: AwsLogDriver.awsLogs({\n        logGroup: appLogGroup,\n        streamPrefix: 'service',\n      }),\n      secrets: {\n        SPRING_DATASOURCE_USERNAME: Secret.fromSecretsManager(\n          dbSecret,\n          'username',\n        ),\n        SPRING_DATASOURCE_PASSWORD: Secret.fromSecretsManager(\n          dbSecret,\n          'password',\n        ),\n      },\n      environment: {\n        SPRING_DATASOURCE_URL: `jdbc:mysql://${db.clusterEndpoint.hostname}:${db.clusterEndpoint.port}/${dbName}`,\n        APPCONFIG_AGENT_APPLICATION:\n          this.node.tryGetContext('workloadName'),\n        APPCONFIG_AGENT_ENVIRONMENT:\n          this.node.tryGetContext('environmentName'),\n        APPCONFIG_AGENT_ENABLED: appConfigEnabled.toString(),\n      },\n    },\n    deregistrationDelay: Duration.seconds(5),\n    responseTimeAlarmThreshold: Duration.seconds(3),\n    targetHealthCheck: {\n      healthyThresholdCount: 2,\n      unhealthyThresholdCount: 2,\n      interval: Duration.seconds(60),\n      path: '/actuator/health',\n    },\n    deploymentConfig,\n    terminationWaitTime: Duration.minutes(5),\n    apiCanaryTimeout: Duration.seconds(5),\n    apiTestSteps: [\n      {\n        name: 'getAll',\n        path: '/api/fruits',\n        jmesPath: 'length(@)',\n        expectedValue: 5,\n      },\n    ],\n  },\n);\n\nif (appConfigEnabled) {\n  service.taskDefinition.addContainer('appconfig-agent', {\n    image: ecs.ContainerImage.fromRegistry(\n      'public.ecr.aws/aws-appconfig/aws-appconfig-agent:2.x',\n    ),\n    essential: false,\n    logging: AwsLogDriver.awsLogs({\n      logGroup: appLogGroup,\n      streamPrefix: 'service',\n    }),\n    environment: {\n      SERVICE_REGION: this.region,\n      ROLE_ARN: props!.appConfigRoleArn!,\n      ROLE_SESSION_NAME: appName,\n      LOG_LEVEL: 'info',\n    },\n    portMappings: [{ containerPort: 2772 }],\n  });\n\n  service.taskDefinition.addToTaskRolePolicy(\n    new PolicyStatement({\n      actions: ['sts:AssumeRole'],\n      resources: [props!.appConfigRoleArn!],\n    }),\n  );\n}\n\nservice.service.connections.allowTo(\n  db,\n  ec2.Port.tcp(db.clusterEndpoint.port),\n);\n\nthis.apiUrl = new CfnOutput(this, 'endpointUrl', {\n  value: `http://${service.listener.loadBalancer.loadBalancerDnsName}`,\n});\n</code></pre> <p>The CDK deployment is then performed for each environment. Since the deployment is the same as Beta, but with the <code>environment</code> and <code>region</code> parameters changed, you can refer back to the Beta deployment for the CircleCI config.</p> <p>Here is the unique Gamma deployment job: </p> <pre><code>    - deploy:\n        name: \"&lt;&lt; matrix.environment &gt;&gt; &lt;&lt; matrix.region&gt;&gt; Deploy\"\n        matrix:\n          parameters:\n            environment: [Gamma]\n            region: [us-east-1, us-west-2]\n        requires:\n          - \"Beta Test\"\n        filters:\n          branches:\n            only:\n              - main\n</code></pre> Database Deploy <p>Spring Boot is configured to run Liquibase on startup. This reads the configuration in <code>src/main/resources/db/changelog/db.changelog-master.yml</code> to define the tables and initial data for the database:</p> <pre><code>databaseChangeLog:\n   - changeSet:\n       id: \"1\"\n       author: AWS\n       changes:\n       - createTable:\n           tableName: fruit\n           columns:\n           - column:\n               name: id\n               type: bigint\n               autoIncrement: true\n               constraints:\n                   primaryKey:  true\n                   nullable:  false\n           - column:\n               name: name\n               type: varchar(250)\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Apple\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Orange\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Banana\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Cherry\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Grape\n\n   - changeSet:\n       id: \"2\"\n       author: AWS\n       changes:\n       - addColumn:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               type: varchar(250)\n               constraints:\n                 nullable: true\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: pome\n           where: name='Apple'\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: berry\n           where: name='Orange'\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: berry\n           where: name='Banana'\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: drupe\n           where: name='Cherry'\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: berry\n           where: name='Grape'\n</code></pre> Deploy Software <p>The Launch Environment action above creates a new Amazon ECS Task Definition for the new docker image and then updates the Amazon ECS Service to use the new Task Definition.</p> Application Monitoring &amp; Logging <p>Amazon ECS uses Amazon CloudWatch Metrics and Amazon CloudWatch Logs for observability by default.</p> Synthetic Tests <p>Amazon CloudWatch Synthetics is used to continuously deliver traffic to the application and assert that requests are successful and responses are received within a given threshold. The canary is defined via CDK using the @cdklabs/cdk-ecs-codedeploy construct:</p> <pre><code>const service = new ApplicationLoadBalancedCodeDeployedFargateService(this, 'Api', {\n  ...\n\n  apiCanaryTimeout: Duration.seconds(5),\n  apiTestSteps: [{\n    name: 'getAll',\n    path: '/api/fruits',\n    jmesPath: 'length(@)',\n    expectedValue: 5,\n  }],\n</code></pre> Performance Tests <p>Apache JMeter is used to run performance tests against the deployed application. The tests are stored in <code>src/test/jmeter</code> and added to the CircleCI workflow:</p> <p>Just like the deployment, you can use CircleCI's reusable config, have a generic test job. Since the testing is the same as Beta, but with the <code>environment</code> and <code>region</code> parameters changed, you can refer back to the Beta testing for the CircleCI config.</p> <p>The test steps are the same for each environment with only the defined parameters changing like <code>environment</code> and <code>region</code>.</p> <p>Here is the unique Gamma deployment job: </p> <pre><code>  - test:\n  name: \"&lt;&lt; matrix.environment &gt;&gt; &lt;&lt; matrix.region&gt;&gt; Test\"\n  matrix:\n    parameters:\n      environment: [Gamma]\n      region: [us-east-1, us-west-2]\n  requires:\n    - &lt;&lt; matrix.environment &gt;&gt; &lt;&lt; matrix.region&gt;&gt; Deploy\n  filters:\n    branches:\n      only:\n        - main\n</code></pre> Resilience Tests <p><code>Not Implemented</code></p> Dynamic Application Security Testing (DAST) <p><code>Not Implemented</code></p>"},{"location":"application-pipeline/ri-circleci-pipeline/#prod","title":"Prod","text":"Manual Approval <p>For manual approvals, you can use CircleCI\u2019s hold job, which pauses the pipeline and waits for someone to approve it before proceeding. In this reference architecture, manual approvals are used to gate deployments to production. If code is pushed to a development or non-production branch, the pipeline will run all jobs up to the Gamma deployment stage. For production branches (in this case, <code>main</code>), the pipeline will deploy to Gamma, and if successful, a manual approval gate will activate. Once a designated individual or group approves this job, the pipeline will continue to deploy to production.</p> <p></p> Database Deploy <p>Spring Boot is configured to run Liquibase on startup. This reads the configuration in <code>src/main/resources/db/changelog/db.changelog-master.yml</code> to define the tables and initial data for the database:</p> <pre><code>databaseChangeLog:\n   - changeSet:\n       id: \"1\"\n       author: AWS\n       changes:\n       - createTable:\n           tableName: fruit\n           columns:\n           - column:\n               name: id\n               type: bigint\n               autoIncrement: true\n               constraints:\n                   primaryKey:  true\n                   nullable:  false\n           - column:\n               name: name\n               type: varchar(250)\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Apple\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Orange\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Banana\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Cherry\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Grape\n\n   - changeSet:\n       id: \"2\"\n       author: AWS\n       changes:\n       - addColumn:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               type: varchar(250)\n               constraints:\n                 nullable: true\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: pome\n           where: name='Apple'\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: berry\n           where: name='Orange'\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: berry\n           where: name='Banana'\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: drupe\n           where: name='Cherry'\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: berry\n           where: name='Grape'\n</code></pre> Progressive Deployment <p>Progressive deployment is implemented with AWS CodeDeploy for ECS. CodeDeploy performs a linear blue/green by deploying the new task definition as a new task with a separate target group and then shifting 10% of traffic every minute until all traffic is shifted. A CloudWatch alarm is monitored by CodeDeploy and an automatic rollback is triggered if the alarm exceeds the threshold.</p> <p></p> <p>Implementation of this type deployment presents challenges due to the following limitations:</p> <ul> <li>aws/aws-cdk #19163 - CDK Pipelines aren't intended to be used with CodeDeploy actions.</li> <li>AWS CloudFormation User Guide - The use of <code>AWS::CodeDeploy::BlueGreen</code> hooks and <code>AWS::CodeDeployBlueGreen</code> restricts the types of changes that can be made. Additionally, you can't use auto-rollback capabilities of CodeDeploy.</li> <li>aws/aws-cdk #5170 - CDK doesn't support defining CloudFormation rollback triggers. This rules out CloudFormation based blue/green deployments.</li> </ul> <p>The solution was to use the @cdklabs/cdk-ecs-codedeploy construct from the Construct Hub which addresses aws/aws-cdk #1559 - Lack of support for Blue/Green ECS Deployment in CDK. </p> <pre><code>const service = new ApplicationLoadBalancedCodeDeployedFargateService(this, 'Api', {\n  cluster,\n  capacityProviderStrategies: [\n    {\n      capacityProvider: 'FARGATE_SPOT',\n      weight: 1,\n    },\n  ],\n  minHealthyPercent: 50,\n  maxHealthyPercent: 200,\n  desiredCount: 3,\n  cpu: 512,\n  memoryLimitMiB: 1024,\n  taskImageOptions: {\n    image,\n    containerName: 'api',\n    containerPort: 8080,\n    family: appName,\n    logDriver: AwsLogDriver.awsLogs({\n      logGroup: appLogGroup,\n      streamPrefix: 'service',\n    }),\n    secrets: {\n      SPRING_DATASOURCE_USERNAME: Secret.fromSecretsManager( dbSecret, 'username' ),\n      SPRING_DATASOURCE_PASSWORD: Secret.fromSecretsManager( dbSecret, 'password' ),\n    },\n    environment: {\n      SPRING_DATASOURCE_URL: `jdbc:mysql://${db.clusterEndpoint.hostname}:${db.clusterEndpoint.port}/${dbName}`,\n    },\n  },\n  deregistrationDelay: Duration.seconds(5),\n  responseTimeAlarmThreshold: Duration.seconds(3),\n  healthCheck: {\n    healthyThresholdCount: 2,\n    unhealthyThresholdCount: 2,\n    interval: Duration.seconds(60),\n    path: '/actuator/health',\n  },\n  deploymentConfig,\n  terminationWaitTime: Duration.minutes(5),\n  apiCanaryTimeout: Duration.seconds(5),\n  apiTestSteps: [{\n    name: 'getAll',\n    path: '/api/fruits',\n    jmesPath: 'length(@)',\n    expectedValue: 5,\n  }],\n});\n\nthis.apiUrl = new CfnOutput(this, 'endpointUrl', {\n  value: `http://${service.listener.loadBalancer.loadBalancerDnsName}`,\n});\n```rrideLogicalId('DeploymentId');\n</code></pre> <p>Deployments are rolled out incrementally across regions in waves using CircleCI\u2019s matrix and parallelism. Each wave consists of a set of regions to deploy to in parallel, allowing you to deploy to multiple regions simultaneously within each wave. Once a wave completes, the pipeline will proceed to the next wave, ensuring a controlled, staggered deployment across regions.</p> Synthetic Tests <p>Amazon CloudWatch Synthetics is used to continuously deliver traffic to the application and assert that requests are successful and responses are received within a given threshold. The canary is defined via CDK using the @cdklabs/cdk-ecs-codedeploy construct:</p> <pre><code>const service = new ApplicationLoadBalancedCodeDeployedFargateService(this, 'Api', {\n  ...\n\n  apiCanaryTimeout: Duration.seconds(5),\n  apiTestSteps: [{\n    name: 'getAll',\n    path: '/api/fruits',\n    jmesPath: 'length(@)',\n    expectedValue: 5,\n  }],\n</code></pre>"},{"location":"application-pipeline/ri-circleci-pipeline/#frequently-asked-questions","title":"Frequently Asked Questions","text":"What operating models does this reference implementation support? <p>This reference implementation can accommodate any operation model with minor updates:</p> <ul> <li>Fully Separated - Restrict the role that CDK uses for CloudFormation execution to only create resources from approved product portfolios in AWS Service Catalog. Ownership of creating the products in Service Catalog is owned by the Platform Engineering team and operational support of Service Catalog is owned by the Platform Operations team. The Platform Engineering team should publish CDK constructs internally that provision AWS resources through Service Catalog. Update the CDK app in the <code>infrastructure/</code> directory to use CDK constructs provided by the <code>Platform Engineering</code> team. Use a CODEOWNERS file to require all changes to the <code>infrastructure/</code> directory be approved by the Application Operations team. Additionally, restrict permissions to the Manual Approval action to only allow members of the Application Operations to approve.</li> <li>Separated AEO and IEO with Centralized Governance - Restrict the role that CDK uses for CloudFormation execution to only create resources from approved product portfolios in AWS Service Catalog. Ownership of creating the products in Service Catalog is owned by the Platform Engineering team and operational support of Service Catalog is owned by the Platform Engineering team. The Platform Engineering team should publish CDK constructs internally that provision AWS resources through Service Catalog. Update the CDK app in the <code>infrastructure/</code> directory to use CDK constructs provided by the <code>Platform Engineering</code> team.</li> <li>Separated AEO and IEO with Decentralized Governance - The Platform Engineering team should publish CDK constructs internally that provision AWS resources in manner that achieve organizational compliance. Update the CDK app in the <code>infrastructure/</code> directory to use CDK constructs provided by the <code>Platform Engineering</code> team.</li> </ul>"},{"location":"application-pipeline/ri-codecatalyst-pipeline/","title":"Amazon CodeCatalyst Pipeline","text":"<p>This presents a reference implementation of the Application Pipeline reference architecture. The pipeline is created with Amazon CodeCatalyst for building the software and performing testing tasks. All the infrastructure for this reference implementation is defined with AWS Cloud Development Kit. The source code for this reference implementation is available in GitHub for review.</p> <p>This reference implementation has been contributed to Amazon CodeCatalyst as blueprint named <code>DevOps deployment pipeline</code>. You can try out this reference implementation in your own CodeCatalyst space by creating a new project from the blueprint.</p> <p></p> Disclaimer <p>This reference implementation is intended to serve as an example of how to accomplish the guidance in the reference architecture using Amazon CodeCatalyst. The reference implementation has intentionally not followed the following AWS Well-Architected best practices to make it accessible by a wider range of customers. Be sure to address these before using parts of this code for any workloads in your own environment:</p> <ul> <li> cdk bootstrap with AdministratorAccess - the default policy used for <code>cdk bootstrap</code> is <code>AdministratorAccess</code> but should be replaced with a more appropriate policy with least privilege in your account.</li> <li> TLS on HTTP endpoint - the listener for the sample application uses HTTP instead of HTTPS to avoid having to create new ACM certificates and Route53 hosted zones. This should be replaced in your account with an <code>HTTPS</code> listener.</li> </ul> <p></p>"},{"location":"application-pipeline/ri-codecatalyst-pipeline/#local-development","title":"Local Development","text":"<p>Developers need fast-feedback for potential issues with their code. Automation should run in their developer workspace to give them feedback before the deployment pipeline runs.</p> Pre-Commit Hooks <p>Pre-Commit hooks are scripts that are executed on the developer's workstation when they try to create a new commit. These hooks have an opportunity to inspect the state of the code before the commit occurs and abort the commit if tests fail. An example of pre-commit hooks are Git hooks.  Examples of tools to configure and store pre-commit hooks as code include but are not limited to husky and pre-commit.</p> <p>The following <code>.pre-commit-config.yaml</code> is added to the repository that will build the code with Maven, run unit tests with JUnit, check for code quality with Checkstyle, run static application security testing with PMD and check for secrets in the code with gitleaks.</p> <pre><code>repos:\n- repo: https://github.com/pre-commit/pre-commit-hooks\n  rev: v2.3.0\n  hooks:\n  -   id: check-yaml\n  -   id: check-json\n  -   id: trailing-whitespace\n- repo: https://github.com/pre-commit/mirrors-eslint\n  rev: v8.23.0\n  hooks:\n  -   id: eslint\n- repo: https://github.com/ejba/pre-commit-maven\n  rev: v0.3.3\n  hooks:\n  -   id: maven-test\n- repo: https://github.com/zricethezav/gitleaks\n  rev: v8.12.0\n  hooks:\n    - id: gitleaks\n</code></pre>"},{"location":"application-pipeline/ri-codecatalyst-pipeline/#source","title":"Source","text":"Application Source Code <p>The application source code can be found in the src/main/java directory. It is intended to serve only as a reference and should be replaced by your own application source code.</p> <p>This reference implementation includes a Spring Boot application that exposes a REST API and uses a database for persistence. The API is implemented in <code>FruitController.java</code>:</p> <pre><code>public class FruitController {\n    /**\n     * JPA repository for fruits.\n     */\n    private final FruitRepository repository;\n\n    /**\n     * Logic to map between entities and DTOs\n     */\n    private final FruitMapper mapper;\n\n    FruitController(final FruitRepository r, final FruitMapper m) {\n        this.repository = r;\n        this.mapper = m;\n    }\n\n    @GetMapping(\"/api/fruits\")\n    List&lt;FruitDTO&gt; all() {\n        return repository.findAll()\n                .stream()\n                .map(mapper::toDto)\n                .collect(Collectors.toList());\n    }\n\n    @PostMapping(\"/api/fruits\")\n    FruitDTO newFruit(@RequestBody final FruitDTO fruit) {\n        return mapper.toDto(repository.save(mapper.toEntity(fruit)));\n    }\n\n    @GetMapping(\"/api/fruits/{id}\")\n    FruitDTO one(@PathVariable final Long id) {\n        return repository.findById(id)\n                .map(mapper::toDto)\n                .orElseThrow(() -&gt; new FruitNotFoundException(id));\n    }\n\n    @PutMapping(\"/api/fruits/{id}\")\n    FruitDTO replaceFruit(\n            @RequestBody final FruitDTO newFruit,\n            @PathVariable final Long id) {\n        newFruit.setId(id);\n        return mapper.toDto(repository.save(mapper.toEntity(newFruit)));\n    }\n\n    @DeleteMapping(\"/api/fruits/{id}\")\n    void deleteFruit(@PathVariable final Long id) {\n        repository.deleteById(id);\n    }\n}\n</code></pre> <p>The application source code is stored in Amazon CodeCatalyst repository that is created and initialized from the blueprint.</p> Test Source Code <p>The test source code can be found in the src/test/java directory. It is intended to serve only as a reference and should be replaced by your own test source code.</p> <p>The reference implementation includes source code for unit, integration and end-to-end testing. Unit and integration tests can be found in <code>src/test/java</code>. For example, <code>FruitControllerWithoutClassificationTest.java</code> performs unit tests of each API path with the JUnit testing library:</p> <pre><code>public void shouldReturnList() throws Exception {\n  when(repository.findAll()).thenReturn(Arrays.asList(new Fruit(\"Mango\", FruitClassification.pome), new Fruit(\"Dragonfruit\", FruitClassification.berry)));\n\n  this.mockMvc.perform(get(\"/api/fruits\")).andDo(print()).andExpect(status().isOk())\n      .andExpect(content().json(\"[{\\\"name\\\": \\\"Mango\\\"}, {\\\"name\\\": \\\"Dragonfruit\\\"}]\"));\n}\n</code></pre> <p>Acceptance tests are preformed with SoapUI and are defined in <code>fruit-api-soapui-project.xml</code>. They are executed by Maven using plugins in <code>pom.xml</code>.</p> Infrastructure Source Code <p>The infrastructure source code can be found in the infrastructure directory. It is intended to serve as a reference but much of the code can also be reused in your own CDK applications.</p> <p>Infrastructure source code defines the deployment of the application are stored in <code>infrastructure/</code> folder and uses AWS Cloud Development Kit.</p> <pre><code>super(scope, id, props);\n\nconst image = new AssetImage('.', { target: 'build' });\n\nconst appName = Stack.of(this)\n  .stackName.toLowerCase()\n  .replace(`-${Stack.of(this).region}-`, '-');\nconst vpc = new ec2.Vpc(this, 'Vpc', {\n  maxAzs: 3,\n  natGateways: props?.natGateways,\n});\nnew FlowLog(this, 'VpcFlowLog', {\n  resourceType: FlowLogResourceType.fromVpc(vpc),\n});\n\nconst dbName = 'fruits';\nconst dbSecret = new DatabaseSecret(this, 'AuroraSecret', {\n  username: 'fruitapi',\n  secretName: `${appName}-DB`,\n});\n\nconst db = new DatabaseCluster(this, 'Database', {\n  engine: DatabaseClusterEngine.auroraMysql({\n    version: AuroraMysqlEngineVersion.VER_3_07_1,\n  }),\n  credentials: Credentials.fromSecret(dbSecret),\n  writer: ClusterInstance.serverlessV2('writer'),\n  defaultDatabaseName: dbName,\n  serverlessV2MaxCapacity: 2,\n  serverlessV2MinCapacity: 0.5,\n  vpc,\n  clusterIdentifier: appName,\n  storageEncrypted: true,\n});\n\nconst cluster = new ecs.Cluster(this, 'Cluster', {\n  vpc,\n  containerInsights: true,\n  clusterName: appName,\n});\nconst appLogGroup = new LogGroup(this, 'AppLogGroup', {\n  retention: RetentionDays.ONE_WEEK,\n  logGroupName: `/aws/ecs/service/${appName}`,\n  removalPolicy: RemovalPolicy.DESTROY,\n});\nlet deploymentConfig: IEcsDeploymentConfig | undefined = undefined;\nif (props?.deploymentConfigName) {\n  deploymentConfig = EcsDeploymentConfig.fromEcsDeploymentConfigName(\n    this,\n    'DeploymentConfig',\n    props.deploymentConfigName,\n  );\n}\nconst appConfigEnabled =\n  props?.appConfigRoleArn !== undefined &amp;&amp;\n  props.appConfigRoleArn.length &gt; 0;\nconst service = new ApplicationLoadBalancedCodeDeployedFargateService(\n  this,\n  'Api',\n  {\n    cluster,\n    capacityProviderStrategies: [\n      {\n        capacityProvider: 'FARGATE_SPOT',\n        weight: 1,\n      },\n    ],\n    minHealthyPercent: 50,\n    maxHealthyPercent: 200,\n    desiredCount: 3,\n    cpu: 512,\n    memoryLimitMiB: 1024,\n    taskImageOptions: {\n      image,\n      containerName: 'api',\n      containerPort: 8080,\n      family: appName,\n      logDriver: AwsLogDriver.awsLogs({\n        logGroup: appLogGroup,\n        streamPrefix: 'service',\n      }),\n      secrets: {\n        SPRING_DATASOURCE_USERNAME: Secret.fromSecretsManager(\n          dbSecret,\n          'username',\n        ),\n        SPRING_DATASOURCE_PASSWORD: Secret.fromSecretsManager(\n          dbSecret,\n          'password',\n        ),\n      },\n      environment: {\n        SPRING_DATASOURCE_URL: `jdbc:mysql://${db.clusterEndpoint.hostname}:${db.clusterEndpoint.port}/${dbName}`,\n        APPCONFIG_AGENT_APPLICATION:\n          this.node.tryGetContext('workloadName'),\n        APPCONFIG_AGENT_ENVIRONMENT:\n          this.node.tryGetContext('environmentName'),\n        APPCONFIG_AGENT_ENABLED: appConfigEnabled.toString(),\n      },\n    },\n    deregistrationDelay: Duration.seconds(5),\n    responseTimeAlarmThreshold: Duration.seconds(3),\n    targetHealthCheck: {\n      healthyThresholdCount: 2,\n      unhealthyThresholdCount: 2,\n      interval: Duration.seconds(60),\n      path: '/actuator/health',\n    },\n    deploymentConfig,\n    terminationWaitTime: Duration.minutes(5),\n    apiCanaryTimeout: Duration.seconds(5),\n    apiTestSteps: [\n      {\n        name: 'getAll',\n        path: '/api/fruits',\n        jmesPath: 'length(@)',\n        expectedValue: 5,\n      },\n    ],\n  },\n);\n\nif (appConfigEnabled) {\n  service.taskDefinition.addContainer('appconfig-agent', {\n    image: ecs.ContainerImage.fromRegistry(\n      'public.ecr.aws/aws-appconfig/aws-appconfig-agent:2.x',\n    ),\n    essential: false,\n    logging: AwsLogDriver.awsLogs({\n      logGroup: appLogGroup,\n      streamPrefix: 'service',\n    }),\n    environment: {\n      SERVICE_REGION: this.region,\n      ROLE_ARN: props!.appConfigRoleArn!,\n      ROLE_SESSION_NAME: appName,\n      LOG_LEVEL: 'info',\n    },\n    portMappings: [{ containerPort: 2772 }],\n  });\n\n  service.taskDefinition.addToTaskRolePolicy(\n    new PolicyStatement({\n      actions: ['sts:AssumeRole'],\n      resources: [props!.appConfigRoleArn!],\n    }),\n  );\n}\n\nservice.service.connections.allowTo(\n  db,\n  ec2.Port.tcp(db.clusterEndpoint.port),\n);\n\nthis.apiUrl = new CfnOutput(this, 'endpointUrl', {\n  value: `http://${service.listener.loadBalancer.loadBalancerDnsName}`,\n});\n</code></pre> <p>Notice that the infrastructure code is written in Typescript which is different from the Application Source Code (Java). This was done intentionally to demonstrate that CDK allows defining infrastructure code in whatever language is most appropriate for the team that owns the use of CDK in the organization.</p> Static Assets <p>There are no static assets used by the sample application.</p> Dependency Manifests <p>All third-party dependencies used by the sample application are define in the <code>pom.xml</code>:</p> <pre><code>&lt;dependencies&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n        &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;\n    &lt;/dependency&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n        &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt;\n    &lt;/dependency&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n        &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;\n    &lt;/dependency&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;org.liquibase&lt;/groupId&gt;\n        &lt;artifactId&gt;liquibase-core&lt;/artifactId&gt;\n    &lt;/dependency&gt;\n&lt;/dependencies&gt;\n</code></pre> Static Configuration <p>Static configuration for the application is defined in <code>src/main/resources/application.yml</code>:</p> <pre><code>spring:\n  application:\n    name: fruit-api\n  main:\n    banner-mode: \"off\"\n  jackson:\n    default-property-inclusion: non_null\n\n\nspringdoc:\n  swagger-ui:\n    path: /swagger-ui\n\nappconfig-agent:\n  environment: alpha\n  log-level-from:\n    configuration: operations\n</code></pre> Database Source Code <p>The database source code can be found in the src/main/resources/db directory. It is intended to serve only as a reference and should be replaced by your own database source code.</p> <p>The code that manages the schema and initial data for the application is defined using Liquibase in <code>src/main/resources/db/changelog/db.changelog-master.yml</code>:</p> <pre><code>databaseChangeLog:\n   - changeSet:\n       id: \"1\"\n       author: AWS\n       changes:\n       - createTable:\n           tableName: fruit\n           columns:\n           - column:\n               name: id\n               type: bigint\n               autoIncrement: true\n               constraints:\n                   primaryKey:  true\n                   nullable:  false\n           - column:\n               name: name\n               type: varchar(250)\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Apple\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Orange\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Banana\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Cherry\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Grape\n\n   - changeSet:\n       id: \"2\"\n       author: AWS\n       changes:\n       - addColumn:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               type: varchar(250)\n               constraints:\n                 nullable: true\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: pome\n           where: name='Apple'\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: berry\n           where: name='Orange'\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: berry\n           where: name='Banana'\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: drupe\n           where: name='Cherry'\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: berry\n           where: name='Grape'\n</code></pre>"},{"location":"application-pipeline/ri-codecatalyst-pipeline/#build","title":"Build","text":"<p>Actions in this stage all run in less than 10 minutes so that developers can take action on fast feedback before moving on to their next task. Each of the actions below are defined as code with AWS Cloud Development Kit.</p> Build Code <p>The Java source code is compiled, unit tested and packaged by Maven. An action is added to the workflow to build and package the source code:</p> <pre><code>  Package:\n    Identifier: aws/build@v1\n    Inputs:\n      Sources:\n        - WorkflowSource\n    Outputs:\n      AutoDiscoverReports:\n        Enabled: true\n        ReportNamePrefix: build\n        SuccessCriteria:\n          PassRate: 100\n      Artifacts:\n        - Name: package\n          Files:\n            - \"**/*\"\n    Configuration:\n      Steps:\n        - Run: mvn verify --batch-mode --no-transfer-progress\n</code></pre> Unit Tests <p>The unit tests are run by Maven at the same time the <code>Build Code</code> action occurs. The results of the unit tests are uploaded to AWS Code Build Test Reports to track over time.</p> <p></p> Code Quality <p>Code quality is enforced through the PMD and Checkstyle Maven plugins:</p> <pre><code>    &lt;plugin&gt;\n        &lt;artifactId&gt;maven-pmd-plugin&lt;/artifactId&gt;\n        &lt;configuration&gt;\n            &lt;printFailingErrors&gt;&lt;/printFailingErrors&gt;\n        &lt;/configuration&gt;\n        &lt;executions&gt;\n            &lt;execution&gt;\n                &lt;phase&gt;test&lt;/phase&gt;\n                &lt;goals&gt;\n                    &lt;goal&gt;check&lt;/goal&gt;\n                &lt;/goals&gt;\n            &lt;/execution&gt;\n        &lt;/executions&gt;\n    &lt;/plugin&gt;\n    &lt;plugin&gt;\n        &lt;artifactId&gt;maven-checkstyle-plugin&lt;/artifactId&gt;\n        &lt;configuration&gt;\n            &lt;printFailingErrors&gt;&lt;/printFailingErrors&gt;\n        &lt;/configuration&gt;\n        &lt;executions&gt;\n            &lt;execution&gt;\n                &lt;phase&gt;test&lt;/phase&gt;\n                &lt;goals&gt;\n                    &lt;goal&gt;check&lt;/goal&gt;\n                &lt;/goals&gt;\n            &lt;/execution&gt;\n        &lt;/executions&gt;\n    &lt;/plugin&gt;\n</code></pre> <p>Additionally, cdk-nag is run against the deployment stack to identify any security issues with the resources being created. The pipeline will fail if any are detected. The following code demonstrates how cdk-nag is called as a part of the build stage. The code also demonstrates how to suppress findings.</p> <pre><code>import { App, Aspects } from 'aws-cdk-lib';\nimport { Annotations, Match, Template } from 'aws-cdk-lib/assertions';\nimport { SynthesisMessage } from 'aws-cdk-lib/cx-api';\nimport { AwsSolutionsChecks, NagSuppressions } from 'cdk-nag';\nimport { DeploymentStack } from '../src/deployment';\n\n\nfunction synthesisMessageToString(sm: SynthesisMessage): string {\n  return `${sm.entry.data} [${sm.id}]`;\n}\nexpect.addSnapshotSerializer({\n  test: (val) =&gt; typeof val === 'string' &amp;&amp; val.match(/^dummy.dkr.ecr.us-east.1/) !== null,\n  serialize: () =&gt; '\"dummy-ecr-image\"',\n});\nexpect.addSnapshotSerializer({\n  test: (val) =&gt; typeof val === 'string' &amp;&amp; val.match(/^[a-f0-9]+\\.zip$/) !== null,\n  serialize: () =&gt; '\"code.zip\"',\n});\n\ndescribe('cdk-nag', () =&gt; {\n  let stack: DeploymentStack;\n  let app: App;\n\n  beforeAll(() =&gt; {\n    const appName = 'fruit-api';\n    const workloadName = 'food';\n    const environmentName = 'unit-test';\n    app = new App({ context: { appName, environmentName, workloadName } });\n    stack = new DeploymentStack(app, 'TestStack', {\n      env: {\n        account: 'dummy',\n        region: 'us-east-1',\n      },\n    });\n    Aspects.of(stack).add(new AwsSolutionsChecks());\n\n    // Suppress CDK-NAG for TaskDefinition role and ecr:GetAuthorizationToken permission\n    NagSuppressions.addResourceSuppressionsByPath(\n      stack,\n      `/${stack.stackName}/Api/TaskDef/ExecutionRole/DefaultPolicy/Resource`,\n      [{ id: 'AwsSolutions-IAM5', reason: 'Allow ecr:GetAuthorizationToken', appliesTo: ['Resource::*'] }],\n    );\n\n    // Suppress CDK-NAG for secret rotation\n    NagSuppressions.addResourceSuppressionsByPath(\n      stack,\n      `/${stack.stackName}/AuroraSecret/Resource`,\n      [{ id: 'AwsSolutions-SMG4', reason: 'Dont require secret rotation' }],\n    );\n\n    // Suppress CDK-NAG for RDS Serverless\n    NagSuppressions.addResourceSuppressionsByPath(\n      stack,\n      `/${stack.stackName}/Database/Resource`,\n      [\n        { id: 'AwsSolutions-RDS6', reason: 'IAM authentication not supported on Serverless v1' },\n        { id: 'AwsSolutions-RDS10', reason: 'Disable delete protection to simplify cleanup of Reference Implementation' },\n        { id: 'AwsSolutions-RDS11', reason: 'Custom port not supported on Serverless v1' },\n        { id: 'AwsSolutions-RDS14', reason: 'Backtrack not supported on Serverless v1' },\n        { id: 'AwsSolutions-RDS16', reason: 'CloudWatch Log Export not supported on Serverless v1' },\n      ],\n    );\n\n    NagSuppressions.addResourceSuppressionsByPath(stack, [\n      `/${stack.stackName}/Api/DeploymentGroup/Deployment/DeploymentProvider/framework-onEvent`,\n      `/${stack.stackName}/Api/DeploymentGroup/Deployment/DeploymentProvider/framework-isComplete`,\n      `/${stack.stackName}/Api/DeploymentGroup/Deployment/DeploymentProvider/framework-onTimeout`,\n      `/${stack.stackName}/Api/DeploymentGroup/Deployment/DeploymentProvider/waiter-state-machine`,\n    ], [\n      { id: 'AwsSolutions-IAM5', reason: 'Unrelated to construct under test' },\n      { id: 'AwsSolutions-L1', reason: 'Unrelated to construct under test' },\n      { id: 'AwsSolutions-SF1', reason: 'Unrelated to construct under test' },\n      { id: 'AwsSolutions-SF2', reason: 'Unrelated to construct under test' },\n    ], true);\n\n    // Ignore findings from access log bucket\n    NagSuppressions.addResourceSuppressionsByPath(stack, [\n      `/${stack.stackName}/Api/AccessLogBucket`,\n    ], [\n      { id: 'AwsSolutions-S1', reason: 'Dont need access logs for access log bucket' },\n      { id: 'AwsSolutions-IAM5', reason: 'Allow resource:*', appliesTo: ['Resource::*'] },\n    ]);\n\n    NagSuppressions.addResourceSuppressionsByPath(stack, [\n      `/${stack.stackName}/Api/Canary/ServiceRole`,\n    ], [{ id: 'AwsSolutions-IAM5', reason: 'Allow resource:*' }]);\n\n    NagSuppressions.addResourceSuppressionsByPath(stack, [\n      `/${stack.stackName}/Api/CanaryArtifactsBucket`,\n    ], [{ id: 'AwsSolutions-S1', reason: 'Dont need access logs for canary bucket' }]);\n\n    NagSuppressions.addResourceSuppressionsByPath(stack, [\n      `/${stack.stackName}/Api/DeploymentGroup/ServiceRole`,\n    ], [\n      { id: 'AwsSolutions-IAM4', reason: 'Allow AWSCodeDeployRoleForECS policy', appliesTo: ['Policy::arn:&lt;AWS::Partition&gt;:iam::aws:policy/AWSCodeDeployRoleForECS'] },\n    ]);\n\n    NagSuppressions.addResourceSuppressionsByPath(stack, [\n      `/${stack.stackName}/Api/DeploymentGroup/Deployment`,\n    ], [\n      {\n        id: 'AwsSolutions-IAM4',\n        reason: 'Allow AWSLambdaBasicExecutionRole policy',\n        appliesTo: ['Policy::arn:&lt;AWS::Partition&gt;:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole'],\n      },\n    ], true);\n\n    NagSuppressions.addResourceSuppressionsByPath(stack, [\n      `/${stack.stackName}/Api/TaskDef`,\n    ], [\n      {\n        id: 'AwsSolutions-ECS2',\n        reason: 'Allow environment variables for configuration of values that are not confidential',\n      },\n    ]);\n\n    NagSuppressions.addResourceSuppressionsByPath(stack, [\n      `/${stack.stackName}/Api/LB/SecurityGroup`,\n    ], [\n      {\n        id: 'AwsSolutions-EC23',\n        reason: 'Allow public inbound access on ELB',\n      },\n    ]);\n  });\n\n  test('Snapshot', () =&gt; {\n    const template = Template.fromStack(stack);\n    expect(template.toJSON()).toMatchSnapshot();\n  });\n\n  test('cdk-nag AwsSolutions Pack errors', () =&gt; {\n    const errors = Annotations.fromStack(stack).findError(\n      '*',\n      Match.stringLikeRegexp('AwsSolutions-.*'),\n    ).map(synthesisMessageToString);\n    expect(errors).toHaveLength(0);\n  });\n\n  test('cdk-nag AwsSolutions Pack warnings', () =&gt; {\n    const warnings = Annotations.fromStack(stack).findWarning(\n      '*',\n      Match.stringLikeRegexp('AwsSolutions-.*'),\n    ).map(synthesisMessageToString);\n    expect(warnings).toHaveLength(0);\n  });\n});\n\ndescribe('Deployment without AppConfig', () =&gt; {\n  let stack: DeploymentStack;\n  let app: App;\n\n  beforeAll(() =&gt; {\n    const appName = 'fruit-api';\n    const environmentName = 'unit-test';\n    app = new App({ context: { appName, environmentName } });\n    stack = new DeploymentStack(app, 'TestStack', {\n      env: {\n        account: 'dummy',\n        region: 'us-east-1',\n      },\n    });\n  });\n\n  test('Snapshot', () =&gt; {\n    const template = Template.fromStack(stack);\n    expect(template.toJSON()).toMatchSnapshot();\n  });\n  test('taskdef', () =&gt; {\n    const template = Template.fromStack(stack);\n    template.hasResourceProperties('AWS::ECS::TaskDefinition', {\n      ContainerDefinitions: [\n        {\n          Environment: [{\n            Name: 'SPRING_DATASOURCE_URL',\n          }, {\n            Name: 'APPCONFIG_AGENT_APPLICATION',\n          }, {\n            Name: 'APPCONFIG_AGENT_ENVIRONMENT',\n            Value: 'unit-test',\n          }, {\n            Name: 'APPCONFIG_AGENT_ENABLED',\n            Value: 'false',\n          }],\n        },\n      ],\n    });\n  });\n});\n\ndescribe('Deployment with AppConfig', () =&gt; {\n  let stack: DeploymentStack;\n  let app: App;\n\n  beforeAll(() =&gt; {\n    const appName = 'fruit-api';\n    const workloadName = 'food';\n    const environmentName = 'unit-test';\n    app = new App({ context: { appName, environmentName, workloadName } });\n    stack = new DeploymentStack(app, 'TestStack', {\n      appConfigRoleArn: 'dummy-role-arn',\n      env: {\n        account: 'dummy',\n        region: 'us-east-1',\n      },\n    });\n  });\n\n  test('Snapshot', () =&gt; {\n    const template = Template.fromStack(stack);\n    expect(template.toJSON()).toMatchSnapshot();\n  });\n  test('taskdef', () =&gt; {\n    const template = Template.fromStack(stack);\n    template.hasResourceProperties('AWS::ECS::TaskDefinition', {\n      ContainerDefinitions: [\n        {\n          Environment: [{\n            Name: 'SPRING_DATASOURCE_URL',\n          }, {\n            Name: 'APPCONFIG_AGENT_APPLICATION',\n            Value: 'food',\n          }, {\n            Name: 'APPCONFIG_AGENT_ENVIRONMENT',\n            Value: 'unit-test',\n          }, {\n            Name: 'APPCONFIG_AGENT_ENABLED',\n            Value: 'true',\n          }],\n        },\n        {\n          Environment: [{\n            Name: 'SERVICE_REGION',\n            Value: 'us-east-1',\n          }, {\n            Name: 'ROLE_ARN',\n            Value: 'dummy-role-arn',\n          }, {\n            Name: 'ROLE_SESSION_NAME',\n          }, {\n            Name: 'LOG_LEVEL',\n            Value: 'info',\n          }],\n        },\n      ],\n    });\n  });\n});\n</code></pre> Secrets Detection <p>Trivy is used to scan the source for secrets. The Trivy GitHub Action is used in the Amazon CodeCatalyst workflow to perform the scan:</p> <pre><code>SCA:\n    Identifier: aws/github-actions-runner@v1\n    Inputs:\n      Sources:\n        - WorkflowSource\n    Configuration:\n      Steps:\n        - name: Trivy Vulnerability Scanner\n          uses: aquasecurity/trivy-action@master\n          with:\n            scan-type: fs\n            ignore-unfixed: true\n            format: cyclonedx\n            output: sbom.json\n            severity: CRITICAL,HIGH\n            security-checks: vuln,config,secret\n</code></pre> Static Application Security Testing (SAST) <p>The SpotBugs Maven plugin is used along with the Find Security Bugs plugin to identify OWASP Top 10 and CWE vulnerabilities in the application source code.</p> Package and Store Artifact(s) <p>AWS Cloud Development Kit handles the packaging and storing of assets during the <code>Synth</code> action and <code>Assets</code> stage. The <code>Synth</code> action generates the CloudFormation templates to be deployed into the subsequent environments along with staging up the files necessary to create a docker image. The <code>Assets</code> stage then performs the docker build step to create a new image and push the image to Amazon ECR repositories in each environment account.</p> <p></p> Software Composition Analysis (SCA) <p>Trivy is used to scan the source for vulnerabilities in its dependencies. The <code>pom.xml</code> and <code>Dockerfile</code> files are scanned for configuration issues or vulnerabilities in any dependencies. The scanning is accomplished by a CDK construct that creates a CodeBuild job to run <code>trivy</code>:</p> <pre><code>SCA:\n    Identifier: aws/github-actions-runner@v1\n    Inputs:\n      Sources:\n        - WorkflowSource\n    Configuration:\n      Steps:\n        - name: Trivy Vulnerability Scanner\n          uses: aquasecurity/trivy-action@master\n          with:\n            scan-type: fs\n            ignore-unfixed: true\n            format: cyclonedx\n            output: sbom.json\n            severity: CRITICAL,HIGH\n            security-checks: vuln,config,secret\n</code></pre> <p>Trivy is also used within the <code>Dockerfile</code> to scan the image after it is built. The <code>docker build</code> will fail if Trivy finds any vulnerabilities in the final image:</p> <pre><code>FROM public.ecr.aws/amazoncorretto/amazoncorretto:17-al2022-jdk as build\nUSER nobody\nWORKDIR /app\nCOPY target/fruit-api.jar /app\nHEALTHCHECK --interval=30s --timeout=5s --start-period=30s --retries=3 CMD /bin/curl --fail --silent localhost:8080/actuator/health | grep UP || exit 1\nENTRYPOINT [\"java\",\"-jar\",\"/app/fruit-api.jar\"]\n\n# Use multi-stage builds to scan newly created image with Trivy. This second stage 'vulnscan'\n# isn't published to Amazon ECR and is never run. It is only used to run the Trivy scan\n# against the newly created image in the 'build' stage.\n#\n# This stage must run as root so Trivy can scan all files in the image, not just\n# those accessible by the nobody user. The user is switched back to 'nobody' at\n# the end to ensure that even if this image is used for something it is done\n# without the 'root' user.\n\nFROM build AS vulnscan\nUSER root\nCOPY --from=aquasec/trivy:latest /usr/local/bin/trivy /usr/local/bin/trivy\nRUN trivy filesystem --exit-code 1 --no-progress --ignore-unfixed -s CRITICAL /\nUSER nobody\n</code></pre> Software Bill of Materials (SBOM) <p>Trivy generates an SBOM in the form of a CycloneDX JSON report. The SBOM is saved as a CodeCatalyst asset.  Trivy supports additional SBOM formats such as SPDX, and SARIF.</p>"},{"location":"application-pipeline/ri-codecatalyst-pipeline/#test-beta","title":"Test (Beta)","text":"Launch Environment <p>The infrastructure for each environment is defined in AWS Cloud Development Kit:</p> <pre><code>super(scope, id, props);\n\nconst image = new AssetImage('.', { target: 'build' });\n\nconst appName = Stack.of(this)\n  .stackName.toLowerCase()\n  .replace(`-${Stack.of(this).region}-`, '-');\nconst vpc = new ec2.Vpc(this, 'Vpc', {\n  maxAzs: 3,\n  natGateways: props?.natGateways,\n});\nnew FlowLog(this, 'VpcFlowLog', {\n  resourceType: FlowLogResourceType.fromVpc(vpc),\n});\n\nconst dbName = 'fruits';\nconst dbSecret = new DatabaseSecret(this, 'AuroraSecret', {\n  username: 'fruitapi',\n  secretName: `${appName}-DB`,\n});\n\nconst db = new DatabaseCluster(this, 'Database', {\n  engine: DatabaseClusterEngine.auroraMysql({\n    version: AuroraMysqlEngineVersion.VER_3_07_1,\n  }),\n  credentials: Credentials.fromSecret(dbSecret),\n  writer: ClusterInstance.serverlessV2('writer'),\n  defaultDatabaseName: dbName,\n  serverlessV2MaxCapacity: 2,\n  serverlessV2MinCapacity: 0.5,\n  vpc,\n  clusterIdentifier: appName,\n  storageEncrypted: true,\n});\n\nconst cluster = new ecs.Cluster(this, 'Cluster', {\n  vpc,\n  containerInsights: true,\n  clusterName: appName,\n});\nconst appLogGroup = new LogGroup(this, 'AppLogGroup', {\n  retention: RetentionDays.ONE_WEEK,\n  logGroupName: `/aws/ecs/service/${appName}`,\n  removalPolicy: RemovalPolicy.DESTROY,\n});\nlet deploymentConfig: IEcsDeploymentConfig | undefined = undefined;\nif (props?.deploymentConfigName) {\n  deploymentConfig = EcsDeploymentConfig.fromEcsDeploymentConfigName(\n    this,\n    'DeploymentConfig',\n    props.deploymentConfigName,\n  );\n}\nconst appConfigEnabled =\n  props?.appConfigRoleArn !== undefined &amp;&amp;\n  props.appConfigRoleArn.length &gt; 0;\nconst service = new ApplicationLoadBalancedCodeDeployedFargateService(\n  this,\n  'Api',\n  {\n    cluster,\n    capacityProviderStrategies: [\n      {\n        capacityProvider: 'FARGATE_SPOT',\n        weight: 1,\n      },\n    ],\n    minHealthyPercent: 50,\n    maxHealthyPercent: 200,\n    desiredCount: 3,\n    cpu: 512,\n    memoryLimitMiB: 1024,\n    taskImageOptions: {\n      image,\n      containerName: 'api',\n      containerPort: 8080,\n      family: appName,\n      logDriver: AwsLogDriver.awsLogs({\n        logGroup: appLogGroup,\n        streamPrefix: 'service',\n      }),\n      secrets: {\n        SPRING_DATASOURCE_USERNAME: Secret.fromSecretsManager(\n          dbSecret,\n          'username',\n        ),\n        SPRING_DATASOURCE_PASSWORD: Secret.fromSecretsManager(\n          dbSecret,\n          'password',\n        ),\n      },\n      environment: {\n        SPRING_DATASOURCE_URL: `jdbc:mysql://${db.clusterEndpoint.hostname}:${db.clusterEndpoint.port}/${dbName}`,\n        APPCONFIG_AGENT_APPLICATION:\n          this.node.tryGetContext('workloadName'),\n        APPCONFIG_AGENT_ENVIRONMENT:\n          this.node.tryGetContext('environmentName'),\n        APPCONFIG_AGENT_ENABLED: appConfigEnabled.toString(),\n      },\n    },\n    deregistrationDelay: Duration.seconds(5),\n    responseTimeAlarmThreshold: Duration.seconds(3),\n    targetHealthCheck: {\n      healthyThresholdCount: 2,\n      unhealthyThresholdCount: 2,\n      interval: Duration.seconds(60),\n      path: '/actuator/health',\n    },\n    deploymentConfig,\n    terminationWaitTime: Duration.minutes(5),\n    apiCanaryTimeout: Duration.seconds(5),\n    apiTestSteps: [\n      {\n        name: 'getAll',\n        path: '/api/fruits',\n        jmesPath: 'length(@)',\n        expectedValue: 5,\n      },\n    ],\n  },\n);\n\nif (appConfigEnabled) {\n  service.taskDefinition.addContainer('appconfig-agent', {\n    image: ecs.ContainerImage.fromRegistry(\n      'public.ecr.aws/aws-appconfig/aws-appconfig-agent:2.x',\n    ),\n    essential: false,\n    logging: AwsLogDriver.awsLogs({\n      logGroup: appLogGroup,\n      streamPrefix: 'service',\n    }),\n    environment: {\n      SERVICE_REGION: this.region,\n      ROLE_ARN: props!.appConfigRoleArn!,\n      ROLE_SESSION_NAME: appName,\n      LOG_LEVEL: 'info',\n    },\n    portMappings: [{ containerPort: 2772 }],\n  });\n\n  service.taskDefinition.addToTaskRolePolicy(\n    new PolicyStatement({\n      actions: ['sts:AssumeRole'],\n      resources: [props!.appConfigRoleArn!],\n    }),\n  );\n}\n\nservice.service.connections.allowTo(\n  db,\n  ec2.Port.tcp(db.clusterEndpoint.port),\n);\n\nthis.apiUrl = new CfnOutput(this, 'endpointUrl', {\n  value: `http://${service.listener.loadBalancer.loadBalancerDnsName}`,\n});\n</code></pre> <p>The CDK deployment is then performed for each environment:</p> <pre><code>Deploy:\n    Identifier: aws/cdk-deploy@v1\n    DependsOn:\n      - Build\n    Inputs:\n      Artifacts:\n        - package\n    Environment:\n      Name: Beta\n      Connections:\n        - Name: beta\n          Role: codecatalyst\n    Configuration:\n      StackName: fruit-api\n      Region: us-west-2\n      Context: '{\"deploymentConfigurationName\":\"CodeDeployDefault.ECSCanary10Percent5Minutes\"}'\n      CfnOutputVariables: '[\"endpointUrl\"]'\n</code></pre> Database Deploy <p>Spring Boot is configured to run Liquibase on startup. This reads the configuration in <code>src/main/resources/db/changelog/db.changelog-master.yml</code> to define the tables and initial data for the database:</p> <pre><code>databaseChangeLog:\n   - changeSet:\n       id: \"1\"\n       author: AWS\n       changes:\n       - createTable:\n           tableName: fruit\n           columns:\n           - column:\n               name: id\n               type: bigint\n               autoIncrement: true\n               constraints:\n                   primaryKey:  true\n                   nullable:  false\n           - column:\n               name: name\n               type: varchar(250)\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Apple\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Orange\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Banana\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Cherry\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Grape\n\n   - changeSet:\n       id: \"2\"\n       author: AWS\n       changes:\n       - addColumn:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               type: varchar(250)\n               constraints:\n                 nullable: true\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: pome\n           where: name='Apple'\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: berry\n           where: name='Orange'\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: berry\n           where: name='Banana'\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: drupe\n           where: name='Cherry'\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: berry\n           where: name='Grape'\n</code></pre> Deploy Software <p>The Launch Environment action above creates a new Amazon ECS Task Definition for the new docker image and then updates the Amazon ECS Service to use the new Task Definition.</p> Integration Tests <p>Integration tests are preformed during the Build Source action. They are defined with SoapUI in <code>fruit-api-soapui-project.xml</code>. They are executed by Maven in the <code>integration-test</code> phase using plugins in <code>pom.xml</code>.  Spring Boot is configure to start a local instance of the application with an H2 database during the <code>pre-integration-test</code> phase and then to terminate on the <code>post-integration-test</code> phase.  The results of the unit tests are uploaded to Amazon CodeCatalyst to track over time.</p> <pre><code>&lt;plugins&gt;\n    &lt;plugin&gt;\n        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n        &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;\n        &lt;executions&gt;\n            &lt;execution&gt;\n                &lt;id&gt;pre-integration-test&lt;/id&gt;\n                &lt;goals&gt;\n                    &lt;goal&gt;start&lt;/goal&gt;\n                &lt;/goals&gt;\n            &lt;/execution&gt;\n            &lt;execution&gt;\n                &lt;id&gt;post-integration-test&lt;/id&gt;\n                &lt;goals&gt;\n                    &lt;goal&gt;stop&lt;/goal&gt;\n                &lt;/goals&gt;\n            &lt;/execution&gt;\n        &lt;/executions&gt;\n    &lt;/plugin&gt;\n    &lt;plugin&gt;\n        &lt;groupId&gt;com.smartbear.soapui&lt;/groupId&gt;\n        &lt;artifactId&gt;soapui-maven-plugin&lt;/artifactId&gt;\n        &lt;version&gt;5.7.0&lt;/version&gt;\n        &lt;configuration&gt;\n            &lt;junitReport&gt;true&lt;/junitReport&gt;\n            &lt;outputFolder&gt;target/soapui-reports&lt;/outputFolder&gt;\n            &lt;endpoint&gt;${soapui.endpoint}&lt;/endpoint&gt;\n        &lt;/configuration&gt;\n        &lt;executions&gt;\n            &lt;execution&gt;\n                &lt;phase&gt;integration-test&lt;/phase&gt;\n                &lt;goals&gt;\n                    &lt;goal&gt;test&lt;/goal&gt;\n                &lt;/goals&gt;\n            &lt;/execution&gt;\n        &lt;/executions&gt;\n    &lt;/plugin&gt;\n&lt;/plugins&gt;\n</code></pre> Acceptance Tests <p>Acceptance tests are preformed after the Launch Environment and Deploy Software actions:</p> <p></p> <p>The tests are defined with SoapUI in <code>fruit-api-soapui-project.xml</code>. They are executed by Maven with the endpoint overridden to the URL from the CloudFormation output. An action is added to the CodeCatalyst workflow to run SoapUI:</p> <pre><code>Test:\n    Identifier: aws/managed-test@v1\n    Inputs:\n      Artifacts:\n        - package\n      Variables:\n        - Name: endpointUrl\n          Value: ${Deploy.endpointUrl}\n    Configuration:\n      Steps:\n        - Run: mvn --batch-mode --no-transfer-progress soapui:test -Dsoapui.endpoint=${endpointUrl}\n        - Run: mvn --batch-mode --no-transfer-progress compile jmeter:jmeter jmeter:results -Djmeter.endpoint=${endpointUrl} -Djmeter.threads=300 -Djmeter.duration=300 -Djmeter.throughput=6000\n    Outputs:\n      AutoDiscoverReports:\n        Enabled: true\n        IncludePaths:\n          - target/soapui-reports/*\n        ReportNamePrefix: Beta\n        SuccessCriteria:\n          PassRate: 100\n</code></pre> <p>The results of the unit tests are uploaded to Amazon CodeCatalyst to track over time.</p>"},{"location":"application-pipeline/ri-codecatalyst-pipeline/#test-gamma","title":"Test (Gamma)","text":"Launch Environment <p>The infrastructure for each environment is defined in AWS Cloud Development Kit:</p> <pre><code>super(scope, id, props);\n\nconst image = new AssetImage('.', { target: 'build' });\n\nconst appName = Stack.of(this)\n  .stackName.toLowerCase()\n  .replace(`-${Stack.of(this).region}-`, '-');\nconst vpc = new ec2.Vpc(this, 'Vpc', {\n  maxAzs: 3,\n  natGateways: props?.natGateways,\n});\nnew FlowLog(this, 'VpcFlowLog', {\n  resourceType: FlowLogResourceType.fromVpc(vpc),\n});\n\nconst dbName = 'fruits';\nconst dbSecret = new DatabaseSecret(this, 'AuroraSecret', {\n  username: 'fruitapi',\n  secretName: `${appName}-DB`,\n});\n\nconst db = new DatabaseCluster(this, 'Database', {\n  engine: DatabaseClusterEngine.auroraMysql({\n    version: AuroraMysqlEngineVersion.VER_3_07_1,\n  }),\n  credentials: Credentials.fromSecret(dbSecret),\n  writer: ClusterInstance.serverlessV2('writer'),\n  defaultDatabaseName: dbName,\n  serverlessV2MaxCapacity: 2,\n  serverlessV2MinCapacity: 0.5,\n  vpc,\n  clusterIdentifier: appName,\n  storageEncrypted: true,\n});\n\nconst cluster = new ecs.Cluster(this, 'Cluster', {\n  vpc,\n  containerInsights: true,\n  clusterName: appName,\n});\nconst appLogGroup = new LogGroup(this, 'AppLogGroup', {\n  retention: RetentionDays.ONE_WEEK,\n  logGroupName: `/aws/ecs/service/${appName}`,\n  removalPolicy: RemovalPolicy.DESTROY,\n});\nlet deploymentConfig: IEcsDeploymentConfig | undefined = undefined;\nif (props?.deploymentConfigName) {\n  deploymentConfig = EcsDeploymentConfig.fromEcsDeploymentConfigName(\n    this,\n    'DeploymentConfig',\n    props.deploymentConfigName,\n  );\n}\nconst appConfigEnabled =\n  props?.appConfigRoleArn !== undefined &amp;&amp;\n  props.appConfigRoleArn.length &gt; 0;\nconst service = new ApplicationLoadBalancedCodeDeployedFargateService(\n  this,\n  'Api',\n  {\n    cluster,\n    capacityProviderStrategies: [\n      {\n        capacityProvider: 'FARGATE_SPOT',\n        weight: 1,\n      },\n    ],\n    minHealthyPercent: 50,\n    maxHealthyPercent: 200,\n    desiredCount: 3,\n    cpu: 512,\n    memoryLimitMiB: 1024,\n    taskImageOptions: {\n      image,\n      containerName: 'api',\n      containerPort: 8080,\n      family: appName,\n      logDriver: AwsLogDriver.awsLogs({\n        logGroup: appLogGroup,\n        streamPrefix: 'service',\n      }),\n      secrets: {\n        SPRING_DATASOURCE_USERNAME: Secret.fromSecretsManager(\n          dbSecret,\n          'username',\n        ),\n        SPRING_DATASOURCE_PASSWORD: Secret.fromSecretsManager(\n          dbSecret,\n          'password',\n        ),\n      },\n      environment: {\n        SPRING_DATASOURCE_URL: `jdbc:mysql://${db.clusterEndpoint.hostname}:${db.clusterEndpoint.port}/${dbName}`,\n        APPCONFIG_AGENT_APPLICATION:\n          this.node.tryGetContext('workloadName'),\n        APPCONFIG_AGENT_ENVIRONMENT:\n          this.node.tryGetContext('environmentName'),\n        APPCONFIG_AGENT_ENABLED: appConfigEnabled.toString(),\n      },\n    },\n    deregistrationDelay: Duration.seconds(5),\n    responseTimeAlarmThreshold: Duration.seconds(3),\n    targetHealthCheck: {\n      healthyThresholdCount: 2,\n      unhealthyThresholdCount: 2,\n      interval: Duration.seconds(60),\n      path: '/actuator/health',\n    },\n    deploymentConfig,\n    terminationWaitTime: Duration.minutes(5),\n    apiCanaryTimeout: Duration.seconds(5),\n    apiTestSteps: [\n      {\n        name: 'getAll',\n        path: '/api/fruits',\n        jmesPath: 'length(@)',\n        expectedValue: 5,\n      },\n    ],\n  },\n);\n\nif (appConfigEnabled) {\n  service.taskDefinition.addContainer('appconfig-agent', {\n    image: ecs.ContainerImage.fromRegistry(\n      'public.ecr.aws/aws-appconfig/aws-appconfig-agent:2.x',\n    ),\n    essential: false,\n    logging: AwsLogDriver.awsLogs({\n      logGroup: appLogGroup,\n      streamPrefix: 'service',\n    }),\n    environment: {\n      SERVICE_REGION: this.region,\n      ROLE_ARN: props!.appConfigRoleArn!,\n      ROLE_SESSION_NAME: appName,\n      LOG_LEVEL: 'info',\n    },\n    portMappings: [{ containerPort: 2772 }],\n  });\n\n  service.taskDefinition.addToTaskRolePolicy(\n    new PolicyStatement({\n      actions: ['sts:AssumeRole'],\n      resources: [props!.appConfigRoleArn!],\n    }),\n  );\n}\n\nservice.service.connections.allowTo(\n  db,\n  ec2.Port.tcp(db.clusterEndpoint.port),\n);\n\nthis.apiUrl = new CfnOutput(this, 'endpointUrl', {\n  value: `http://${service.listener.loadBalancer.loadBalancerDnsName}`,\n});\n</code></pre> <p>The CDK deployment is then performed for each environment:</p> <pre><code>Deploy:\n    Identifier: aws/cdk-deploy@v1\n    DependsOn:\n      - Build\n    Inputs:\n      Artifacts:\n        - package\n    Environment:\n      Name: Beta\n      Connections:\n        - Name: beta\n          Role: codecatalyst\n    Configuration:\n      StackName: fruit-api\n      Region: us-west-2\n      Context: '{\"deploymentConfigurationName\":\"CodeDeployDefault.ECSCanary10Percent5Minutes\"}'\n      CfnOutputVariables: '[\"endpointUrl\"]'\n</code></pre> Database Deploy <p>Spring Boot is configured to run Liquibase on startup. This reads the configuration in <code>src/main/resources/db/changelog/db.changelog-master.yml</code> to define the tables and initial data for the database:</p> <pre><code>databaseChangeLog:\n   - changeSet:\n       id: \"1\"\n       author: AWS\n       changes:\n       - createTable:\n           tableName: fruit\n           columns:\n           - column:\n               name: id\n               type: bigint\n               autoIncrement: true\n               constraints:\n                   primaryKey:  true\n                   nullable:  false\n           - column:\n               name: name\n               type: varchar(250)\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Apple\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Orange\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Banana\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Cherry\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Grape\n\n   - changeSet:\n       id: \"2\"\n       author: AWS\n       changes:\n       - addColumn:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               type: varchar(250)\n               constraints:\n                 nullable: true\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: pome\n           where: name='Apple'\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: berry\n           where: name='Orange'\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: berry\n           where: name='Banana'\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: drupe\n           where: name='Cherry'\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: berry\n           where: name='Grape'\n</code></pre> Deploy Software <p>The Launch Environment action above creates a new Amazon ECS Task Definition for the new docker image and then updates the Amazon ECS Service to use the new Task Definition.</p> Application Monitoring &amp; Logging <p>Amazon ECS uses Amazon CloudWatch Metrics and Amazon CloudWatch Logs for observability by default.</p> Synthetic Tests <p>Amazon CloudWatch Synthetics is used to continuously deliver traffic to the application and assert that requests are successful and responses are received within a given threshold. The canary is defined via CDK using the @cdklabs/cdk-ecs-codedeploy construct:</p> <pre><code>const service = new ApplicationLoadBalancedCodeDeployedFargateService(this, 'Api', {\n  ...\n\n  apiCanaryTimeout: Duration.seconds(5),\n  apiTestSteps: [{\n    name: 'getAll',\n    path: '/api/fruits',\n    jmesPath: 'length(@)',\n    expectedValue: 5,\n  }],\n</code></pre> Performance Tests <p>Apache JMeter is used to run performance tests against the deployed application. The tests are stored in <code>src/test/jmeter</code> and added to the CodeCatalyst workflow:</p> <pre><code>Test:\n    Identifier: aws/managed-test@v1\n    Inputs:\n      Artifacts:\n        - package\n      Variables:\n        - Name: endpointUrl\n          Value: ${Deploy.endpointUrl}\n    Configuration:\n      Steps:\n        - Run: mvn --batch-mode --no-transfer-progress soapui:test -Dsoapui.endpoint=${endpointUrl}\n        - Run: mvn --batch-mode --no-transfer-progress compile jmeter:jmeter jmeter:results -Djmeter.endpoint=${endpointUrl} -Djmeter.threads=300 -Djmeter.duration=300 -Djmeter.throughput=6000\n    Outputs:\n      AutoDiscoverReports:\n        Enabled: true\n        IncludePaths:\n          - target/soapui-reports/*\n        ReportNamePrefix: Beta\n        SuccessCriteria:\n          PassRate: 100\n</code></pre> Resilience Tests <p><code>Not Implemented</code></p> Dynamic Application Security Testing (DAST) <p><code>Not Implemented</code></p>"},{"location":"application-pipeline/ri-codecatalyst-pipeline/#prod","title":"Prod","text":"Manual Approval <p><code>Not Implemented</code></p> Database Deploy <p>Spring Boot is configured to run Liquibase on startup. This reads the configuration in <code>src/main/resources/db/changelog/db.changelog-master.yml</code> to define the tables and initial data for the database:</p> <pre><code>databaseChangeLog:\n   - changeSet:\n       id: \"1\"\n       author: AWS\n       changes:\n       - createTable:\n           tableName: fruit\n           columns:\n           - column:\n               name: id\n               type: bigint\n               autoIncrement: true\n               constraints:\n                   primaryKey:  true\n                   nullable:  false\n           - column:\n               name: name\n               type: varchar(250)\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Apple\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Orange\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Banana\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Cherry\n\n       - insert:\n           tableName: fruit\n           columns:\n           - column:\n               name: name\n               value: Grape\n\n   - changeSet:\n       id: \"2\"\n       author: AWS\n       changes:\n       - addColumn:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               type: varchar(250)\n               constraints:\n                 nullable: true\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: pome\n           where: name='Apple'\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: berry\n           where: name='Orange'\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: berry\n           where: name='Banana'\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: drupe\n           where: name='Cherry'\n\n       - update:\n           tableName: fruit\n           columns:\n           - column:\n               name: classification\n               value: berry\n           where: name='Grape'\n</code></pre> Progressive Deployment <p>Progressive deployment is implemented with AWS CodeDeploy for ECS. CodeDeploy performs a linear blue/green by deploying the new task definition as a new task with a separate target group and then shifting 10% of traffic every minute until all traffic is shifted. A CloudWatch alarm is monitored by CodeDeploy and an automatic rollback is triggered if the alarm exceeds the threshold.</p> <p></p> <p>Implementation of this type deployment presents challenges due to the following limitations:</p> <ul> <li>aws/aws-cdk #19163 - CDK Pipelines aren't intended to be used with CodeDeploy actions.</li> <li>AWS CloudFormation User Guide - The use of <code>AWS::CodeDeploy::BlueGreen</code> hooks and <code>AWS::CodeDeployBlueGreen</code> restricts the types of changes that can be made. Additionally, you can't use auto-rollback capabilities of CodeDeploy.</li> <li>aws/aws-cdk #5170 - CDK doesn't support defining CloudFormation rollback triggers. This rules out CloudFormation based blue/green deployments.</li> </ul> <p>The solution was to use the @cdklabs/cdk-ecs-codedeploy construct from the Construct Hub which addresses aws/aws-cdk #1559 - Lack of support for Blue/Green ECS Deployment in CDK. </p> <pre><code>const service = new ApplicationLoadBalancedCodeDeployedFargateService(this, 'Api', {\n  cluster,\n  capacityProviderStrategies: [\n    {\n      capacityProvider: 'FARGATE_SPOT',\n      weight: 1,\n    },\n  ],\n  minHealthyPercent: 50,\n  maxHealthyPercent: 200,\n  desiredCount: 3,\n  cpu: 512,\n  memoryLimitMiB: 1024,\n  taskImageOptions: {\n    image,\n    containerName: 'api',\n    containerPort: 8080,\n    family: appName,\n    logDriver: AwsLogDriver.awsLogs({\n      logGroup: appLogGroup,\n      streamPrefix: 'service',\n    }),\n    secrets: {\n      SPRING_DATASOURCE_USERNAME: Secret.fromSecretsManager( dbSecret, 'username' ),\n      SPRING_DATASOURCE_PASSWORD: Secret.fromSecretsManager( dbSecret, 'password' ),\n    },\n    environment: {\n      SPRING_DATASOURCE_URL: `jdbc:mysql://${db.clusterEndpoint.hostname}:${db.clusterEndpoint.port}/${dbName}`,\n    },\n  },\n  deregistrationDelay: Duration.seconds(5),\n  responseTimeAlarmThreshold: Duration.seconds(3),\n  healthCheck: {\n    healthyThresholdCount: 2,\n    unhealthyThresholdCount: 2,\n    interval: Duration.seconds(60),\n    path: '/actuator/health',\n  },\n  deploymentConfig,\n  terminationWaitTime: Duration.minutes(5),\n  apiCanaryTimeout: Duration.seconds(5),\n  apiTestSteps: [{\n    name: 'getAll',\n    path: '/api/fruits',\n    jmesPath: 'length(@)',\n    expectedValue: 5,\n  }],\n});\n\nthis.apiUrl = new CfnOutput(this, 'endpointUrl', {\n  value: `http://${service.listener.loadBalancer.loadBalancerDnsName}`,\n});\n```rrideLogicalId('DeploymentId');\n</code></pre> <p>Deployments are made incrementally across regions as waves using the action groups and the CDK deploy action. Each wave contains a list of regions to deploy to in parallel. One wave must fully complete before the next wave starts. The diagram below shows how each wave deploys to 2 regions at a time.</p> <p></p> Synthetic Tests <p>Amazon CloudWatch Synthetics is used to continuously deliver traffic to the application and assert that requests are successful and responses are received within a given threshold. The canary is defined via CDK using the @cdklabs/cdk-ecs-codedeploy construct:</p> <pre><code>const service = new ApplicationLoadBalancedCodeDeployedFargateService(this, 'Api', {\n  ...\n\n  apiCanaryTimeout: Duration.seconds(5),\n  apiTestSteps: [{\n    name: 'getAll',\n    path: '/api/fruits',\n    jmesPath: 'length(@)',\n    expectedValue: 5,\n  }],\n</code></pre>"},{"location":"application-pipeline/ri-codecatalyst-pipeline/#frequently-asked-questions","title":"Frequently Asked Questions","text":"What operating models does this reference implementation support? <p>This reference implementation can accomodate any operation model with minor updates:</p> <ul> <li>Fully Separated - Restrict the role that CDK uses for CloudFormation execution to only create resources from approved product portfolios in AWS Service Catalog. Ownership of creating the products in Service Catalog is owned by the Platform Engineering team and operational support of Service Catalog is owned by the Platform Operations team. The Platform Engineering team should publish CDK constructs internally that provision AWS resources through Service Catalog. Update the CDK app in the <code>infrastructure/</code> directory to use CDK constructs provided by the <code>Platform Engineering</code> team. Use a CODEOWNERS file to require all changes to the <code>infrastructure/</code> directory be approved by the Application Operations team. Additionally, restrict permissions to the Manual Approval action to only allow members of the Application Operations to approve.</li> <li>Separated AEO and IEO with Centralized Governance - Restrict the role that CDK uses for CloudFormation execution to only create resources from approved product portfolios in AWS Service Catalog. Ownership of creating the products in Service Catalog is owned by the Platform Engineering team and operational support of Service Catalog is owned by the Platform Engineering team. The Platform Engineering team should publish CDK constructs internally that provision AWS resources through Service Catalog. Update the CDK app in the <code>infrastructure/</code> directory to use CDK constructs provided by the <code>Platform Engineering</code> team.</li> <li>Separated AEO and IEO with Decentralized Governance - The Platform Engineering team should publish CDK constructs internally that provision AWS resources in manner that achieve organizational compliance. Update the CDK app in the <code>infrastructure/</code> directory to use CDK constructs provided by the <code>Platform Engineering</code> team.</li> </ul>"},{"location":"compute-image-pipeline/","title":"Compute Image Pipeline","text":"<p>The compute image pipeline applies infrastructure as code to create compute images that can be used by application/service deployment pipelines to launch environments from code. As part of an image baking deployment pipeline, run image scanning and infrastructure analysis to identify vulnerable configuration code (e.g., container manifests) or vulnerable runtime images (e.g. EC2 instances and containers). Use static analysis and fully-managed services for preventative and detective controls. On AWS, the output artifacts are often Amazon Machine Images (AMIs) or Container Images published to a container registry.</p>"},{"location":"compute-image-pipeline/#source","title":"Source","text":"<ul> <li>Test source code</li> <li>Infrastructure as code</li> <li>Configuration</li> </ul>"},{"location":"compute-image-pipeline/#pre-commit-hooks","title":"Pre-Commit hooks","text":"<ul> <li>Secrets Detection - Identify secrets such as usernames, passwords, and access keys in code and other files before they are published to a repository by using pre-commit hooks. When discovering secrets, the code push should fail immediately.</li> <li>IDE Plugins - Warn developers in their IDE using plugins and extensions such as. Examples could include markdown linters, yaml/json validators, and flake8/PEP8 code quality analyzers.</li> </ul>"},{"location":"compute-image-pipeline/#build","title":"Build","text":"<p>All actions run In this stage are also run on developers\u2019 local environments prior to code commit and peer review.</p> <ul> <li>Policy as Code - Run preventative automated checks are run to ensure that the code and environments conform to organization/team policy. For example, these checks might alert when code defines volumes or buckets as not encrypted, services not enabled, or endpoints not protected. When policies are violated, AWS recommends the build fails so that developers can fix the errors. (e.g., cfn_nag, CloudFormation Guard, OPA, IAM Access Analyzer, Amazon Inspector Network Reachability, VPC Reachability Analyzer)</li> <li>Secrets Detection &amp; Repo Cleansing - Identify secrets such as usernames, passwords, and access keys. When secrets are discovered, the build fails and all secrets in the source code repo history are purged. (e.g., Amazon CodeGuru Secrets Detection, git-secrets)</li> <li>Vulnerability Assessment - Collects events from vulnerability intelligence sources such as CVE, the National Vulnerability Database (NVD), and MITRE and alert on vulnerabilities. (e.g., Amazon Inspector)</li> <li>IaC Unit Tests - Run automated test-driven infrastructure based on requirements (e.g., Cucumber/Gherkin, Rego)</li> </ul>"},{"location":"compute-image-pipeline/#test-multiple-stages","title":"Test (Multiple Stages)","text":"<ul> <li>Build Environment - Launch an environment from IaC templates (e.g., EC2 Image Builder, Packer)</li> <li>Integration Tests - Run tests against the launched environment to check for errors (e.g., AWS TaskCat)</li> <li>Approval - Optional action as part of a automated workflow, obtain authorized human approval before completing deployment. (e.g., CodePipeline Approval)</li> </ul>"},{"location":"compute-image-pipeline/#prod","title":"Prod","text":"<ul> <li>Generate Service Catalog Product - Provide shared services through a service catalog so that teams can obtain the software through self-service means in a controlled manner. IAM policies ensure only the necessary access is applied to principals.</li> <li>Package and Store Artifact(s) - Generate AMI or Container Image and store in a binary repository so that it can be consumed by an application/service pipeline. (e.g., AWS AMI, Amazon Elastic Container Registry, Docker Hub, Amazon S3)</li> </ul>"},{"location":"dynamic-configuration-pipeline/","title":"Architecture","text":"<p>Workloads following modern architecture styles (e.g. microservices) typically comprises multiple components. Each component has configuration that controls how the component behaves, which could be either mechanisms to control which features are enabled per environment to decouple release from deployment (e.g. Feature Flags) or operational configuration (e.g. log level, throttling thresholds, connection/request limits, alerts, notifications).</p> <p>The dynamic configuration pipeline enables teams to manage configuration for an entire workload and all its components in all environments as code so that all configurations are tracked in a code versioning system and can follow the common code review/approval process (e.g. pull/merge requests). The dynamic configuration pipeline can roll out configuration changes in a progressive and safe way to ensure that configuration changes do not break the workload in any environment.</p> <p></p>"},{"location":"dynamic-configuration-pipeline/#local-development","title":"Local Development","text":"<p>Developers need rapid feedback to make them aware of potential issues with their code. Automation should run in their development environment to give them feedback before the deployment pipeline runs.</p> Pre-Commit Hooks <p>Pre-Commit hooks are scripts that are executed on the developer's workstation when they try to create a new commit. These hooks have an opportunity to inspect the state of the code before the commit occurs and abort the commit if tests fail. An example of pre-commit hooks are Git hooks.  Examples of tools to configure and store pre-commit hooks as code include but are not limited to husky and pre-commit.</p> IDE Plugins <p>Warn developers of potential issues with their source code in their IDE using plugins and extensions including but not limited to Visual Studio Code - Python Extension and IntelliJ IDEA - JavaScript linters.</p>"},{"location":"dynamic-configuration-pipeline/#source","title":"Source","text":"<p>The source stage pulls in various types of code from a distributed version control system such as Git.</p> Infrastructure Source Code <p>Code that defines the infrastructure necessary to host the Dynamic Configuration. Examples of infrastructure source code include but are not limited to AWS Cloud Development Kit, AWS CloudFormation and HashiCorp Terraform. All Infrastructure Source Code is required to be stored in the same repository as the dynamic configuration definitions to allow infrastructure to be created and updated on the same lifecycle as the Dynamic Configuration.</p> Feature Flag definitions <p>Workload features often span multiple components and components can have dependencies on each other. Traditionally, features are released by deploying the corresponding code changes. As different components are owned by different teams, releasing features that span multiple components traditionally requires coordination between those teams to ensure that components' interdependencies are satisfied. The more teams' components require changes to implement a workload's feature, the more complex the coordination effort to safely and consistently release the feature becomes. This results in longer lead times, lower deployment frequency, and higher change failure rates.</p> <p>Additionally, modern teams who aim to achieve continuous deployment (i.e. deploy to production in full automation without any manual approvals) prefer trunk-based development, which means that a single branch manages the code that is deployed to all environments. While trunk-based development is a great way to achieve continuous deployment, it, by design, does not allow excluding code from being deployed to production, which in turn makes it challenging to coordinate the timing of releasing cross-component features consistently and safely.</p> <p>An effective way to solve this problem is to use Feature Flags to separate release from deployment. A Feature Flag is a mechanism that allows teams to enable/disable certain code fragments using a configuration item that is managed outside the codebase it is used in. In its simplest form, a Feature Flag has a name and a boolean value that is used in an if/else statement. For features that span multiple components, the corresponding feature flag can be used in all components. By wrapping code changes for a new feature in a statement that only executes the new code when the feature flag is turned on and continues to execute the old code when the feature flag is turned off, deploying the code does not release the feature, resulting in what is also called a \"dark release\". Releasing features is done by turning the corresponding feature flag on. This allows multiple teams to deploy changes to their components continuously and independently of each other, while features are released safely using feature flags. In case a feature turns out to be broken, the rollback procedure does not require deploying the previous version of the code but instead, the rollback procedure is as simple as turning the feature flag off.</p> <p>Some examples for the use of feature flags are: introducing new functionality to an existing workload, enabling or disabling functionality on a given workload without the need for re-deployments or restarts. For more details see: Using AWS AppConfig Feature Flags. Feature Flags are managed per environment, which allows releasing features for different environments independently. Feature Flags can be stored in any format, including but not limited to YAML, JSON, and XML.</p> Operational Configuration definition <p>Code that defined Operational Configuration that is deployed and managed by the Dynamic Configuration Pipeline. Operational Configuration is managed per environment, which allows to configure environments independently from each other. As an example, the default log level may be set to <code>DEBUG</code> in test environments whereas production environments are set to <code>ERROR</code> to only capture errors in the logs. Using the Dynamic Configuration Pipeline, Operational Configuration can be changed on the fly without redeploying the respective workload. Operational Configuration can be stored in any format, including but not limited to YAML, JSON, and XML.</p>"},{"location":"dynamic-configuration-pipeline/#build","title":"Build","text":"<p>All actions run in this stage are also run on developer's local environments prior to code commit and peer review. Actions in this stage should all run in less than 10 minutes so that developers can take action on fast feedback before moving on to their next task. If it\u2019s taking more time, consider decoupling the system to reduce dependencies, optimizing the process, using more efficient tooling, or moving some of the actions to latter stages. Each of the actions below are defined and run in code.</p> Build Code <p>Convert code into artifacts that can be promoted through environments. Most builds complete in seconds. While the Dynamic Configuration Pipeline does not house application source code that needs to be built, it may contain Infrastructure Source Code that needs to be built, e.g. CDK Source Code.</p> Unit Tests <p>Run the unit tests to verify that the <code>infrastructure as code (IaC)</code> complies with the specification expressed as unit tests to avoid unintended changes to the infrastructure. These tests are fast-running tests with zero dependencies on external systems returning results in seconds. In the case of CDK, unit tests are expressed in commonly used programming language-specific unit test frameworks, including but are not limited to JUnit, Jest, and pytest. Other IaC technologies such as Terraform have other unit test frameworks and mechanisms that can be leveraged to ensure that IaC is in line with the specification. Test results should be published as artifacts such as AWS CodeBuild Test Reports.</p> Code Quality <p>Run various automated static analysis tools that generate reports on code quality, coding standards, security, code coverage, and other aspects according to the team and/or organization\u2019s best practices. AWS recommends that teams fail the build when important practices are violated (e.g., a security violation is discovered in the code). These checks usually run in seconds. Examples of tools to measure code quality include but are not limited to Amazon CodeGuru, SonarQube, black, and ESLint.</p> <p></p> Secrets Detection <p>Identify secrets such as usernames, passwords, and access keys in code. When discovering secrets, the build should fail immediately. Examples of secret detection tools include but are not limited to GitGuardian and gitleaks.</p> <p></p> Static Application Security Testing (SAST) <p>Analyze code for application security violations such as XML External Entity Processing, SQL Injection, and Cross Site Scripting. Any findings that exceed the configured threshold will immediately fail the build and stop any forward progress in the pipeline. Examples of tools to perform static application security testing include but are not limited to Amazon CodeGuru, SonarQube, and Checkmarx.</p> <p></p> Software Composition Analysis (SCA) <p>Run software composition analysis (SCA) tools to find vulnerabilities to package repositories related to open source use, licensing, and security vulnerabilities. SCA tools also launch workflows to fix these vulnerabilities. Any findings that exceed the configured threshold will immediately fail the build and stop any forward progress in the pipeline. These tools can run directly against the source code or a software bill of materials (SBOM). Example SCA tools include but are not limited to Dependabot, Snyk, and Blackduck.</p> <p></p> Software Bill of Materials (SBOM) <p>Generate a software bill of materials (SBOM) report detailing all the dependencies used. Examples of SBOM formats include SPDX and CycloneDX</p>"},{"location":"dynamic-configuration-pipeline/#test-beta","title":"Test (Beta)","text":"<p>Testing is performed in a beta environment to validate that the latest code is functioning as expected. This validation is done by first deploying the code and then running integration and end-to-end tests against the deployment. Beta environments will have dependencies on the applications and services from other teams in their gamma environments. All actions performed in this stage should complete within 30 minutes to provide fast-feedback.</p> Deploy Feature Flags <p>Deploy Feature Flags to the beta environment. Software deployments should be performed through Infrastructure Source Code. Access to the beta environment should be handled via cross-account IAM roles rather than long lived credentials from IAM users. Examples of tools to define feature flags include but are not limited to: AWS AppConfig, Split.io and LaunchDarkly.</p> Deploy Operational Configuration <p>Deploy Operational Configurations to the beta environment. Software deployments should be performed through Infrastructure Source Code. Access to the beta environment should be handled via cross-account IAM roles rather than long lived credentials from IAM users. Examples of tools to define operational configurations include but are not limited to: AWS AppConfig, Split.io and LaunchDarkly.</p> Integration Tests <p>Run automated tests that verify if the application satisfies business requirements. These tests require the workload to be running in the beta environment. Integration tests may come in the form of behavior-driven tests, automated acceptance tests, or automated tests linked to requirements and/or stories in a tracking system. Test results should be published somewhere such as AWS CodeBuild Test Reports. Examples of tools to define integration tests include but are not limited to Cucumber, vRest, and SoapUI.</p> <p></p> Acceptance Tests <p>Run automated testing from the users\u2019 perspective in the beta environment. These tests verify the user workflow, including when performed through a UI. These test are the slowest to run and hardest to maintain and therefore it is recommended to only have a few end-to-end tests that cover the most important application workflows. Test results should be published somewhere such as AWS CodeBuild Test Reports. Examples of tools to define end-to-end tests include but are not limited to Cypress, Selenium, and Telerik Test Studio.</p> <p></p>"},{"location":"dynamic-configuration-pipeline/#test-gamma","title":"Test (Gamma)","text":"<p>Testing is performed in a gamma environment to validate that the latest code can be safely deployed to production. The environment is as production-like as possible including configuration, monitoring, and traffic. Additionally, the environment should match the same regions that the production environment uses. The gamma environment is used by other team's beta environments and therefore must maintain acceptable service levels to avoid impacting other team productivity. All actions performed in this stage should complete within 30 minutes to provide fast-feedback.</p> Deploy Feature Flags <p>Deploy Feature Flags to the gamma environment. Software deployments should be performed through Infrastructure Source Code. Access to the gamma environment should be handled via cross-account IAM roles rather than long lived credentials from IAM users.</p> Deploy Operational Configuration <p>Deploy Operational Configurations to the gamma environment. Software deployments should be performed through Infrastructure Source Code. Access to the gamma environment should be handled via cross-account IAM roles rather than long lived credentials from IAM users.</p> Integration Tests <p>Run automated tests that verify if the application satisfies business requirements. These tests require the workload to be running in the gamma environment. Integration tests may come in the form of behavior-driven tests, automated acceptance tests, or automated tests linked to requirements and/or stories in a tracking system. Test results should be published somewhere such as AWS CodeBuild Test Reports. Examples of tools to define integration tests include but are not limited to Cucumber, vRest, and SoapUI.</p> <p></p> Acceptance Tests <p>Run automated testing from the users\u2019 perspective in the gamma environment. These tests verify the user workflow, including when performed through a UI. These test are the slowest to run and hardest to maintain and therefore it is recommended to only have a few end-to-end tests that cover the most important application workflows. Test results should be published somewhere such as AWS CodeBuild Test Reports. Examples of tools to define end-to-end tests include but are not limited to Cypress, Selenium, and Telerik Test Studio.</p> <p></p> Monitoring &amp; Logging <p>Monitor deployments across regions and fail when threshold breached. The thresholds for metric alarms should be defined in the Infrastructure Source Code and deployed along with the rest of the infrastructure in an environment. Ideally, deployments should be automatically failed and rolled back when error thresholds are breached. Examples of automated rollback include AWS CloudFormation monitor &amp; rollback, AWS CodeDeploy rollback and Flagger.</p> <p></p> Synthetic Tests <p>Tests that run continuously in the background in a given environment to generate traffic and verify the system is healthy. These tests serve two purposes:</p> <ol> <li>Ensure there is always adequate traffic in the environment to trigger alarms if a deployment is unhealthy </li> <li>Test specific workflows and assert that the system is functioning correctly. </li> </ol> <p>Examples of tools that can be used for synthetic tests include but are not limited to Amazon CloudWatch Synthetics,Dynatrace Synthetic Monitoring, and Datadog Synthetic Monitoring.</p> <p></p> Performance Tests <p>Run longer-running automated capacity tests against environments that simulate production capacity. Measure metrics such as the transaction success rates, response time and throughput. Determine if application meets performance requirements and compare metrics to past performance to look for performance degredation. Examples of tools that can be used for performance tests include but are not limited to JMeter, Locust, and Gatling.</p> <p></p> Resilience Tests <p>Inject failures into environments to identify areas of the application that are susceptible to failure. Tests are defined as code and applied to the environment while the system is under load. The success rate, response time and throughput are measured during the periods when the failures are injected and compared to periods without the failures. Any significant deviation should fail the pipeline. Examples of tools that can be used for chaos/resilience testing include but are not limited to AWS Fault Injection Simulator, Gremlin, and ChaosToolkit.</p> <p></p> Dynamic Application Security Testing (DAST) <p>Perform testing of web applications and APIs by running automated scans against it to identify vulnerabilities through techniques such as cross-site scripting (XSS) and SQL injection(SQLi).  Examples of tools that can be used for dynamic application security testing include but are not limited to OWASP ZAP, StackHawk, and AppScan. See AWS Guidance on Penetration Testing for info on penetration testing in an AWS environment.</p>"},{"location":"dynamic-configuration-pipeline/#prod","title":"Prod","text":"Manual Approval <p>As part of an automated workflow, obtain authorized human approval before deploying to the production environment.</p> Progressively deploy Feature Flags and Operational Configuration <p>Deployments should be made progressively in waves to limit the impact of failures. A common approach is to deploy changes to a subset of AWS regions and allow sufficient bake time to monitor performance and behavior before proceeding with additional waves of AWS regions.</p> <p>Software should be deployed using one of progressive deployment involving controlled rollout of a change through techniques such as canary deployments, feature flags, and traffic shifting. Software deployments should be performed through Infrastructure Source Code. Access to the production environment should be handled via cross-account IAM roles rather than long lived credentials from IAM users. Examples of tools to deploy software include but are not limited to AWS CodeDeploy. Ideally, deployments should be automatically failed and rolled back when error thresholds are breached. Examples of automated rollback include AWS CloudFormation monitor &amp; rollback, AWS CodeDeploy rollback and Flagger.</p> <p></p> Acceptance Tests <p>Run automated testing from the users\u2019 perspective in the beta environment. These tests verify the user workflow, including when performed through a UI. These test are the slowest to run and hardest to maintain and therefore it is recommended to only have a few end-to-end tests that cover the most important application workflows. Test results should be published somewhere such as AWS CodeBuild Test Reports. Examples of tools to define end-to-end tests include but are not limited to Cypress, Selenium, and Telerik Test Studio.</p> <p></p> Synthetic Tests <p>Tests that run continuously in the background in a given environment to generate traffic and verify the system is healthy. These tests serve two purposes:</p> <ol> <li>Ensure there is always adequate traffic in the environment to trigger alarms if a deployment is unhealthy</li> <li>Test specific workflows and assert that the system is functioning correctly.</li> </ol> <p>Examples of tools that can be used for synthetic tests include but are not limited to Amazon CloudWatch Synthetics,Dynatrace Synthetic Monitoring, and Datadog Synthetic Monitoring.</p> <p></p> Monitoring &amp; Logging <p>Monitor deployments across regions and fail when threshold breached. The thresholds for metric alarms should be defined in the Infrastructure Source Code and deployed along with the rest of the infrastructure in an environment. Ideally, deployments should be automatically failed and rolled back when error thresholds are breached. Examples of automated rollback include AWS CloudFormation monitor &amp; rollback, AWS CodeDeploy rollback and Flagger.</p> <p></p> Synthetic Tests <p>Tests that run continuously in the background in a given environment to generate traffic and verify the system is healthy. These tests serve two purposes:</p> <ol> <li>Ensure there is always adequate traffic in the environment to trigger alarms if a deployment is unhealthy </li> <li>Test specific workflows and assert that the system is functioning correctly. </li> </ol> <p>Examples of tools that can be used for synthetic tests include but are not limited to Amazon CloudWatch Synthetics,Dynatrace Synthetic Monitoring, and Datadog Synthetic Monitoring.</p> <p></p>"},{"location":"dynamic-configuration-pipeline/ri-cdk-codepipeline-appconfig/","title":"Dynamic Config Pipeline reference implementation","text":"<p>This presents a reference implementation of the Dynamic Configuration Pipeline reference architecture. All infrastructure for this reference implementation is defined using the AWS Cloud Development Kit (CDK). The pipeline is defined using CDK Pipelines, which provisions the pipeline in AWS CodePipeline. The source code for this reference implementation is available in GitHub for running in your own AWS accounts.</p> <p>This reference implementation follows the following architecture: </p>"},{"location":"dynamic-configuration-pipeline/ri-cdk-codepipeline-appconfig/#local-development","title":"Local Development","text":"<p>Developers need fast feedback for potential issues with their code. Automation should run in their developer workspace to give them feedback before the deployment pipeline runs.</p> Pre-Commit Hooks <p>Pre-Commit hooks are scripts that are executed on the developer's workstation when they try to create a new commit. These hooks have an opportunity to inspect the state of the code before the commit occurs and abort the commit if tests fail. An example of pre-commit hooks are Git hooks.  Examples of tools to configure and store pre-commit hooks as code include but are not limited to husky and pre-commit.</p> <pre><code>repos:\n- repo: https://github.com/pre-commit/pre-commit-hooks\n  rev: v2.3.0\n  hooks:\n  -   id: check-yaml\n  -   id: check-json\n  -   id: trailing-whitespace\n- repo: https://github.com/pre-commit/mirrors-eslint\n  rev: v8.23.0\n  hooks:\n  -   id: eslint\n- repo: https://github.com/gitleaks/gitleaks\n  rev: v8.16.2\n  hooks:\n    - id: gitleaks\n</code></pre>"},{"location":"dynamic-configuration-pipeline/ri-cdk-codepipeline-appconfig/#source","title":"Source","text":"Infrastructure Source Code <p>The infrastructure source code can be found in the infrastructure directory. It is intended to serve as a reference, but much of the code can also be reused in your own CDK applications.</p> <p>The infrastructure source code defines both the deployment pipeline (AWS CodePipeline and the infrastructure that allows to store, manage and access the configuration (AWS AppConfig. </p> <pre><code>import { Stack, StackProps } from 'aws-cdk-lib';\nimport { ManagedPolicy, Role } from 'aws-cdk-lib/aws-iam';\nimport { Construct } from 'constructs';\nimport { OrgPathsPrincipal } from './org-paths-principal';\nimport { Config, WorkloadEnvironment } from '../config';\n\nexport class DynamicConfigurationGlobalStack extends Stack {\n  constructor(\n    scope: Construct,\n    id: string,\n    workloadName: string,\n    workloadEnvironments: WorkloadEnvironment[],\n    props?: StackProps,\n  ) {\n    super(scope, id, props);\n    workloadEnvironments.forEach((workloadEnvironment) =&gt; {\n      const role = new Role(\n        this,\n        Config.generateName(workloadName, 'DynamicConfigRole', workloadEnvironment.name),\n        {\n          roleName: Config.generateName(\n            workloadName,\n            'DynamicConfigRole',\n            workloadEnvironment.name,\n          ),\n          assumedBy: new OrgPathsPrincipal([\n            // This is set to '*' all accounts in the workload organization path are expected to need access to the Dynamic Config\n            [workloadEnvironment.workloadOrganizationPath, '*'].join('/'),\n          ]),\n        },\n      );\n\n      workloadEnvironment.getUniqueRegions().forEach((region) =&gt; {\n        const policy = ManagedPolicy.fromManagedPolicyName(\n          this,\n          Config.generateName(\n            'DynamicConfigPolicy',\n            workloadName,\n            workloadEnvironment.name,\n            region,\n          ),\n          Config.generateName('DynamicConfigPolicy', workloadName, workloadEnvironment.name, region),\n        );\n        role.addManagedPolicy(policy);\n      });\n    });\n  }\n}\n</code></pre> <p>Please note that the infrastructure code is written in Typescript, which is one of six (as of April 2023) programming languages supported by CDK.</p> IaC Unit Test Code <p>The infrastructure as code unit test source code can be found in the infrastructure/test directory. It is intended to serve as a reference but much of the code can also be reused in your own CDK applications.</p> <pre><code>test('Environments are generated on us-west-2', () =&gt; {\n  const app = new cdk.App({ context: context });\n  const config = Config.load(app.node);\n  const stack = new DynamicConfigurationCoreStack(\n    app,\n    'Core',\n    config.workloadName,\n    config.workloadEnvironments,\n    {\n      env: { account: '123456789012', region: 'us-west-2' },\n    },\n  );\n  const template = Template.fromStack(stack);\n\n  template.resourceCountIs('AWS::AppConfig::Environment', 3);\n  template.hasResourceProperties('AWS::AppConfig::Environment', { Name: 'alpha' });\n  template.hasResourceProperties('AWS::AppConfig::Environment', { Name: 'beta' });\n  template.hasResourceProperties('AWS::AppConfig::Environment', { Name: 'prod' });\n});\n</code></pre> Feature Flags <p>Feature Flags are stored in AWS AppConfig, which is part of AWS Systems Manager. Feature Flags are defined in the config/features directory. Within that directory you will see 2 files, one for the feature flag definitions and the other for the values:</p> <p>definitions.yaml: defines all feature flag and their types.</p> <pre><code>classification: boolean\n</code></pre> <p>values.yaml: Defines the values of all feature flags. Feature Flags are not intended to be used for environment- or region-specific configuration, e.g. enabling a feature only in production or enabling specific features in specific regions. Feature Flags are intended to enable separating release from deployment, so feature flag changes are released in the same way a software release version would be released. For every change to the Feature Flag values in this file, AppConfig creates a new version, which resembles the sum of all feature flag values. This version is deployed to the environments following the Software Development Lifecycle, i.e. to Beta first, then to Gamma and then to Production.</p> <pre><code>classification: true\n</code></pre> Operational Configuration <p>Operational Configuration is stored in AWS AppConfig as well. Operational Configuration is defined in the config/operations directory. Within that directory, there is a JSON Schema file that defines the requires structure of operational configuration in every environment as well as environment-specific YAML files defining the configuration values per environment.</p> <p>schema.json: defines the JSON schema that the Operational Configuration for each environment needs to satisfy.</p> <pre><code>{\n  \"type\": \"object\",\n  \"properties\": {\n    \"logLevel\": { \"enum\": [\"FATAL\", \"ERROR\", \"WARN\", \"INFO\", \"DEBUG\", \"TRACE\"] },\n    \"environment\": { \"type\": \"string\" }\n  },\n  \"required\": [\"logLevel\", \"environment\"]\n}\n</code></pre> <p>{environmentName}.yaml: stores the operational configuration values, whereas<code>{environmentName}</code> matches the <code>name</code> parameter of the <code>workloadEnvironments</code> within the <code>cdk.json</code> file.</p> <pre><code>logLevel: ERROR\n</code></pre>"},{"location":"dynamic-configuration-pipeline/ri-cdk-codepipeline-appconfig/#build","title":"Build","text":"<p>Actions in this stage all run in less than 10 minutes so that developers can take action on fast feedback before moving on to their next task. Each of the actions below are defined as code with AWS Cloud Development Kit.</p> Synthesize Code <p>Uses your AWS Cloud Development Kit code to generate the AWS CloudFormation templates for your pipeline and resources.</p> IaC Unit Tests <p>The unit tests are run by npm at the same time the <code>synth</code> action occurs. The results of the unit tests are uploaded to AWS Code Build Test Reports to be able to track them over time.</p> <p></p> Code Quality <p>ESLint statically analyzes your code to quickly find issues. You can use ESLint to create a series of assertions (called lint rules) that define how your code should look or behave. ESLint also has auto-fixer suggestions to help you improve your code. The pipeline will fail if any errors are found.</p> <p></p> <p>Additionally, cdk-nag is run to identify any security issues with the resources being created. The pipeline will fail if any are detected. The following code demonstrates how cdk-nag is called as a part of the build stage. The code also demonstrates how to suppress findings.</p> <pre><code>import * as process from 'process';\nimport * as cdk from 'aws-cdk-lib';\nimport { Aspects } from 'aws-cdk-lib';\nimport { AwsSolutionsChecks, NagSuppressions } from 'cdk-nag';\nimport { Config } from './config';\nimport { PipelineStack } from './pipeline-stack';\n\nconst app = new cdk.App();\nconst config = Config.load();\n\nconst props = {\n  description: `${config.workloadName} pipeline (${config.solutionCode})`,\n  env: {\n    account: process.env.CDK_DEPLOY_ACCOUNT || process.env.CDK_DEFAULT_ACCOUNT,\n    region: process.env.CDK_DEPLOY_REGION || process.env.CDK_DEFAULT_REGION,\n  },\n};\n\nnew PipelineStack(app, 'DynamicConfigurationPipeline', props);\n\nAspects.of(app).add(new AwsSolutionsChecks());\n\nNagSuppressions.addResourceSuppressions(\n  app,\n  [\n    { id: 'AwsSolutions-KMS5', reason: 'Default CodePipeline KMS key' },\n    { id: 'AwsSolutions-S1', reason: 'Default CodePipeline S3 bucket' },\n  ],\n  true,\n);\n</code></pre> Secrets Detection <p>Gitleaks is used for secrets detection. The scanning is accomplished by a CDK CodeBuild job to run <code>Gitleaks</code>:</p> <pre><code>const gitleaks = new CodeBuildStep('GitLeaks', {\n  input: source.codePipelineSource,\n  buildEnvironment: {\n    buildImage: LinuxBuildImage.AMAZON_LINUX_2_2,\n  },\n  installCommands: [\n    `VERSION=$(curl https://api.github.com/repositories/119190187/releases/latest | jq .tag_name -r | sed 's/v//')`,\n    `if [ \\${VERSION} == null ]; then VERSION=8.16.3; fi`,\n    `FILENAME=gitleaks_\\${VERSION}_linux_x64.tar.gz`,\n    `wget https://github.com/gitleaks/gitleaks/releases/download/v$VERSION/$FILENAME`,\n    `tar -zxvf $FILENAME gitleaks`,\n    `chmod +x gitleaks`,\n  ],\n  commands: [\n    `./gitleaks detect --source . --no-git --redact -v -r gitleaks.log`, // # no-git because the remote branch won't have a .git folder.\n  ],\n});\n</code></pre> Static Application Security Testing (SAST) <p>The CodeBuild jobs for Gitleaks and Trivy are also used for SAST.</p> Software Composition Analysis (SCA) <p>Trivy is used to scan the source for vulnerabilities in its dependencies. The configuration files are scanned for configuration issues or vulnerabilities in any dependencies. The scanning is accomplished by a CDK construct that creates a CodeBuild job to run <code>trivy</code>:</p> <pre><code>import { TrivyScan } from './code-analysis/trivy-scan';\n\n\u22ef\n\n    const trivyScan = new TrivyScan('TrivyScan', {\n      source: source.codePipelineSource,\n      severity: ['CRITICAL', 'HIGH'],\n      checks: ['vuln', 'config', 'secret'],\n    });\n</code></pre> Software Bill of Materials (SBOM) <p>Trivy generates an SBOM in the form of a CycloneDX JSON report. The SBOM is saved as a CodePipeline asset.  Trivy supports additional SBOM formats such as SPDX, and SARIF.</p>"},{"location":"dynamic-configuration-pipeline/ri-cdk-codepipeline-appconfig/#deployments","title":"Deployments","text":"<p>This reference implementations contains multiple CloudFormation stack deployments.</p>"},{"location":"dynamic-configuration-pipeline/ri-cdk-codepipeline-appconfig/#core","title":"Core","text":"<p>Creates the AppConfig application, deployment strategy, environments, feature flags and operational configurations and stores their references in the System Manager Parameter Store for all regions listed in <code>cdk.json</code> under <code>workloadEnvironments</code>. It also generates IAM Policies for access to those resources in preparation for the deployment stage.</p> <p><code>workloadEnvironments</code> is populated by the bootstrap.ts script and may also be edited manually.</p> <pre><code>{\n\"workloadEnvironments\": [\n    {\n        \"name\": \"alpha\",\n        \"workloadOrganizationPath\": \"o-orgid/r-rootid/ou-rootid-ouid/ou-abcd-aaaaaaa0\",\n        \"waves\": [\n            {\n                \"name\": \"alpha\",\n                \"regions\": [\"us-west-2\"]\n            }\n        ]\n    },\n    {\n        \"name\": \"beta\",\n        \"workloadOrganizationPath\": \"o-orgid/r-rootid/ou-rootid-ouid/ou-abcd-aaaaaaa0\",\n        \"waves\": [\n            {\n                \"name\": \"beta\",\n                \"regions\": [\"us-west-2\"]\n            }\n        ]\n    },\n    {\n        \"name\": \"gamma\",\n        \"workloadOrganizationPath\": \"o-orgid/r-rootid/ou-rootid-ouid/ou-abcd-bbbbbbb0\",\n        \"waves\": [\n            {\n                \"name\": \"gamma\",\n                \"regions\": [\"eu-central-1\", \"us-east-1\"]\n            }\n        ]\n    },\n    {\n        \"name\": \"prod\",\n        \"workloadOrganizationPath\": \"o-orgid/r-rootid/ou-rootid-ouid/ou-abcd-ccccccc0\",\n        \"waves\": [\n            {\n                \"name\": \"Prod-Americas\",\n                \"regions\": [\"us-east-1\", \"us-west-2\"]\n            },\n            {\n                \"name\": \"Prod-Europe\",\n                \"regions\": [\"eu-central-1\", \"eu-west-1\"]\n            }\n        ]\n    }\n],\n}\n</code></pre> <p>The <code>name</code> property represents the workload environment name to be used.</p> <p>The <code>workloadOrganizationPath</code> property is the organizational unit path used to share your AWS resources with groups of AWS accounts in AWS Organizations.</p> <p>The <code>waves</code> array defines the number of wave deployments each environment will have. Each wave will have a <code>name</code> and list of <code>regions</code> where the deployment will happen. Deployments will be done in the order specified in <code>workloadEnvironments</code> in <code>cdk.json</code> file.</p> <p>See Automating safe, hands-off deployments for more details about wave deployments.</p>"},{"location":"dynamic-configuration-pipeline/ri-cdk-codepipeline-appconfig/#global","title":"Global","text":"<p>Generates one role per environment, and attaches the managed policies created by the <code>core</code> stage pertinent to that specific environment.</p> <pre><code>import { Stack, StackProps } from 'aws-cdk-lib';\nimport { ManagedPolicy, Role } from 'aws-cdk-lib/aws-iam';\nimport { Construct } from 'constructs';\nimport { OrgPathsPrincipal } from './org-paths-principal';\nimport { Config, WorkloadEnvironment } from '../config';\n\nexport class DynamicConfigurationGlobalStack extends Stack {\n  constructor(\n    scope: Construct,\n    id: string,\n    workloadName: string,\n    workloadEnvironments: WorkloadEnvironment[],\n    props?: StackProps,\n  ) {\n    super(scope, id, props);\n    workloadEnvironments.forEach((workloadEnvironment) =&gt; {\n      const role = new Role(\n        this,\n        Config.generateName(workloadName, 'DynamicConfigRole', workloadEnvironment.name),\n        {\n          roleName: Config.generateName(\n            workloadName,\n            'DynamicConfigRole',\n            workloadEnvironment.name,\n          ),\n          assumedBy: new OrgPathsPrincipal([\n            // This is set to '*' all accounts in the workload organization path are expected to need access to the Dynamic Config\n            [workloadEnvironment.workloadOrganizationPath, '*'].join('/'),\n          ]),\n        },\n      );\n\n      workloadEnvironment.getUniqueRegions().forEach((region) =&gt; {\n        const policy = ManagedPolicy.fromManagedPolicyName(\n          this,\n          Config.generateName(\n            'DynamicConfigPolicy',\n            workloadName,\n            workloadEnvironment.name,\n            region,\n          ),\n          Config.generateName('DynamicConfigPolicy', workloadName, workloadEnvironment.name, region),\n        );\n        role.addManagedPolicy(policy);\n      });\n    });\n  }\n}\n</code></pre>"},{"location":"dynamic-configuration-pipeline/ri-cdk-codepipeline-appconfig/#service-discovery","title":"Service Discovery","text":"<p>Adds the ARN of the roles created by the <code>Global</code> stage to the parameter store within the tools account, so they can be referenced by the application pipeline.</p> <pre><code>import { Stack, StackProps } from 'aws-cdk-lib';\nimport { StringParameter } from 'aws-cdk-lib/aws-ssm';\nimport { Construct } from 'constructs';\nimport { Config, WorkloadEnvironment } from '../config';\n\nexport class ServiceDiscoveryStack extends Stack {\n  constructor(\n    scope: Construct,\n    id: string,\n    workloadName: string,\n    dynamicConfigAccountNumber: string,\n    workloadEnvironments: WorkloadEnvironment[],\n    props?: StackProps,\n  ) {\n    super(scope, id, props);\n    workloadEnvironments.forEach((workloadEnvironment) =&gt; {\n      const roleArn = Stack.of(this).formatArn({\n        region: '',\n        service: 'iam',\n        account: dynamicConfigAccountNumber,\n        resource: 'role',\n        resourceName: Config.generateName(\n          workloadName,\n          'DynamicConfigRole',\n          workloadEnvironment.name,\n        ),\n      });\n\n      new StringParameter(\n        this,\n        Config.generateName(workloadName, 'DynamicConfigRoleParameter', workloadEnvironment.name),\n        {\n          parameterName: `/${workloadName}/dynamic_config_role-${workloadEnvironment.name}`,\n          stringValue: roleArn,\n        },\n      );\n    });\n  }\n}\n</code></pre>"},{"location":"dynamic-configuration-pipeline/ri-cdk-codepipeline-appconfig/#environment","title":"Environment","text":"<p>Deploys the configuration (Feature Flags and Operational Configuration) to AWS AppConfig for all regions listed in the <code>cdk.json</code> under <code>workloadEnvironments</code>.</p> <p>The <code>waves</code> array defines the number of Waves for  each environment. Each Wave has a <code>name</code> and list of <code>regions</code> where the deployment will happen. Deployments will be done in the order specified in <code>workloadEnvironments</code> in <code>cdk.json</code>.</p> <p>See Automating safe, hands-off deployments for more details about wave deployments.</p>"}]}